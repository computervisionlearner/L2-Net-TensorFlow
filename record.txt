18-06-04 22:43-INFO-Restoring parameters from ckpt/model.ckpt-390385
18-06-04 22:43-INFO->> Step 390380 run_train: loss = 5.3939  (9.563 sec)
18-06-04 22:43-INFO->> Step 390390 run_train: loss = 5.3976  (0.160 sec)
18-06-04 22:43-INFO->> Step 390400 run_train: loss = 5.4367  (0.118 sec)
18-06-04 22:43-INFO->> Step 390410 run_train: loss = 5.4649  (0.145 sec)
18-06-04 22:43-INFO->> Step 390420 run_train: loss = 5.4345  (0.146 sec)
18-06-04 22:43-INFO->> Step 390430 run_train: loss = 5.4320  (0.117 sec)
18-06-04 22:43-INFO->> Step 390440 run_train: loss = 5.4066  (0.187 sec)
18-06-04 22:43-INFO->> Step 390450 run_train: loss = 5.4320  (0.141 sec)
18-06-04 22:43-INFO->> Step 390460 run_train: loss = 5.4413  (0.165 sec)
18-06-04 22:43-INFO->> Step 390470 run_train: loss = 5.4778  (0.155 sec)
18-06-04 22:43-INFO->> Step 390480 run_train: loss = 5.3603  (0.158 sec)
18-06-04 22:43-INFO->> Step 390490 run_train: loss = 5.4234  (0.181 sec)
18-06-04 22:43-INFO->> Step 390500 run_train: loss = 5.3938  (0.166 sec)
18-06-04 22:43-INFO->> Step 390510 run_train: loss = 5.4622  (0.158 sec)
18-06-04 22:43-INFO->> Step 390520 run_train: loss = 5.4030  (0.163 sec)
18-06-04 22:43-INFO->> Step 390530 run_train: loss = 5.4532  (0.149 sec)
18-06-04 22:43-INFO->> Step 390540 run_train: loss = 5.4174  (0.147 sec)
18-06-04 22:43-INFO->> Step 390550 run_train: loss = 5.5136  (0.164 sec)
18-06-04 22:43-INFO->> Step 390560 run_train: loss = 5.4823  (0.149 sec)
18-06-04 22:43-INFO->> Step 390570 run_train: loss = 5.4851  (0.176 sec)
18-06-04 22:43-INFO->> Step 390580 run_train: loss = 5.4513  (0.149 sec)
18-06-04 22:43-INFO->> Step 390590 run_train: loss = 5.4581  (0.085 sec)
18-06-04 22:43-INFO->> Step 390600 run_train: loss = 5.4682  (0.165 sec)
18-06-04 22:43-INFO->> Step 390610 run_train: loss = 5.4284  (0.125 sec)
18-06-04 22:43-INFO->> Step 390620 run_train: loss = 5.3762  (0.152 sec)
18-06-04 22:44-INFO->> Step 390630 run_train: loss = 5.4514  (0.147 sec)
18-06-04 22:44-INFO->> Step 390640 run_train: loss = 5.4802  (0.129 sec)
18-06-04 22:44-INFO->> Step 390650 run_train: loss = 5.4013  (0.214 sec)
18-06-04 22:44-INFO->> Step 390660 run_train: loss = 5.4313  (0.156 sec)
18-06-04 22:44-INFO->> Step 390670 run_train: loss = 5.4978  (0.142 sec)
18-06-04 22:44-INFO->> Step 390680 run_train: loss = 5.4065  (0.154 sec)
18-06-04 22:44-INFO->> Step 390690 run_train: loss = 5.4663  (0.140 sec)
18-06-04 22:44-INFO->> Step 390700 run_train: loss = 5.5015  (0.161 sec)
18-06-04 22:44-INFO->> Step 390710 run_train: loss = 5.3957  (0.403 sec)
18-06-04 22:44-INFO->> Step 390720 run_train: loss = 5.4429  (0.201 sec)
18-06-04 22:44-INFO->> Step 390730 run_train: loss = 5.4559  (0.189 sec)
18-06-04 22:44-INFO->> Step 390740 run_train: loss = 5.4590  (0.118 sec)
18-06-04 22:44-INFO->> Step 390750 run_train: loss = 5.4125  (0.164 sec)
18-06-04 22:44-INFO->> Step 390760 run_train: loss = 5.4150  (0.136 sec)
18-06-04 22:44-INFO->> Step 390770 run_train: loss = 5.4270  (0.148 sec)
18-06-04 22:44-INFO->> Step 390780 run_train: loss = 5.3445  (0.186 sec)
18-06-04 22:44-INFO->> Step 390790 run_train: loss = 5.4668  (0.177 sec)
18-06-04 22:44-INFO->> Step 390800 run_train: loss = 5.4228  (0.156 sec)
18-06-04 22:44-INFO->> Step 390810 run_train: loss = 5.4189  (0.182 sec)
18-06-04 22:44-INFO->> Step 390820 run_train: loss = 5.4615  (0.191 sec)
18-06-04 22:44-INFO->> Step 390830 run_train: loss = 5.3644  (0.147 sec)
18-06-04 22:44-INFO->> Step 390840 run_train: loss = 5.4781  (0.130 sec)
18-06-04 22:44-INFO->> Step 390850 run_train: loss = 5.4751  (0.186 sec)
18-06-04 22:44-INFO->> Step 390860 run_train: loss = 5.3720  (0.141 sec)
18-06-04 22:44-INFO->> Step 390870 run_train: loss = 5.4537  (0.158 sec)
18-06-04 22:44-INFO->> Step 390880 run_train: loss = 5.4372  (0.196 sec)
18-06-04 22:44-INFO->> Step 390890 run_train: loss = 5.4152  (0.197 sec)
18-06-04 22:44-INFO->> Step 390900 run_train: loss = 5.3931  (0.155 sec)
18-06-04 22:44-INFO->> Step 390910 run_train: loss = 5.4924  (0.191 sec)
18-06-04 22:44-INFO->> Step 390920 run_train: loss = 5.4964  (0.224 sec)
18-06-04 22:44-INFO->> Step 390930 run_train: loss = 5.4498  (0.160 sec)
18-06-04 22:44-INFO->> Step 390940 run_train: loss = 5.4664  (0.148 sec)
18-06-04 22:44-INFO->> Step 390950 run_train: loss = 5.4071  (0.169 sec)
18-06-04 22:44-INFO->> Step 390960 run_train: loss = 5.4204  (0.151 sec)
18-06-04 22:44-INFO->> Step 390970 run_train: loss = 5.4621  (0.141 sec)
18-06-04 22:44-INFO->> Step 390980 run_train: loss = 5.4538  (0.175 sec)
18-06-04 22:44-INFO->> Step 390990 run_train: loss = 5.4147  (0.163 sec)
18-06-04 22:45-INFO->> Step 391000 run_train: loss = 5.3995  (0.175 sec)
18-06-04 22:45-INFO->> 2018-06-04 22:45:00.353419 Saving in ckpt
18-06-04 22:45-INFO-Test Data Eval:
18-06-04 22:45-INFO-fpr95 = 0.16989074123273112 and auc = 0.9694267583757862
18-06-04 22:45-INFO->> Step 391010 run_train: loss = 5.4396  (0.160 sec)
18-06-04 22:45-INFO->> Step 391020 run_train: loss = 5.4464  (0.140 sec)
18-06-04 22:45-INFO->> Step 391030 run_train: loss = 5.4435  (0.166 sec)
18-06-04 22:45-INFO->> Step 391040 run_train: loss = 5.4315  (0.202 sec)
18-06-04 22:45-INFO->> Step 391050 run_train: loss = 5.4561  (0.186 sec)
18-06-04 22:45-INFO->> Step 391060 run_train: loss = 5.4850  (0.155 sec)
18-06-04 22:45-INFO->> Step 391070 run_train: loss = 5.4168  (0.149 sec)
18-06-04 22:45-INFO->> Step 391080 run_train: loss = 5.4197  (0.120 sec)
18-06-04 22:45-INFO->> Step 391090 run_train: loss = 5.4683  (0.138 sec)
18-06-04 22:45-INFO->> Step 391100 run_train: loss = 5.3991  (0.171 sec)
18-06-04 22:45-INFO->> Step 391110 run_train: loss = 5.3460  (0.132 sec)
18-06-04 22:46-INFO->> Step 391120 run_train: loss = 5.3933  (0.188 sec)
18-06-04 22:46-INFO->> Step 391130 run_train: loss = 5.4501  (0.182 sec)
18-06-04 22:46-INFO->> Step 391140 run_train: loss = 5.4236  (0.143 sec)
18-06-04 22:46-INFO->> Step 391150 run_train: loss = 5.4142  (0.127 sec)
18-06-04 22:46-INFO->> Step 391160 run_train: loss = 5.4133  (0.157 sec)
18-06-04 22:46-INFO->> Step 391170 run_train: loss = 5.4288  (0.162 sec)
18-06-04 22:46-INFO->> Step 391180 run_train: loss = 5.4078  (0.140 sec)
18-06-04 22:46-INFO->> Step 391190 run_train: loss = 5.4506  (0.149 sec)
18-06-04 22:46-INFO->> Step 391200 run_train: loss = 5.4436  (0.168 sec)
18-06-04 22:46-INFO->> Step 391210 run_train: loss = 5.4517  (0.157 sec)
18-06-04 22:46-INFO->> Step 391220 run_train: loss = 5.4877  (0.173 sec)
18-06-04 22:46-INFO->> Step 391230 run_train: loss = 5.4663  (0.160 sec)
18-06-04 22:46-INFO->> Step 391240 run_train: loss = 5.4030  (0.144 sec)
18-06-04 22:46-INFO->> Step 391250 run_train: loss = 5.4448  (0.132 sec)
18-06-04 22:46-INFO->> Step 391260 run_train: loss = 5.4004  (0.188 sec)
18-06-04 22:46-INFO->> Step 391270 run_train: loss = 5.4410  (0.168 sec)
18-06-04 22:46-INFO->> Step 391280 run_train: loss = 5.4451  (0.171 sec)
18-06-04 22:46-INFO->> Step 391290 run_train: loss = 5.4872  (0.150 sec)
18-06-04 22:46-INFO->> Step 391300 run_train: loss = 5.4289  (0.181 sec)
18-06-04 22:46-INFO->> Step 391310 run_train: loss = 5.4254  (0.147 sec)
18-06-04 22:46-INFO->> Step 391320 run_train: loss = 5.4674  (0.149 sec)
18-06-04 22:46-INFO->> Step 391330 run_train: loss = 5.3822  (0.151 sec)
18-06-04 22:46-INFO->> Step 391340 run_train: loss = 5.4363  (0.161 sec)
18-06-04 22:46-INFO->> Step 391350 run_train: loss = 5.4123  (0.161 sec)
18-06-04 22:46-INFO->> Step 391360 run_train: loss = 5.4485  (0.177 sec)
18-06-04 22:46-INFO->> Step 391370 run_train: loss = 5.4297  (0.190 sec)
18-06-04 22:46-INFO->> Step 391380 run_train: loss = 5.4520  (0.151 sec)
18-06-04 22:46-INFO->> Step 391390 run_train: loss = 5.5002  (0.165 sec)
18-06-04 22:46-INFO->> Step 391400 run_train: loss = 5.4367  (0.147 sec)
18-06-04 22:46-INFO->> Step 391410 run_train: loss = 5.4013  (0.138 sec)
18-06-04 22:46-INFO->> Step 391420 run_train: loss = 5.4250  (0.148 sec)
18-06-04 22:46-INFO->> Step 391430 run_train: loss = 5.4578  (0.156 sec)
18-06-04 22:46-INFO->> Step 391440 run_train: loss = 5.4476  (0.152 sec)
18-06-04 22:46-INFO->> Step 391450 run_train: loss = 5.4171  (0.187 sec)
18-06-04 22:46-INFO->> Step 391460 run_train: loss = 5.3452  (0.144 sec)
18-06-04 22:46-INFO->> Step 391470 run_train: loss = 5.4599  (0.154 sec)
18-06-04 22:46-INFO->> Step 391480 run_train: loss = 5.4268  (0.177 sec)
18-06-04 22:46-INFO->> Step 391490 run_train: loss = 5.4328  (0.131 sec)
18-06-04 22:47-INFO->> Step 391500 run_train: loss = 5.4515  (0.167 sec)
18-06-04 22:47-INFO->> Step 391510 run_train: loss = 5.5024  (0.191 sec)
18-06-04 22:47-INFO->> Step 391520 run_train: loss = 5.4017  (0.147 sec)
18-06-04 22:47-INFO->> Step 391530 run_train: loss = 5.4439  (0.222 sec)
18-06-04 22:47-INFO->> Step 391540 run_train: loss = 5.4518  (0.150 sec)
18-06-04 22:47-INFO->> Step 391550 run_train: loss = 5.4107  (0.160 sec)
18-06-04 22:47-INFO->> Step 391560 run_train: loss = 5.3953  (0.188 sec)
18-06-04 22:47-INFO->> Step 391570 run_train: loss = 5.4859  (0.156 sec)
18-06-04 22:47-INFO->> Step 391580 run_train: loss = 5.4466  (0.179 sec)
18-06-04 22:47-INFO->> Step 391590 run_train: loss = 5.4013  (0.156 sec)
18-06-04 22:47-INFO->> Step 391600 run_train: loss = 5.4905  (0.168 sec)
18-06-04 22:47-INFO->> Step 391610 run_train: loss = 5.4293  (0.118 sec)
18-06-04 22:47-INFO->> Step 391620 run_train: loss = 5.4053  (0.155 sec)
18-06-04 22:47-INFO->> Step 391630 run_train: loss = 5.4635  (0.198 sec)
18-06-04 22:47-INFO->> Step 391640 run_train: loss = 5.3778  (0.172 sec)
18-06-04 22:47-INFO->> Step 391650 run_train: loss = 5.3902  (0.190 sec)
18-06-04 22:47-INFO->> Step 391660 run_train: loss = 5.3980  (0.134 sec)
18-06-04 22:47-INFO->> Step 391670 run_train: loss = 5.4474  (0.142 sec)
18-06-04 22:47-INFO->> Step 391680 run_train: loss = 5.3855  (0.136 sec)
18-06-04 22:47-INFO->> Step 391690 run_train: loss = 5.4175  (0.152 sec)
18-06-04 22:47-INFO->> Step 391700 run_train: loss = 5.4135  (0.157 sec)
18-06-04 22:47-INFO->> Step 391710 run_train: loss = 5.4611  (0.150 sec)
18-06-04 22:47-INFO->> Step 391720 run_train: loss = 5.4113  (0.158 sec)
18-06-04 22:47-INFO->> Step 391730 run_train: loss = 5.4357  (0.146 sec)
18-06-04 22:47-INFO->> Step 391740 run_train: loss = 5.4071  (0.160 sec)
18-06-04 22:47-INFO->> Step 391750 run_train: loss = 5.4805  (0.174 sec)
18-06-04 22:47-INFO->> Step 391760 run_train: loss = 5.4703  (0.134 sec)
18-06-04 22:47-INFO->> Step 391770 run_train: loss = 5.4529  (0.147 sec)
18-06-04 22:47-INFO->> Step 391780 run_train: loss = 5.4726  (0.134 sec)
18-06-04 22:47-INFO->> Step 391790 run_train: loss = 5.3651  (0.148 sec)
18-06-04 22:47-INFO->> Step 391800 run_train: loss = 5.3909  (0.161 sec)
18-06-04 22:47-INFO->> Step 391810 run_train: loss = 5.4128  (0.152 sec)
18-06-04 22:47-INFO->> Step 391820 run_train: loss = 5.4763  (0.171 sec)
18-06-04 22:47-INFO->> Step 391830 run_train: loss = 5.3930  (0.169 sec)
18-06-04 22:47-INFO->> Step 391840 run_train: loss = 5.4664  (0.190 sec)
18-06-04 22:47-INFO->> Step 391850 run_train: loss = 5.3725  (0.153 sec)
18-06-04 22:48-INFO->> Step 391860 run_train: loss = 5.4368  (0.114 sec)
18-06-04 22:48-INFO->> Step 391870 run_train: loss = 5.4673  (0.168 sec)
18-06-04 22:48-INFO->> Step 391880 run_train: loss = 5.4002  (0.170 sec)
18-06-04 22:48-INFO->> Step 391890 run_train: loss = 5.4719  (0.156 sec)
18-06-04 22:48-INFO->> Step 391900 run_train: loss = 5.3628  (0.187 sec)
18-06-04 22:48-INFO->> Step 391910 run_train: loss = 5.4113  (0.179 sec)
18-06-04 22:48-INFO->> Step 391920 run_train: loss = 5.4448  (0.194 sec)
18-06-04 22:48-INFO->> Step 391930 run_train: loss = 5.4334  (0.152 sec)
18-06-04 22:48-INFO->> Step 391940 run_train: loss = 5.4884  (0.197 sec)
18-06-04 22:48-INFO->> Step 391950 run_train: loss = 5.4344  (0.201 sec)
18-06-04 22:48-INFO->> Step 391960 run_train: loss = 5.3710  (0.150 sec)
18-06-04 22:48-INFO->> Step 391970 run_train: loss = 5.4359  (0.154 sec)
18-06-04 22:48-INFO->> Step 391980 run_train: loss = 5.4272  (0.171 sec)
18-06-04 22:48-INFO->> Step 391990 run_train: loss = 5.4215  (0.151 sec)
18-06-04 22:48-INFO->> Step 392000 run_train: loss = 5.4240  (0.138 sec)
18-06-04 22:48-INFO->> 2018-06-04 22:48:23.730746 Saving in ckpt
18-06-04 22:48-INFO-Test Data Eval:
18-06-04 22:49-INFO-fpr95 = 0.172970908607864 and auc = 0.9689531352863849
18-06-04 22:49-INFO->> Step 392010 run_train: loss = 5.4383  (0.154 sec)
18-06-04 22:49-INFO->> Step 392020 run_train: loss = 5.4731  (0.158 sec)
18-06-04 22:49-INFO->> Step 392030 run_train: loss = 5.4346  (0.182 sec)
18-06-04 22:49-INFO->> Step 392040 run_train: loss = 5.4278  (0.174 sec)
18-06-04 22:49-INFO->> Step 392050 run_train: loss = 5.4555  (0.164 sec)
18-06-04 22:49-INFO->> Step 392060 run_train: loss = 5.4099  (0.159 sec)
18-06-04 22:49-INFO->> Step 392070 run_train: loss = 5.4194  (0.152 sec)
18-06-04 22:49-INFO->> Step 392080 run_train: loss = 5.4202  (0.150 sec)
18-06-04 22:49-INFO->> Step 392090 run_train: loss = 5.4096  (0.131 sec)
18-06-04 22:49-INFO->> Step 392100 run_train: loss = 5.4072  (0.184 sec)
18-06-04 22:49-INFO->> Step 392110 run_train: loss = 5.4480  (0.130 sec)
18-06-04 22:49-INFO->> Step 392120 run_train: loss = 5.3915  (0.165 sec)
18-06-04 22:49-INFO->> Step 392130 run_train: loss = 5.4527  (0.157 sec)
18-06-04 22:49-INFO->> Step 392140 run_train: loss = 5.3942  (0.164 sec)
18-06-04 22:49-INFO->> Step 392150 run_train: loss = 5.4298  (0.165 sec)
18-06-04 22:49-INFO->> Step 392160 run_train: loss = 5.4242  (0.200 sec)
18-06-04 22:49-INFO->> Step 392170 run_train: loss = 5.3292  (0.149 sec)
18-06-04 22:49-INFO->> Step 392180 run_train: loss = 5.3752  (0.164 sec)
18-06-04 22:49-INFO->> Step 392190 run_train: loss = 5.4223  (0.157 sec)
18-06-04 22:49-INFO->> Step 392200 run_train: loss = 5.4331  (0.178 sec)
18-06-04 22:49-INFO->> Step 392210 run_train: loss = 5.4345  (0.147 sec)
18-06-04 22:49-INFO->> Step 392220 run_train: loss = 5.4296  (0.192 sec)
18-06-04 22:49-INFO->> Step 392230 run_train: loss = 5.3836  (0.149 sec)
18-06-04 22:49-INFO->> Step 392240 run_train: loss = 5.4205  (0.137 sec)
18-06-04 22:49-INFO->> Step 392250 run_train: loss = 5.4403  (0.161 sec)
18-06-04 22:49-INFO->> Step 392260 run_train: loss = 5.4274  (0.221 sec)
18-06-04 22:49-INFO->> Step 392270 run_train: loss = 5.3795  (0.147 sec)
18-06-04 22:49-INFO->> Step 392280 run_train: loss = 5.4242  (0.158 sec)
18-06-04 22:49-INFO->> Step 392290 run_train: loss = 5.4353  (0.168 sec)
18-06-04 22:49-INFO->> Step 392300 run_train: loss = 5.3802  (0.150 sec)
18-06-04 22:49-INFO->> Step 392310 run_train: loss = 5.4671  (0.125 sec)
18-06-04 22:49-INFO->> Step 392320 run_train: loss = 5.3621  (0.157 sec)
18-06-04 22:49-INFO->> Step 392330 run_train: loss = 5.4154  (0.150 sec)
18-06-04 22:50-INFO->> Step 392340 run_train: loss = 5.3812  (0.175 sec)
18-06-04 22:50-INFO->> Step 392350 run_train: loss = 5.4560  (0.164 sec)
18-06-04 22:50-INFO->> Step 392360 run_train: loss = 5.4460  (0.154 sec)
18-06-04 22:50-INFO->> Step 392370 run_train: loss = 5.4096  (0.160 sec)
18-06-04 22:50-INFO->> Step 392380 run_train: loss = 5.4003  (0.168 sec)
18-06-04 22:50-INFO->> Step 392390 run_train: loss = 5.4246  (0.190 sec)
18-06-04 22:50-INFO->> Step 392400 run_train: loss = 5.4275  (0.172 sec)
18-06-04 22:50-INFO->> Step 392410 run_train: loss = 5.3946  (0.134 sec)
18-06-04 22:50-INFO->> Step 392420 run_train: loss = 5.4188  (0.173 sec)
18-06-04 22:50-INFO->> Step 392430 run_train: loss = 5.3340  (0.163 sec)
18-06-04 22:50-INFO->> Step 392440 run_train: loss = 5.5104  (0.158 sec)
18-06-04 22:50-INFO->> Step 392450 run_train: loss = 5.4274  (0.160 sec)
18-06-04 22:50-INFO->> Step 392460 run_train: loss = 5.3801  (0.179 sec)
18-06-04 22:50-INFO->> Step 392470 run_train: loss = 5.4384  (0.132 sec)
18-06-04 22:50-INFO->> Step 392480 run_train: loss = 5.3751  (0.186 sec)
18-06-04 22:50-INFO->> Step 392490 run_train: loss = 5.4337  (0.159 sec)
18-06-04 22:50-INFO->> Step 392500 run_train: loss = 5.4073  (0.149 sec)
18-06-04 22:50-INFO->> Step 392510 run_train: loss = 5.4999  (0.144 sec)
18-06-04 22:50-INFO->> Step 392520 run_train: loss = 5.4177  (0.181 sec)
18-06-04 22:50-INFO->> Step 392530 run_train: loss = 5.3962  (0.133 sec)
18-06-04 22:50-INFO->> Step 392540 run_train: loss = 5.3544  (0.156 sec)
18-06-04 22:50-INFO->> Step 392550 run_train: loss = 5.4368  (0.162 sec)
18-06-04 22:50-INFO->> Step 392560 run_train: loss = 5.4010  (0.185 sec)
18-06-04 22:50-INFO->> Step 392570 run_train: loss = 5.4613  (0.158 sec)
18-06-04 22:50-INFO->> Step 392580 run_train: loss = 5.4163  (0.186 sec)
18-06-04 22:50-INFO->> Step 392590 run_train: loss = 5.3429  (0.185 sec)
18-06-04 22:50-INFO->> Step 392600 run_train: loss = 5.3927  (0.143 sec)
18-06-04 22:50-INFO->> Step 392610 run_train: loss = 5.4152  (0.149 sec)
18-06-04 22:50-INFO->> Step 392620 run_train: loss = 5.4240  (0.139 sec)
18-06-04 22:50-INFO->> Step 392630 run_train: loss = 5.4225  (0.153 sec)
18-06-04 22:50-INFO->> Step 392640 run_train: loss = 5.4043  (0.212 sec)
18-06-04 22:50-INFO->> Step 392650 run_train: loss = 5.4091  (0.138 sec)
18-06-04 22:50-INFO->> Step 392660 run_train: loss = 5.4817  (0.178 sec)
18-06-04 22:50-INFO->> Step 392670 run_train: loss = 5.4426  (0.154 sec)
18-06-04 22:50-INFO->> Step 392680 run_train: loss = 5.4271  (0.146 sec)
18-06-04 22:50-INFO->> Step 392690 run_train: loss = 5.4641  (0.138 sec)
18-06-04 22:50-INFO->> Step 392700 run_train: loss = 5.4750  (0.155 sec)
18-06-04 22:50-INFO->> Step 392710 run_train: loss = 5.4106  (0.201 sec)
18-06-04 22:51-INFO->> Step 392720 run_train: loss = 5.4238  (0.176 sec)
18-06-04 22:51-INFO->> Step 392730 run_train: loss = 5.4275  (0.138 sec)
18-06-04 22:51-INFO->> Step 392740 run_train: loss = 5.3863  (0.168 sec)
18-06-04 22:51-INFO->> Step 392750 run_train: loss = 5.4123  (0.147 sec)
18-06-04 22:51-INFO->> Step 392760 run_train: loss = 5.4697  (0.135 sec)
18-06-04 22:51-INFO->> Step 392770 run_train: loss = 5.3871  (0.150 sec)
18-06-04 22:51-INFO->> Step 392780 run_train: loss = 5.4292  (0.172 sec)
18-06-04 22:51-INFO->> Step 392790 run_train: loss = 5.4472  (0.140 sec)
18-06-04 22:51-INFO->> Step 392800 run_train: loss = 5.4004  (0.197 sec)
18-06-04 22:51-INFO->> Step 392810 run_train: loss = 5.4013  (0.136 sec)
18-06-04 22:51-INFO->> Step 392820 run_train: loss = 5.4176  (0.187 sec)
18-06-04 22:51-INFO->> Step 392830 run_train: loss = 5.4408  (0.154 sec)
18-06-04 22:51-INFO->> Step 392840 run_train: loss = 5.3609  (0.176 sec)
18-06-04 22:51-INFO->> Step 392850 run_train: loss = 5.4555  (0.139 sec)
18-06-04 22:51-INFO->> Step 392860 run_train: loss = 5.4230  (0.213 sec)
18-06-04 22:51-INFO->> Step 392870 run_train: loss = 5.3969  (0.121 sec)
18-06-04 22:51-INFO->> Step 392880 run_train: loss = 5.3579  (0.144 sec)
18-06-04 22:51-INFO->> Step 392890 run_train: loss = 5.4166  (0.185 sec)
18-06-04 22:51-INFO->> Step 392900 run_train: loss = 5.5084  (0.183 sec)
18-06-04 22:51-INFO->> Step 392910 run_train: loss = 5.3615  (0.138 sec)
18-06-04 22:51-INFO->> Step 392920 run_train: loss = 5.4275  (0.161 sec)
18-06-04 22:51-INFO->> Step 392930 run_train: loss = 5.3806  (0.214 sec)
18-06-04 22:51-INFO->> Step 392940 run_train: loss = 5.4557  (0.156 sec)
18-06-04 22:51-INFO->> Step 392950 run_train: loss = 5.4126  (0.163 sec)
18-06-04 22:51-INFO->> Step 392960 run_train: loss = 5.4088  (0.150 sec)
18-06-04 22:51-INFO->> Step 392970 run_train: loss = 5.4253  (0.166 sec)
18-06-04 22:51-INFO->> Step 392980 run_train: loss = 5.3943  (0.152 sec)
18-06-04 22:51-INFO->> Step 392990 run_train: loss = 5.4462  (0.129 sec)
18-06-04 22:51-INFO->> Step 393000 run_train: loss = 5.3947  (0.179 sec)
18-06-04 22:51-INFO->> 2018-06-04 22:51:45.899881 Saving in ckpt
18-06-04 22:51-INFO-Test Data Eval:
18-06-04 22:52-INFO-fpr95 = 0.1709866498405951 and auc = 0.96934762678624
18-06-04 22:52-INFO->> Step 393010 run_train: loss = 5.3846  (0.177 sec)
18-06-04 22:52-INFO->> Step 393020 run_train: loss = 5.4626  (0.182 sec)
18-06-04 22:52-INFO->> Step 393030 run_train: loss = 5.4174  (0.171 sec)
18-06-04 22:52-INFO->> Step 393040 run_train: loss = 5.4552  (0.178 sec)
18-06-04 22:52-INFO->> Step 393050 run_train: loss = 5.4481  (0.135 sec)
18-06-04 22:52-INFO->> Step 393060 run_train: loss = 5.3834  (0.163 sec)
18-06-04 22:52-INFO->> Step 393070 run_train: loss = 5.4648  (0.150 sec)
18-06-04 22:52-INFO->> Step 393080 run_train: loss = 5.4476  (0.166 sec)
18-06-04 22:52-INFO->> Step 393090 run_train: loss = 5.4264  (0.156 sec)
18-06-04 22:52-INFO->> Step 393100 run_train: loss = 5.4720  (0.165 sec)
18-06-04 22:52-INFO->> Step 393110 run_train: loss = 5.4326  (0.129 sec)
18-06-04 22:52-INFO->> Step 393120 run_train: loss = 5.4575  (0.168 sec)
18-06-04 22:52-INFO->> Step 393130 run_train: loss = 5.4656  (0.173 sec)
18-06-04 22:52-INFO->> Step 393140 run_train: loss = 5.3351  (0.185 sec)
18-06-04 22:52-INFO->> Step 393150 run_train: loss = 5.4801  (0.125 sec)
18-06-04 22:52-INFO->> Step 393160 run_train: loss = 5.4168  (0.168 sec)
18-06-04 22:52-INFO->> Step 393170 run_train: loss = 5.4227  (0.127 sec)
18-06-04 22:52-INFO->> Step 393180 run_train: loss = 5.4598  (0.136 sec)
18-06-04 22:52-INFO->> Step 393190 run_train: loss = 5.4010  (0.160 sec)
18-06-04 22:52-INFO->> Step 393200 run_train: loss = 5.3313  (0.141 sec)
18-06-04 22:53-INFO->> Step 393210 run_train: loss = 5.4173  (0.191 sec)
18-06-04 22:53-INFO->> Step 393220 run_train: loss = 5.4094  (0.152 sec)
18-06-04 22:53-INFO->> Step 393230 run_train: loss = 5.3971  (0.161 sec)
18-06-04 22:53-INFO->> Step 393240 run_train: loss = 5.4415  (0.141 sec)
18-06-04 22:53-INFO->> Step 393250 run_train: loss = 5.4361  (0.188 sec)
18-06-04 22:53-INFO->> Step 393260 run_train: loss = 5.5238  (0.163 sec)
18-06-04 22:53-INFO->> Step 393270 run_train: loss = 5.4316  (0.161 sec)
18-06-04 22:53-INFO->> Step 393280 run_train: loss = 5.3942  (0.150 sec)
18-06-04 22:53-INFO->> Step 393290 run_train: loss = 5.4250  (0.155 sec)
18-06-04 22:53-INFO->> Step 393300 run_train: loss = 5.3917  (0.140 sec)
18-06-04 22:53-INFO->> Step 393310 run_train: loss = 5.3867  (0.162 sec)
18-06-04 22:53-INFO->> Step 393320 run_train: loss = 5.3744  (0.141 sec)
18-06-04 22:53-INFO->> Step 393330 run_train: loss = 5.3870  (0.168 sec)
18-06-04 22:53-INFO->> Step 393340 run_train: loss = 5.3839  (0.175 sec)
18-06-04 22:53-INFO->> Step 393350 run_train: loss = 5.4074  (0.131 sec)
18-06-04 22:53-INFO->> Step 393360 run_train: loss = 5.4320  (0.193 sec)
18-06-04 22:53-INFO->> Step 393370 run_train: loss = 5.4117  (0.164 sec)
18-06-04 22:53-INFO->> Step 393380 run_train: loss = 5.4147  (0.152 sec)
18-06-04 22:53-INFO->> Step 393390 run_train: loss = 5.4945  (0.138 sec)
18-06-04 22:53-INFO->> Step 393400 run_train: loss = 5.4125  (0.135 sec)
18-06-04 22:53-INFO->> Step 393410 run_train: loss = 5.3703  (0.133 sec)
18-06-04 22:53-INFO->> Step 393420 run_train: loss = 5.4820  (0.167 sec)
18-06-04 22:53-INFO->> Step 393430 run_train: loss = 5.4254  (0.152 sec)
18-06-04 22:53-INFO->> Step 393440 run_train: loss = 5.3088  (0.176 sec)
18-06-04 22:53-INFO->> Step 393450 run_train: loss = 5.4326  (0.147 sec)
18-06-04 22:53-INFO->> Step 393460 run_train: loss = 5.4053  (0.144 sec)
18-06-04 22:53-INFO->> Step 393470 run_train: loss = 5.4495  (0.185 sec)
18-06-04 22:53-INFO->> Step 393480 run_train: loss = 5.4613  (0.183 sec)
18-06-04 22:53-INFO->> Step 393490 run_train: loss = 5.3891  (0.191 sec)
18-06-04 22:53-INFO->> Step 393500 run_train: loss = 5.3913  (0.140 sec)
18-06-04 22:53-INFO->> Step 393510 run_train: loss = 5.4659  (0.167 sec)
18-06-04 22:53-INFO->> Step 393520 run_train: loss = 5.4625  (0.160 sec)
18-06-04 22:53-INFO->> Step 393530 run_train: loss = 5.4890  (0.186 sec)
18-06-04 22:53-INFO->> Step 393540 run_train: loss = 5.3554  (0.115 sec)
18-06-04 22:53-INFO->> Step 393550 run_train: loss = 5.5089  (0.191 sec)
18-06-04 22:53-INFO->> Step 393560 run_train: loss = 5.4412  (0.131 sec)
18-06-04 22:53-INFO->> Step 393570 run_train: loss = 5.4470  (0.153 sec)
18-06-04 22:54-INFO->> Step 393580 run_train: loss = 5.4454  (0.159 sec)
18-06-04 22:54-INFO->> Step 393590 run_train: loss = 5.4194  (0.160 sec)
18-06-04 22:54-INFO->> Step 393600 run_train: loss = 5.4156  (0.129 sec)
18-06-04 22:54-INFO->> Step 393610 run_train: loss = 5.3862  (0.150 sec)
18-06-04 22:54-INFO->> Step 393620 run_train: loss = 5.4495  (0.133 sec)
18-06-04 22:54-INFO->> Step 393630 run_train: loss = 5.4168  (0.180 sec)
18-06-04 22:54-INFO->> Step 393640 run_train: loss = 5.4898  (0.176 sec)
18-06-04 22:54-INFO->> Step 393650 run_train: loss = 5.4286  (0.131 sec)
18-06-04 22:54-INFO->> Step 393660 run_train: loss = 5.3652  (0.160 sec)
18-06-04 22:54-INFO->> Step 393670 run_train: loss = 5.4354  (0.200 sec)
18-06-04 22:54-INFO->> Step 393680 run_train: loss = 5.3829  (0.150 sec)
18-06-04 22:54-INFO->> Step 393690 run_train: loss = 5.4650  (0.153 sec)
18-06-04 22:54-INFO->> Step 393700 run_train: loss = 5.4400  (0.190 sec)
18-06-04 22:54-INFO->> Step 393710 run_train: loss = 5.4392  (0.176 sec)
18-06-04 22:54-INFO->> Step 393720 run_train: loss = 5.3915  (0.160 sec)
18-06-04 22:54-INFO->> Step 393730 run_train: loss = 5.4286  (0.165 sec)
18-06-04 22:54-INFO->> Step 393740 run_train: loss = 5.4252  (0.150 sec)
18-06-04 22:54-INFO->> Step 393750 run_train: loss = 5.4462  (0.160 sec)
18-06-04 22:54-INFO->> Step 393760 run_train: loss = 5.4368  (0.178 sec)
18-06-04 22:54-INFO->> Step 393770 run_train: loss = 5.3739  (0.159 sec)
18-06-04 22:54-INFO->> Step 393780 run_train: loss = 5.4587  (0.181 sec)
18-06-04 22:54-INFO->> Step 393790 run_train: loss = 5.4060  (0.158 sec)
18-06-04 22:54-INFO->> Step 393800 run_train: loss = 5.3598  (0.161 sec)
18-06-04 22:54-INFO->> Step 393810 run_train: loss = 5.4179  (0.185 sec)
18-06-04 22:54-INFO->> Step 393820 run_train: loss = 5.4542  (0.181 sec)
18-06-04 22:54-INFO->> Step 393830 run_train: loss = 5.3993  (0.158 sec)
18-06-04 22:54-INFO->> Step 393840 run_train: loss = 5.4137  (0.170 sec)
18-06-04 22:54-INFO->> Step 393850 run_train: loss = 5.4198  (0.142 sec)
18-06-04 22:54-INFO->> Step 393860 run_train: loss = 5.4532  (0.161 sec)
18-06-04 22:54-INFO->> Step 393870 run_train: loss = 5.4440  (0.149 sec)
18-06-04 22:54-INFO->> Step 393880 run_train: loss = 5.4446  (0.152 sec)
18-06-04 22:54-INFO->> Step 393890 run_train: loss = 5.5115  (0.186 sec)
18-06-04 22:54-INFO->> Step 393900 run_train: loss = 5.4680  (0.168 sec)
18-06-04 22:54-INFO->> Step 393910 run_train: loss = 5.4186  (0.193 sec)
18-06-04 22:54-INFO->> Step 393920 run_train: loss = 5.4555  (0.133 sec)
18-06-04 22:54-INFO->> Step 393930 run_train: loss = 5.4771  (0.160 sec)
18-06-04 22:54-INFO->> Step 393940 run_train: loss = 5.4463  (0.175 sec)
18-06-04 22:55-INFO->> Step 393950 run_train: loss = 5.4057  (0.158 sec)
18-06-04 22:55-INFO->> Step 393960 run_train: loss = 5.4103  (0.158 sec)
18-06-04 22:55-INFO->> Step 393970 run_train: loss = 5.4463  (0.162 sec)
18-06-04 22:55-INFO->> Step 393980 run_train: loss = 5.4267  (0.180 sec)
18-06-04 22:55-INFO->> Step 393990 run_train: loss = 5.4708  (0.178 sec)
18-06-04 22:55-INFO->> Step 394000 run_train: loss = 5.4603  (0.188 sec)
18-06-04 22:55-INFO->> 2018-06-04 22:55:08.763664 Saving in ckpt
18-06-04 22:55-INFO-Test Data Eval:
18-06-04 22:55-INFO-fpr95 = 0.16905220510095642 and auc = 0.9694427955607272
18-06-04 22:55-INFO->> Step 394010 run_train: loss = 5.4464  (0.148 sec)
18-06-04 22:55-INFO->> Step 394020 run_train: loss = 5.4286  (0.171 sec)
18-06-04 22:55-INFO->> Step 394030 run_train: loss = 5.3782  (0.194 sec)
18-06-04 22:55-INFO->> Step 394040 run_train: loss = 5.3602  (0.193 sec)
18-06-04 22:55-INFO->> Step 394050 run_train: loss = 5.4497  (0.205 sec)
18-06-04 22:55-INFO->> Step 394060 run_train: loss = 5.4231  (0.159 sec)
18-06-04 22:56-INFO->> Step 394070 run_train: loss = 5.4267  (0.143 sec)
18-06-04 22:56-INFO->> Step 394080 run_train: loss = 5.4348  (0.157 sec)
18-06-04 22:56-INFO->> Step 394090 run_train: loss = 5.3919  (0.148 sec)
18-06-04 22:56-INFO->> Step 394100 run_train: loss = 5.4601  (0.135 sec)
18-06-04 22:56-INFO->> Step 394110 run_train: loss = 5.3757  (0.150 sec)
18-06-04 22:56-INFO->> Step 394120 run_train: loss = 5.4495  (0.153 sec)
18-06-04 22:56-INFO->> Step 394130 run_train: loss = 5.4207  (0.156 sec)
18-06-04 22:56-INFO->> Step 394140 run_train: loss = 5.4634  (0.155 sec)
18-06-04 22:56-INFO->> Step 394150 run_train: loss = 5.4674  (0.156 sec)
18-06-04 22:56-INFO->> Step 394160 run_train: loss = 5.4811  (0.157 sec)
18-06-04 22:56-INFO->> Step 394170 run_train: loss = 5.3884  (0.175 sec)
18-06-04 22:56-INFO->> Step 394180 run_train: loss = 5.3904  (0.164 sec)
18-06-04 22:56-INFO->> Step 394190 run_train: loss = 5.4255  (0.162 sec)
18-06-04 22:56-INFO->> Step 394200 run_train: loss = 5.4505  (0.231 sec)
18-06-04 22:56-INFO->> Step 394210 run_train: loss = 5.4370  (0.170 sec)
18-06-04 22:56-INFO->> Step 394220 run_train: loss = 5.3703  (0.196 sec)
18-06-04 22:56-INFO->> Step 394230 run_train: loss = 5.3682  (0.165 sec)
18-06-04 22:56-INFO->> Step 394240 run_train: loss = 5.4376  (0.150 sec)
18-06-04 22:56-INFO->> Step 394250 run_train: loss = 5.4255  (0.173 sec)
18-06-04 22:56-INFO->> Step 394260 run_train: loss = 5.4495  (0.178 sec)
18-06-04 22:56-INFO->> Step 394270 run_train: loss = 5.3985  (0.159 sec)
18-06-04 22:56-INFO->> Step 394280 run_train: loss = 5.4439  (0.170 sec)
18-06-04 22:56-INFO->> Step 394290 run_train: loss = 5.3916  (0.170 sec)
18-06-04 22:56-INFO->> Step 394300 run_train: loss = 5.4650  (0.138 sec)
18-06-04 22:56-INFO->> Step 394310 run_train: loss = 5.3944  (0.162 sec)
18-06-04 22:56-INFO->> Step 394320 run_train: loss = 5.4569  (0.183 sec)
18-06-04 22:56-INFO->> Step 394330 run_train: loss = 5.3976  (0.143 sec)
18-06-04 22:56-INFO->> Step 394340 run_train: loss = 5.4390  (0.143 sec)
18-06-04 22:56-INFO->> Step 394350 run_train: loss = 5.3929  (0.175 sec)
18-06-04 22:56-INFO->> Step 394360 run_train: loss = 5.4378  (0.158 sec)
18-06-04 22:56-INFO->> Step 394370 run_train: loss = 5.3969  (0.166 sec)
18-06-04 22:56-INFO->> Step 394380 run_train: loss = 5.4130  (0.177 sec)
18-06-04 22:56-INFO->> Step 394390 run_train: loss = 5.4097  (0.191 sec)
18-06-04 22:56-INFO->> Step 394400 run_train: loss = 5.4551  (0.189 sec)
18-06-04 22:56-INFO->> Step 394410 run_train: loss = 5.4507  (0.140 sec)
18-06-04 22:56-INFO->> Step 394420 run_train: loss = 5.4254  (0.158 sec)
18-06-04 22:56-INFO->> Step 394430 run_train: loss = 5.3873  (0.183 sec)
18-06-04 22:57-INFO->> Step 394440 run_train: loss = 5.4872  (0.202 sec)
18-06-04 22:57-INFO->> Step 394450 run_train: loss = 5.4398  (0.181 sec)
18-06-04 22:57-INFO->> Step 394460 run_train: loss = 5.4206  (0.236 sec)
18-06-04 22:57-INFO->> Step 394470 run_train: loss = 5.3859  (0.168 sec)
18-06-04 22:57-INFO->> Step 394480 run_train: loss = 5.3722  (0.154 sec)
18-06-04 22:57-INFO->> Step 394490 run_train: loss = 5.4493  (0.159 sec)
18-06-04 22:57-INFO->> Step 394500 run_train: loss = 5.4140  (0.154 sec)
18-06-04 22:57-INFO->> Step 394510 run_train: loss = 5.4569  (0.166 sec)
18-06-04 22:57-INFO->> Step 394520 run_train: loss = 5.4391  (0.150 sec)
18-06-04 22:57-INFO->> Step 394530 run_train: loss = 5.4467  (0.161 sec)
18-06-04 22:57-INFO->> Step 394540 run_train: loss = 5.4026  (0.142 sec)
18-06-04 22:57-INFO->> Step 394550 run_train: loss = 5.4497  (0.163 sec)
18-06-04 22:57-INFO->> Step 394560 run_train: loss = 5.4447  (0.153 sec)
18-06-04 22:57-INFO->> Step 394570 run_train: loss = 5.4044  (0.171 sec)
18-06-04 22:57-INFO->> Step 394580 run_train: loss = 5.4577  (0.146 sec)
18-06-04 22:57-INFO->> Step 394590 run_train: loss = 5.4179  (0.139 sec)
18-06-04 22:57-INFO->> Step 394600 run_train: loss = 5.4431  (0.163 sec)
18-06-04 22:57-INFO->> Step 394610 run_train: loss = 5.4528  (0.179 sec)
18-06-04 22:57-INFO->> Step 394620 run_train: loss = 5.4540  (0.180 sec)
18-06-04 22:57-INFO->> Step 394630 run_train: loss = 5.4475  (0.127 sec)
18-06-04 22:57-INFO->> Step 394640 run_train: loss = 5.4145  (0.155 sec)
18-06-04 22:57-INFO->> Step 394650 run_train: loss = 5.3658  (0.126 sec)
18-06-04 22:57-INFO->> Step 394660 run_train: loss = 5.3768  (0.195 sec)
18-06-04 22:57-INFO->> Step 394670 run_train: loss = 5.4709  (0.142 sec)
18-06-04 22:57-INFO->> Step 394680 run_train: loss = 5.3761  (0.177 sec)
18-06-04 22:57-INFO->> Step 394690 run_train: loss = 5.4240  (0.119 sec)
18-06-04 22:57-INFO->> Step 394700 run_train: loss = 5.4584  (0.150 sec)
18-06-04 22:57-INFO->> Step 394710 run_train: loss = 5.4131  (0.145 sec)
18-06-04 22:57-INFO->> Step 394720 run_train: loss = 5.4289  (0.148 sec)
18-06-04 22:57-INFO->> Step 394730 run_train: loss = 5.4446  (0.167 sec)
18-06-04 22:57-INFO->> Step 394740 run_train: loss = 5.3770  (0.155 sec)
18-06-04 22:57-INFO->> Step 394750 run_train: loss = 5.5078  (0.158 sec)
18-06-04 22:57-INFO->> Step 394760 run_train: loss = 5.4407  (0.173 sec)
18-06-04 22:57-INFO->> Step 394770 run_train: loss = 5.4067  (0.171 sec)
18-06-04 22:57-INFO->> Step 394780 run_train: loss = 5.3922  (0.170 sec)
18-06-04 22:57-INFO->> Step 394790 run_train: loss = 5.4299  (0.174 sec)
18-06-04 22:57-INFO->> Step 394800 run_train: loss = 5.3862  (0.181 sec)
18-06-04 22:58-INFO->> Step 394810 run_train: loss = 5.4373  (0.181 sec)
18-06-04 22:58-INFO->> Step 394820 run_train: loss = 5.4216  (0.152 sec)
18-06-04 22:58-INFO->> Step 394830 run_train: loss = 5.4129  (0.172 sec)
18-06-04 22:58-INFO->> Step 394840 run_train: loss = 5.3943  (0.146 sec)
18-06-04 22:58-INFO->> Step 394850 run_train: loss = 5.4218  (0.165 sec)
18-06-04 22:58-INFO->> Step 394860 run_train: loss = 5.3988  (0.160 sec)
18-06-04 22:58-INFO->> Step 394870 run_train: loss = 5.3862  (0.152 sec)
18-06-04 22:58-INFO->> Step 394880 run_train: loss = 5.4582  (0.164 sec)
18-06-04 22:58-INFO->> Step 394890 run_train: loss = 5.4685  (0.153 sec)
18-06-04 22:58-INFO->> Step 394900 run_train: loss = 5.4175  (0.177 sec)
18-06-04 22:58-INFO->> Step 394910 run_train: loss = 5.3705  (0.164 sec)
18-06-04 22:58-INFO->> Step 394920 run_train: loss = 5.3975  (0.117 sec)
18-06-04 22:58-INFO->> Step 394930 run_train: loss = 5.4495  (0.160 sec)
18-06-04 22:58-INFO->> Step 394940 run_train: loss = 5.4741  (0.141 sec)
18-06-04 22:58-INFO->> Step 394950 run_train: loss = 5.4572  (0.144 sec)
18-06-04 22:58-INFO->> Step 394960 run_train: loss = 5.3889  (0.185 sec)
18-06-04 22:58-INFO->> Step 394970 run_train: loss = 5.4275  (0.173 sec)
18-06-04 22:58-INFO->> Step 394980 run_train: loss = 5.3987  (0.142 sec)
18-06-04 22:58-INFO->> Step 394990 run_train: loss = 5.3856  (0.147 sec)
18-06-04 22:58-INFO->> Step 395000 run_train: loss = 5.4359  (0.131 sec)
18-06-04 22:58-INFO->> 2018-06-04 22:58:30.744555 Saving in ckpt
18-06-04 22:58-INFO-Test Data Eval:
18-06-04 22:59-INFO-fpr95 = 0.1742826780021254 and auc = 0.9691501831933806
18-06-04 22:59-INFO->> Step 395010 run_train: loss = 5.4374  (0.172 sec)
18-06-04 22:59-INFO->> Step 395020 run_train: loss = 5.3920  (0.176 sec)
18-06-04 22:59-INFO->> Step 395030 run_train: loss = 5.4012  (0.154 sec)
18-06-04 22:59-INFO->> Step 395040 run_train: loss = 5.4664  (0.151 sec)
18-06-04 22:59-INFO->> Step 395050 run_train: loss = 5.4130  (0.165 sec)
18-06-04 22:59-INFO->> Step 395060 run_train: loss = 5.4413  (0.153 sec)
18-06-04 22:59-INFO->> Step 395070 run_train: loss = 5.4857  (0.177 sec)
18-06-04 22:59-INFO->> Step 395080 run_train: loss = 5.4426  (0.139 sec)
18-06-04 22:59-INFO->> Step 395090 run_train: loss = 5.4792  (0.129 sec)
18-06-04 22:59-INFO->> Step 395100 run_train: loss = 5.4265  (0.123 sec)
18-06-04 22:59-INFO->> Step 395110 run_train: loss = 5.4088  (0.150 sec)
18-06-04 22:59-INFO->> Step 395120 run_train: loss = 5.4473  (0.187 sec)
18-06-04 22:59-INFO->> Step 395130 run_train: loss = 5.4424  (0.144 sec)
18-06-04 22:59-INFO->> Step 395140 run_train: loss = 5.3821  (0.118 sec)
18-06-04 22:59-INFO->> Step 395150 run_train: loss = 5.3906  (0.156 sec)
18-06-04 22:59-INFO->> Step 395160 run_train: loss = 5.4534  (0.135 sec)
18-06-04 22:59-INFO->> Step 395170 run_train: loss = 5.4382  (0.142 sec)
18-06-04 22:59-INFO->> Step 395180 run_train: loss = 5.5077  (0.171 sec)
18-06-04 22:59-INFO->> Step 395190 run_train: loss = 5.4311  (0.184 sec)
18-06-04 22:59-INFO->> Step 395200 run_train: loss = 5.3948  (0.164 sec)
18-06-04 22:59-INFO->> Step 395210 run_train: loss = 5.4416  (0.164 sec)
18-06-04 22:59-INFO->> Step 395220 run_train: loss = 5.4021  (0.185 sec)
18-06-04 22:59-INFO->> Step 395230 run_train: loss = 5.4418  (0.138 sec)
18-06-04 22:59-INFO->> Step 395240 run_train: loss = 5.4572  (0.159 sec)
18-06-04 22:59-INFO->> Step 395250 run_train: loss = 5.4236  (0.198 sec)
18-06-04 22:59-INFO->> Step 395260 run_train: loss = 5.3977  (0.156 sec)
18-06-04 22:59-INFO->> Step 395270 run_train: loss = 5.4068  (0.168 sec)
18-06-04 22:59-INFO->> Step 395280 run_train: loss = 5.3724  (0.116 sec)
18-06-04 22:59-INFO->> Step 395290 run_train: loss = 5.4836  (0.131 sec)
18-06-04 23:00-INFO->> Step 395300 run_train: loss = 5.3933  (0.157 sec)
18-06-04 23:00-INFO->> Step 395310 run_train: loss = 5.4038  (0.162 sec)
18-06-04 23:00-INFO->> Step 395320 run_train: loss = 5.4032  (0.179 sec)
18-06-04 23:00-INFO->> Step 395330 run_train: loss = 5.4215  (0.146 sec)
18-06-04 23:00-INFO->> Step 395340 run_train: loss = 5.4079  (0.153 sec)
18-06-04 23:00-INFO->> Step 395350 run_train: loss = 5.4959  (0.169 sec)
18-06-04 23:00-INFO->> Step 395360 run_train: loss = 5.4594  (0.166 sec)
18-06-04 23:00-INFO->> Step 395370 run_train: loss = 5.4608  (0.170 sec)
18-06-04 23:00-INFO->> Step 395380 run_train: loss = 5.4598  (0.191 sec)
18-06-04 23:00-INFO->> Step 395390 run_train: loss = 5.4365  (0.124 sec)
18-06-04 23:00-INFO->> Step 395400 run_train: loss = 5.4100  (0.153 sec)
18-06-04 23:00-INFO->> Step 395410 run_train: loss = 5.3918  (0.162 sec)
18-06-04 23:00-INFO->> Step 395420 run_train: loss = 5.4505  (0.138 sec)
18-06-04 23:00-INFO->> Step 395430 run_train: loss = 5.4340  (0.157 sec)
18-06-04 23:00-INFO->> Step 395440 run_train: loss = 5.3275  (0.138 sec)
18-06-04 23:00-INFO->> Step 395450 run_train: loss = 5.4655  (0.183 sec)
18-06-04 23:00-INFO->> Step 395460 run_train: loss = 5.4373  (0.100 sec)
18-06-04 23:00-INFO->> Step 395470 run_train: loss = 5.4096  (0.153 sec)
18-06-04 23:00-INFO->> Step 395480 run_train: loss = 5.3947  (0.171 sec)
18-06-04 23:00-INFO->> Step 395490 run_train: loss = 5.3996  (0.174 sec)
18-06-04 23:00-INFO->> Step 395500 run_train: loss = 5.4072  (0.165 sec)
18-06-04 23:00-INFO->> Step 395510 run_train: loss = 5.4855  (0.157 sec)
18-06-04 23:00-INFO->> Step 395520 run_train: loss = 5.4156  (0.140 sec)
18-06-04 23:00-INFO->> Step 395530 run_train: loss = 5.4371  (0.163 sec)
18-06-04 23:00-INFO->> Step 395540 run_train: loss = 5.4579  (0.202 sec)
18-06-04 23:00-INFO->> Step 395550 run_train: loss = 5.4272  (0.170 sec)
18-06-04 23:00-INFO->> Step 395560 run_train: loss = 5.4232  (0.166 sec)
18-06-04 23:00-INFO->> Step 395570 run_train: loss = 5.4291  (0.179 sec)
18-06-04 23:00-INFO->> Step 395580 run_train: loss = 5.4611  (0.143 sec)
18-06-04 23:00-INFO->> Step 395590 run_train: loss = 5.4270  (0.188 sec)
18-06-04 23:00-INFO->> Step 395600 run_train: loss = 5.4476  (0.138 sec)
18-06-04 23:00-INFO->> Step 395610 run_train: loss = 5.4209  (0.177 sec)
18-06-04 23:00-INFO->> Step 395620 run_train: loss = 5.3446  (0.174 sec)
18-06-04 23:00-INFO->> Step 395630 run_train: loss = 5.3887  (0.189 sec)
18-06-04 23:00-INFO->> Step 395640 run_train: loss = 5.3823  (0.153 sec)
18-06-04 23:00-INFO->> Step 395650 run_train: loss = 5.4750  (0.156 sec)
18-06-04 23:00-INFO->> Step 395660 run_train: loss = 5.4257  (0.140 sec)
18-06-04 23:00-INFO->> Step 395670 run_train: loss = 5.4405  (0.173 sec)
18-06-04 23:01-INFO->> Step 395680 run_train: loss = 5.3931  (0.148 sec)
18-06-04 23:01-INFO->> Step 395690 run_train: loss = 5.4326  (0.147 sec)
18-06-04 23:01-INFO->> Step 395700 run_train: loss = 5.4625  (0.165 sec)
18-06-04 23:01-INFO->> Step 395710 run_train: loss = 5.3603  (0.170 sec)
18-06-04 23:01-INFO->> Step 395720 run_train: loss = 5.4228  (0.141 sec)
18-06-04 23:01-INFO->> Step 395730 run_train: loss = 5.4301  (0.165 sec)
18-06-04 23:01-INFO->> Step 395740 run_train: loss = 5.5107  (0.156 sec)
18-06-04 23:01-INFO->> Step 395750 run_train: loss = 5.4442  (0.143 sec)
18-06-04 23:01-INFO->> Step 395760 run_train: loss = 5.4287  (0.168 sec)
18-06-04 23:01-INFO->> Step 395770 run_train: loss = 5.4798  (0.155 sec)
18-06-04 23:01-INFO->> Step 395780 run_train: loss = 5.4576  (0.126 sec)
18-06-04 23:01-INFO->> Step 395790 run_train: loss = 5.3818  (0.143 sec)
18-06-04 23:01-INFO->> Step 395800 run_train: loss = 5.4444  (0.187 sec)
18-06-04 23:01-INFO->> Step 395810 run_train: loss = 5.4246  (0.124 sec)
18-06-04 23:01-INFO->> Step 395820 run_train: loss = 5.4293  (0.160 sec)
18-06-04 23:01-INFO->> Step 395830 run_train: loss = 5.3609  (0.162 sec)
18-06-04 23:01-INFO->> Step 395840 run_train: loss = 5.3942  (0.147 sec)
18-06-04 23:01-INFO->> Step 395850 run_train: loss = 5.4558  (0.112 sec)
18-06-04 23:01-INFO->> Step 395860 run_train: loss = 5.3882  (0.176 sec)
18-06-04 23:01-INFO->> Step 395870 run_train: loss = 5.4413  (0.138 sec)
18-06-04 23:01-INFO->> Step 395880 run_train: loss = 5.4788  (0.152 sec)
18-06-04 23:01-INFO->> Step 395890 run_train: loss = 5.4489  (0.137 sec)
18-06-04 23:01-INFO->> Step 395900 run_train: loss = 5.4228  (0.173 sec)
18-06-04 23:01-INFO->> Step 395910 run_train: loss = 5.4000  (0.189 sec)
18-06-04 23:01-INFO->> Step 395920 run_train: loss = 5.4122  (0.167 sec)
18-06-04 23:01-INFO->> Step 395930 run_train: loss = 5.4069  (0.096 sec)
18-06-04 23:01-INFO->> Step 395940 run_train: loss = 5.4531  (0.184 sec)
18-06-04 23:01-INFO->> Step 395950 run_train: loss = 5.4454  (0.146 sec)
18-06-04 23:01-INFO->> Step 395960 run_train: loss = 5.4741  (0.184 sec)
18-06-04 23:01-INFO->> Step 395970 run_train: loss = 5.4613  (0.152 sec)
18-06-04 23:01-INFO->> Step 395980 run_train: loss = 5.4675  (0.157 sec)
18-06-04 23:01-INFO->> Step 395990 run_train: loss = 5.4697  (0.128 sec)
18-06-04 23:01-INFO->> Step 396000 run_train: loss = 5.4488  (0.139 sec)
18-06-04 23:01-INFO->> 2018-06-04 23:01:52.497147 Saving in ckpt
18-06-04 23:01-INFO-Test Data Eval:
18-06-04 23:02-INFO-fpr95 = 0.17581861052072265 and auc = 0.9690450869118137
18-06-04 23:02-INFO->> Step 396010 run_train: loss = 5.3705  (0.151 sec)
18-06-04 23:02-INFO->> Step 396020 run_train: loss = 5.4357  (0.133 sec)
18-06-04 23:02-INFO->> Step 396030 run_train: loss = 5.4548  (0.159 sec)
18-06-04 23:02-INFO->> Step 396040 run_train: loss = 5.4427  (0.146 sec)
18-06-04 23:02-INFO->> Step 396050 run_train: loss = 5.4030  (0.145 sec)
18-06-04 23:02-INFO->> Step 396060 run_train: loss = 5.4584  (0.169 sec)
18-06-04 23:02-INFO->> Step 396070 run_train: loss = 5.4621  (0.190 sec)
18-06-04 23:02-INFO->> Step 396080 run_train: loss = 5.3606  (0.131 sec)
18-06-04 23:02-INFO->> Step 396090 run_train: loss = 5.4596  (0.173 sec)
18-06-04 23:02-INFO->> Step 396100 run_train: loss = 5.4659  (0.203 sec)
18-06-04 23:02-INFO->> Step 396110 run_train: loss = 5.4945  (0.132 sec)
18-06-04 23:02-INFO->> Step 396120 run_train: loss = 5.3299  (0.146 sec)
18-06-04 23:02-INFO->> Step 396130 run_train: loss = 5.4430  (0.173 sec)
18-06-04 23:02-INFO->> Step 396140 run_train: loss = 5.3866  (0.146 sec)
18-06-04 23:02-INFO->> Step 396150 run_train: loss = 5.4195  (0.142 sec)
18-06-04 23:02-INFO->> Step 396160 run_train: loss = 5.4370  (0.183 sec)
18-06-04 23:03-INFO->> Step 396170 run_train: loss = 5.4462  (0.175 sec)
18-06-04 23:03-INFO->> Step 396180 run_train: loss = 5.3965  (0.158 sec)
18-06-04 23:03-INFO->> Step 396190 run_train: loss = 5.4244  (0.125 sec)
18-06-04 23:03-INFO->> Step 396200 run_train: loss = 5.4513  (0.158 sec)
18-06-04 23:03-INFO->> Step 396210 run_train: loss = 5.4083  (0.201 sec)
18-06-04 23:03-INFO->> Step 396220 run_train: loss = 5.4063  (0.177 sec)
18-06-04 23:03-INFO->> Step 396230 run_train: loss = 5.4236  (0.183 sec)
18-06-04 23:03-INFO->> Step 396240 run_train: loss = 5.4823  (0.143 sec)
18-06-04 23:03-INFO->> Step 396250 run_train: loss = 5.4002  (0.188 sec)
18-06-04 23:03-INFO->> Step 396260 run_train: loss = 5.3968  (0.160 sec)
18-06-04 23:03-INFO->> Step 396270 run_train: loss = 5.4695  (0.186 sec)
18-06-04 23:03-INFO->> Step 396280 run_train: loss = 5.4299  (0.156 sec)
18-06-04 23:03-INFO->> Step 396290 run_train: loss = 5.4461  (0.133 sec)
18-06-04 23:03-INFO->> Step 396300 run_train: loss = 5.4116  (0.142 sec)
18-06-04 23:03-INFO->> Step 396310 run_train: loss = 5.3964  (0.195 sec)
18-06-04 23:03-INFO->> Step 396320 run_train: loss = 5.4731  (0.147 sec)
18-06-04 23:03-INFO->> Step 396330 run_train: loss = 5.4447  (0.182 sec)
18-06-04 23:03-INFO->> Step 396340 run_train: loss = 5.3342  (0.188 sec)
18-06-04 23:03-INFO->> Step 396350 run_train: loss = 5.3934  (0.171 sec)
18-06-04 23:03-INFO->> Step 396360 run_train: loss = 5.4362  (0.125 sec)
18-06-04 23:03-INFO->> Step 396370 run_train: loss = 5.4614  (0.182 sec)
18-06-04 23:03-INFO->> Step 396380 run_train: loss = 5.3678  (0.153 sec)
18-06-04 23:03-INFO->> Step 396390 run_train: loss = 5.4927  (0.166 sec)
18-06-04 23:03-INFO->> Step 396400 run_train: loss = 5.4064  (0.192 sec)
18-06-04 23:03-INFO->> Step 396410 run_train: loss = 5.4231  (0.146 sec)
18-06-04 23:03-INFO->> Step 396420 run_train: loss = 5.3574  (0.129 sec)
18-06-04 23:03-INFO->> Step 396430 run_train: loss = 5.4348  (0.150 sec)
18-06-04 23:03-INFO->> Step 396440 run_train: loss = 5.4102  (0.194 sec)
18-06-04 23:03-INFO->> Step 396450 run_train: loss = 5.3701  (0.181 sec)
18-06-04 23:03-INFO->> Step 396460 run_train: loss = 5.3715  (0.155 sec)
18-06-04 23:03-INFO->> Step 396470 run_train: loss = 5.3859  (0.149 sec)
18-06-04 23:03-INFO->> Step 396480 run_train: loss = 5.4098  (0.184 sec)
18-06-04 23:03-INFO->> Step 396490 run_train: loss = 5.4563  (0.150 sec)
18-06-04 23:03-INFO->> Step 396500 run_train: loss = 5.4460  (0.151 sec)
18-06-04 23:03-INFO->> Step 396510 run_train: loss = 5.4454  (0.144 sec)
18-06-04 23:03-INFO->> Step 396520 run_train: loss = 5.4233  (0.166 sec)
18-06-04 23:03-INFO->> Step 396530 run_train: loss = 5.4085  (0.146 sec)
18-06-04 23:03-INFO->> Step 396540 run_train: loss = 5.4802  (0.134 sec)
18-06-04 23:04-INFO->> Step 396550 run_train: loss = 5.4462  (0.203 sec)
18-06-04 23:04-INFO->> Step 396560 run_train: loss = 5.4740  (0.105 sec)
18-06-04 23:04-INFO->> Step 396570 run_train: loss = 5.4520  (0.136 sec)
18-06-04 23:04-INFO->> Step 396580 run_train: loss = 5.4120  (0.176 sec)
18-06-04 23:04-INFO->> Step 396590 run_train: loss = 5.4378  (0.168 sec)
18-06-04 23:04-INFO->> Step 396600 run_train: loss = 5.4765  (0.141 sec)
18-06-04 23:04-INFO->> Step 396610 run_train: loss = 5.4255  (0.145 sec)
18-06-04 23:04-INFO->> Step 396620 run_train: loss = 5.4042  (0.160 sec)
18-06-04 23:04-INFO->> Step 396630 run_train: loss = 5.4383  (0.148 sec)
18-06-04 23:04-INFO->> Step 396640 run_train: loss = 5.3641  (0.136 sec)
18-06-04 23:04-INFO->> Step 396650 run_train: loss = 5.4433  (0.182 sec)
18-06-04 23:04-INFO->> Step 396660 run_train: loss = 5.3369  (0.132 sec)
18-06-04 23:04-INFO->> Step 396670 run_train: loss = 5.4148  (0.168 sec)
18-06-04 23:04-INFO->> Step 396680 run_train: loss = 5.4190  (0.187 sec)
18-06-04 23:04-INFO->> Step 396690 run_train: loss = 5.3950  (0.170 sec)
18-06-04 23:04-INFO->> Step 396700 run_train: loss = 5.4098  (0.168 sec)
18-06-04 23:04-INFO->> Step 396710 run_train: loss = 5.4985  (0.147 sec)
18-06-04 23:04-INFO->> Step 396720 run_train: loss = 5.3811  (0.174 sec)
18-06-04 23:04-INFO->> Step 396730 run_train: loss = 5.4387  (0.133 sec)
18-06-04 23:04-INFO->> Step 396740 run_train: loss = 5.4386  (0.162 sec)
18-06-04 23:04-INFO->> Step 396750 run_train: loss = 5.4365  (0.170 sec)
18-06-04 23:04-INFO->> Step 396760 run_train: loss = 5.3070  (0.171 sec)
18-06-04 23:04-INFO->> Step 396770 run_train: loss = 5.5082  (0.153 sec)
18-06-04 23:04-INFO->> Step 396780 run_train: loss = 5.3991  (0.151 sec)
18-06-04 23:04-INFO->> Step 396790 run_train: loss = 5.4092  (0.148 sec)
18-06-04 23:04-INFO->> Step 396800 run_train: loss = 5.4047  (0.140 sec)
18-06-04 23:04-INFO->> Step 396810 run_train: loss = 5.4022  (0.149 sec)
18-06-04 23:04-INFO->> Step 396820 run_train: loss = 5.3630  (0.161 sec)
18-06-04 23:04-INFO->> Step 396830 run_train: loss = 5.4383  (0.180 sec)
18-06-04 23:04-INFO->> Step 396840 run_train: loss = 5.4177  (0.115 sec)
18-06-04 23:04-INFO->> Step 396850 run_train: loss = 5.3694  (0.157 sec)
18-06-04 23:04-INFO->> Step 396860 run_train: loss = 5.4293  (0.171 sec)
18-06-04 23:04-INFO->> Step 396870 run_train: loss = 5.4350  (0.167 sec)
18-06-04 23:04-INFO->> Step 396880 run_train: loss = 5.4890  (0.181 sec)
18-06-04 23:04-INFO->> Step 396890 run_train: loss = 5.4537  (0.184 sec)
18-06-04 23:04-INFO->> Step 396900 run_train: loss = 5.4212  (0.166 sec)
18-06-04 23:04-INFO->> Step 396910 run_train: loss = 5.4753  (0.182 sec)
18-06-04 23:04-INFO->> Step 396920 run_train: loss = 5.3921  (0.186 sec)
18-06-04 23:05-INFO->> Step 396930 run_train: loss = 5.3964  (0.208 sec)
18-06-04 23:05-INFO->> Step 396940 run_train: loss = 5.4246  (0.156 sec)
18-06-04 23:05-INFO->> Step 396950 run_train: loss = 5.4309  (0.199 sec)
18-06-04 23:05-INFO->> Step 396960 run_train: loss = 5.4339  (0.167 sec)
18-06-04 23:05-INFO->> Step 396970 run_train: loss = 5.3999  (0.195 sec)
18-06-04 23:05-INFO->> Step 396980 run_train: loss = 5.4755  (0.168 sec)
18-06-04 23:05-INFO->> Step 396990 run_train: loss = 5.4189  (0.158 sec)
18-06-04 23:05-INFO->> Step 397000 run_train: loss = 5.4779  (0.172 sec)
18-06-04 23:05-INFO->> 2018-06-04 23:05:12.544477 Saving in ckpt
18-06-04 23:05-INFO-Test Data Eval:
18-06-04 23:05-INFO-fpr95 = 0.17083720775770456 and auc = 0.9694715890223964
18-06-04 23:05-INFO->> Step 397010 run_train: loss = 5.4125  (0.201 sec)
18-06-04 23:05-INFO->> Step 397020 run_train: loss = 5.4135  (0.166 sec)
18-06-04 23:05-INFO->> Step 397030 run_train: loss = 5.3692  (0.203 sec)
18-06-04 23:06-INFO->> Step 397040 run_train: loss = 5.4249  (0.156 sec)
18-06-04 23:06-INFO->> Step 397050 run_train: loss = 5.3741  (0.186 sec)
18-06-04 23:06-INFO->> Step 397060 run_train: loss = 5.4117  (0.155 sec)
18-06-04 23:06-INFO->> Step 397070 run_train: loss = 5.3625  (0.201 sec)
18-06-04 23:06-INFO->> Step 397080 run_train: loss = 5.3749  (0.163 sec)
18-06-04 23:06-INFO->> Step 397090 run_train: loss = 5.4366  (0.167 sec)
18-06-04 23:06-INFO->> Step 397100 run_train: loss = 5.4084  (0.166 sec)
18-06-04 23:06-INFO->> Step 397110 run_train: loss = 5.4289  (0.167 sec)
18-06-04 23:06-INFO->> Step 397120 run_train: loss = 5.3818  (0.138 sec)
18-06-04 23:06-INFO->> Step 397130 run_train: loss = 5.4353  (0.168 sec)
18-06-04 23:06-INFO->> Step 397140 run_train: loss = 5.4120  (0.173 sec)
18-06-04 23:06-INFO->> Step 397150 run_train: loss = 5.4403  (0.169 sec)
18-06-04 23:06-INFO->> Step 397160 run_train: loss = 5.4341  (0.133 sec)
18-06-04 23:06-INFO->> Step 397170 run_train: loss = 5.4606  (0.163 sec)
18-06-04 23:06-INFO->> Step 397180 run_train: loss = 5.3996  (0.164 sec)
18-06-04 23:06-INFO->> Step 397190 run_train: loss = 5.4454  (0.170 sec)
18-06-04 23:06-INFO->> Step 397200 run_train: loss = 5.4526  (0.137 sec)
18-06-04 23:06-INFO->> Step 397210 run_train: loss = 5.4150  (0.153 sec)
18-06-04 23:06-INFO->> Step 397220 run_train: loss = 5.4219  (0.162 sec)
18-06-04 23:06-INFO->> Step 397230 run_train: loss = 5.4806  (0.164 sec)
18-06-04 23:06-INFO->> Step 397240 run_train: loss = 5.4736  (0.177 sec)
18-06-04 23:06-INFO->> Step 397250 run_train: loss = 5.4407  (0.188 sec)
18-06-04 23:06-INFO->> Step 397260 run_train: loss = 5.4322  (0.165 sec)
18-06-04 23:06-INFO->> Step 397270 run_train: loss = 5.4409  (0.140 sec)
18-06-04 23:06-INFO->> Step 397280 run_train: loss = 5.4223  (0.152 sec)
18-06-04 23:06-INFO->> Step 397290 run_train: loss = 5.4196  (0.143 sec)
18-06-04 23:06-INFO->> Step 397300 run_train: loss = 5.4295  (0.158 sec)
18-06-04 23:06-INFO->> Step 397310 run_train: loss = 5.4888  (0.165 sec)
18-06-04 23:06-INFO->> Step 397320 run_train: loss = 5.4111  (0.160 sec)
18-06-04 23:06-INFO->> Step 397330 run_train: loss = 5.4907  (0.161 sec)
18-06-04 23:06-INFO->> Step 397340 run_train: loss = 5.4172  (0.131 sec)
18-06-04 23:06-INFO->> Step 397350 run_train: loss = 5.4264  (0.166 sec)
18-06-04 23:06-INFO->> Step 397360 run_train: loss = 5.3908  (0.186 sec)
18-06-04 23:06-INFO->> Step 397370 run_train: loss = 5.4427  (0.139 sec)
18-06-04 23:06-INFO->> Step 397380 run_train: loss = 5.4011  (0.164 sec)
18-06-04 23:06-INFO->> Step 397390 run_train: loss = 5.4428  (0.113 sec)
18-06-04 23:06-INFO->> Step 397400 run_train: loss = 5.3955  (0.150 sec)
18-06-04 23:06-INFO->> Step 397410 run_train: loss = 5.4061  (0.167 sec)
18-06-04 23:07-INFO->> Step 397420 run_train: loss = 5.4271  (0.161 sec)
18-06-04 23:07-INFO->> Step 397430 run_train: loss = 5.4288  (0.158 sec)
18-06-04 23:07-INFO->> Step 397440 run_train: loss = 5.4257  (0.189 sec)
18-06-04 23:07-INFO->> Step 397450 run_train: loss = 5.4623  (0.172 sec)
18-06-04 23:07-INFO->> Step 397460 run_train: loss = 5.5157  (0.155 sec)
18-06-04 23:07-INFO->> Step 397470 run_train: loss = 5.4572  (0.158 sec)
18-06-04 23:07-INFO->> Step 397480 run_train: loss = 5.4455  (0.174 sec)
18-06-04 23:07-INFO->> Step 397490 run_train: loss = 5.3935  (0.166 sec)
18-06-04 23:07-INFO->> Step 397500 run_train: loss = 5.4065  (0.120 sec)
18-06-04 23:07-INFO->> Step 397510 run_train: loss = 5.4238  (0.153 sec)
18-06-04 23:07-INFO->> Step 397520 run_train: loss = 5.4627  (0.191 sec)
18-06-04 23:07-INFO->> Step 397530 run_train: loss = 5.4122  (0.131 sec)
18-06-04 23:07-INFO->> Step 397540 run_train: loss = 5.3736  (0.166 sec)
18-06-04 23:07-INFO->> Step 397550 run_train: loss = 5.3541  (0.152 sec)
18-06-04 23:07-INFO->> Step 397560 run_train: loss = 5.4639  (0.156 sec)
18-06-04 23:07-INFO->> Step 397570 run_train: loss = 5.4236  (0.175 sec)
18-06-04 23:07-INFO->> Step 397580 run_train: loss = 5.4006  (0.146 sec)
18-06-04 23:07-INFO->> Step 397590 run_train: loss = 5.4515  (0.188 sec)
18-06-04 23:07-INFO->> Step 397600 run_train: loss = 5.3906  (0.158 sec)
18-06-04 23:07-INFO->> Step 397610 run_train: loss = 5.4465  (0.147 sec)
18-06-04 23:07-INFO->> Step 397620 run_train: loss = 5.4912  (0.168 sec)
18-06-04 23:07-INFO->> Step 397630 run_train: loss = 5.4322  (0.176 sec)
18-06-04 23:07-INFO->> Step 397640 run_train: loss = 5.3929  (0.152 sec)
18-06-04 23:07-INFO->> Step 397650 run_train: loss = 5.3990  (0.157 sec)
18-06-04 23:07-INFO->> Step 397660 run_train: loss = 5.4151  (0.162 sec)
18-06-04 23:07-INFO->> Step 397670 run_train: loss = 5.4267  (0.159 sec)
18-06-04 23:07-INFO->> Step 397680 run_train: loss = 5.4099  (0.181 sec)
18-06-04 23:07-INFO->> Step 397690 run_train: loss = 5.4309  (0.139 sec)
18-06-04 23:07-INFO->> Step 397700 run_train: loss = 5.4380  (0.119 sec)
18-06-04 23:07-INFO->> Step 397710 run_train: loss = 5.4947  (0.148 sec)
18-06-04 23:07-INFO->> Step 397720 run_train: loss = 5.3878  (0.194 sec)
18-06-04 23:07-INFO->> Step 397730 run_train: loss = 5.4126  (0.109 sec)
18-06-04 23:07-INFO->> Step 397740 run_train: loss = 5.4551  (0.171 sec)
18-06-04 23:07-INFO->> Step 397750 run_train: loss = 5.4251  (0.132 sec)
18-06-04 23:07-INFO->> Step 397760 run_train: loss = 5.3627  (0.160 sec)
18-06-04 23:07-INFO->> Step 397770 run_train: loss = 5.4307  (0.151 sec)
18-06-04 23:07-INFO->> Step 397780 run_train: loss = 5.4227  (0.147 sec)
18-06-04 23:08-INFO->> Step 397790 run_train: loss = 5.4585  (0.165 sec)
18-06-04 23:08-INFO->> Step 397800 run_train: loss = 5.3821  (0.183 sec)
18-06-04 23:08-INFO->> Step 397810 run_train: loss = 5.4732  (0.174 sec)
18-06-04 23:08-INFO->> Step 397820 run_train: loss = 5.4523  (0.149 sec)
18-06-04 23:08-INFO->> Step 397830 run_train: loss = 5.4606  (0.158 sec)
18-06-04 23:08-INFO->> Step 397840 run_train: loss = 5.4897  (0.165 sec)
18-06-04 23:08-INFO->> Step 397850 run_train: loss = 5.4462  (0.165 sec)
18-06-04 23:08-INFO->> Step 397860 run_train: loss = 5.3992  (0.133 sec)
18-06-04 23:08-INFO->> Step 397870 run_train: loss = 5.4829  (0.154 sec)
18-06-04 23:08-INFO->> Step 397880 run_train: loss = 5.4319  (0.184 sec)
18-06-04 23:08-INFO->> Step 397890 run_train: loss = 5.3743  (0.169 sec)
18-06-04 23:08-INFO->> Step 397900 run_train: loss = 5.4454  (0.214 sec)
18-06-04 23:08-INFO->> Step 397910 run_train: loss = 5.4465  (0.182 sec)
18-06-04 23:08-INFO->> Step 397920 run_train: loss = 5.3781  (0.165 sec)
18-06-04 23:08-INFO->> Step 397930 run_train: loss = 5.4483  (0.136 sec)
18-06-04 23:08-INFO->> Step 397940 run_train: loss = 5.4127  (0.138 sec)
18-06-04 23:08-INFO->> Step 397950 run_train: loss = 5.3998  (0.161 sec)
18-06-04 23:08-INFO->> Step 397960 run_train: loss = 5.4572  (0.220 sec)
18-06-04 23:08-INFO->> Step 397970 run_train: loss = 5.4653  (0.175 sec)
18-06-04 23:08-INFO->> Step 397980 run_train: loss = 5.4039  (0.145 sec)
18-06-04 23:08-INFO->> Step 397990 run_train: loss = 5.4526  (0.136 sec)
18-06-04 23:08-INFO->> Step 398000 run_train: loss = 5.4134  (0.156 sec)
18-06-04 23:08-INFO->> 2018-06-04 23:08:33.570436 Saving in ckpt
18-06-04 23:08-INFO-Test Data Eval:
18-06-04 23:09-INFO-fpr95 = 0.17516272582359194 and auc = 0.9689243188024913
18-06-04 23:09-INFO->> Step 398010 run_train: loss = 5.4293  (0.164 sec)
18-06-04 23:09-INFO->> Step 398020 run_train: loss = 5.4696  (0.138 sec)
18-06-04 23:09-INFO->> Step 398030 run_train: loss = 5.4304  (0.151 sec)
18-06-04 23:09-INFO->> Step 398040 run_train: loss = 5.4579  (0.129 sec)
18-06-04 23:09-INFO->> Step 398050 run_train: loss = 5.4171  (0.153 sec)
18-06-04 23:09-INFO->> Step 398060 run_train: loss = 5.4657  (0.167 sec)
18-06-04 23:09-INFO->> Step 398070 run_train: loss = 5.4733  (0.157 sec)
18-06-04 23:09-INFO->> Step 398080 run_train: loss = 5.4579  (0.204 sec)
18-06-04 23:09-INFO->> Step 398090 run_train: loss = 5.4651  (0.160 sec)
18-06-04 23:09-INFO->> Step 398100 run_train: loss = 5.4360  (0.145 sec)
18-06-04 23:09-INFO->> Step 398110 run_train: loss = 5.4759  (0.153 sec)
18-06-04 23:09-INFO->> Step 398120 run_train: loss = 5.4939  (0.126 sec)
18-06-04 23:09-INFO->> Step 398130 run_train: loss = 5.4163  (0.176 sec)
18-06-04 23:09-INFO->> Step 398140 run_train: loss = 5.4206  (0.134 sec)
18-06-04 23:09-INFO->> Step 398150 run_train: loss = 5.4221  (0.149 sec)
18-06-04 23:09-INFO->> Step 398160 run_train: loss = 5.3488  (0.151 sec)
18-06-04 23:09-INFO->> Step 398170 run_train: loss = 5.4013  (0.156 sec)
18-06-04 23:09-INFO->> Step 398180 run_train: loss = 5.4232  (0.183 sec)
18-06-04 23:09-INFO->> Step 398190 run_train: loss = 5.4085  (0.185 sec)
18-06-04 23:09-INFO->> Step 398200 run_train: loss = 5.4160  (0.153 sec)
18-06-04 23:09-INFO->> Step 398210 run_train: loss = 5.3865  (0.153 sec)
18-06-04 23:09-INFO->> Step 398220 run_train: loss = 5.4724  (0.136 sec)
18-06-04 23:09-INFO->> Step 398230 run_train: loss = 5.4773  (0.139 sec)
18-06-04 23:09-INFO->> Step 398240 run_train: loss = 5.4359  (0.187 sec)
18-06-04 23:09-INFO->> Step 398250 run_train: loss = 5.3866  (0.189 sec)
18-06-04 23:09-INFO->> Step 398260 run_train: loss = 5.3980  (0.148 sec)
18-06-04 23:09-INFO->> Step 398270 run_train: loss = 5.4021  (0.154 sec)
18-06-04 23:09-INFO->> Step 398280 run_train: loss = 5.4658  (0.163 sec)
18-06-04 23:10-INFO->> Step 398290 run_train: loss = 5.3834  (0.134 sec)
18-06-04 23:10-INFO->> Step 398300 run_train: loss = 5.4692  (0.140 sec)
18-06-04 23:10-INFO->> Step 398310 run_train: loss = 5.4291  (0.159 sec)
18-06-04 23:10-INFO->> Step 398320 run_train: loss = 5.4295  (0.162 sec)
18-06-04 23:10-INFO->> Step 398330 run_train: loss = 5.4272  (0.160 sec)
18-06-04 23:10-INFO->> Step 398340 run_train: loss = 5.4438  (0.153 sec)
18-06-04 23:10-INFO->> Step 398350 run_train: loss = 5.3854  (0.167 sec)
18-06-04 23:10-INFO->> Step 398360 run_train: loss = 5.5066  (0.153 sec)
18-06-04 23:10-INFO->> Step 398370 run_train: loss = 5.4025  (0.156 sec)
18-06-04 23:10-INFO->> Step 398380 run_train: loss = 5.4312  (0.156 sec)
18-06-04 23:10-INFO->> Step 398390 run_train: loss = 5.4026  (0.154 sec)
18-06-04 23:10-INFO->> Step 398400 run_train: loss = 5.4484  (0.146 sec)
18-06-04 23:10-INFO->> Step 398410 run_train: loss = 5.4592  (0.162 sec)
18-06-04 23:10-INFO->> Step 398420 run_train: loss = 5.5039  (0.136 sec)
18-06-04 23:10-INFO->> Step 398430 run_train: loss = 5.4465  (0.162 sec)
18-06-04 23:10-INFO->> Step 398440 run_train: loss = 5.4358  (0.152 sec)
18-06-04 23:10-INFO->> Step 398450 run_train: loss = 5.4105  (0.148 sec)
18-06-04 23:10-INFO->> Step 398460 run_train: loss = 5.5125  (0.149 sec)
18-06-04 23:10-INFO->> Step 398470 run_train: loss = 5.3490  (0.163 sec)
18-06-04 23:10-INFO->> Step 398480 run_train: loss = 5.3538  (0.158 sec)
18-06-04 23:10-INFO->> Step 398490 run_train: loss = 5.4083  (0.192 sec)
18-06-04 23:10-INFO->> Step 398500 run_train: loss = 5.4519  (0.171 sec)
18-06-04 23:10-INFO->> Step 398510 run_train: loss = 5.3908  (0.157 sec)
18-06-04 23:10-INFO->> Step 398520 run_train: loss = 5.4554  (0.161 sec)
18-06-04 23:10-INFO->> Step 398530 run_train: loss = 5.4790  (0.161 sec)
18-06-04 23:10-INFO->> Step 398540 run_train: loss = 5.4169  (0.195 sec)
18-06-04 23:10-INFO->> Step 398550 run_train: loss = 5.4843  (0.152 sec)
18-06-04 23:10-INFO->> Step 398560 run_train: loss = 5.4063  (0.161 sec)
18-06-04 23:10-INFO->> Step 398570 run_train: loss = 5.4728  (0.133 sec)
18-06-04 23:10-INFO->> Step 398580 run_train: loss = 5.4409  (0.187 sec)
18-06-04 23:10-INFO->> Step 398590 run_train: loss = 5.3537  (0.195 sec)
18-06-04 23:10-INFO->> Step 398600 run_train: loss = 5.4342  (0.165 sec)
18-06-04 23:10-INFO->> Step 398610 run_train: loss = 5.4090  (0.137 sec)
18-06-04 23:10-INFO->> Step 398620 run_train: loss = 5.3613  (0.146 sec)
18-06-04 23:10-INFO->> Step 398630 run_train: loss = 5.4420  (0.159 sec)
18-06-04 23:10-INFO->> Step 398640 run_train: loss = 5.4204  (0.144 sec)
18-06-04 23:10-INFO->> Step 398650 run_train: loss = 5.4382  (0.181 sec)
18-06-04 23:11-INFO->> Step 398660 run_train: loss = 5.3998  (0.172 sec)
18-06-04 23:11-INFO->> Step 398670 run_train: loss = 5.4752  (0.145 sec)
18-06-04 23:11-INFO->> Step 398680 run_train: loss = 5.4398  (0.147 sec)
18-06-04 23:11-INFO->> Step 398690 run_train: loss = 5.4299  (0.153 sec)
18-06-04 23:11-INFO->> Step 398700 run_train: loss = 5.3840  (0.147 sec)
18-06-04 23:11-INFO->> Step 398710 run_train: loss = 5.3381  (0.176 sec)
18-06-04 23:11-INFO->> Step 398720 run_train: loss = 5.4454  (0.156 sec)
18-06-04 23:11-INFO->> Step 398730 run_train: loss = 5.4772  (0.183 sec)
18-06-04 23:11-INFO->> Step 398740 run_train: loss = 5.4161  (0.175 sec)
18-06-04 23:11-INFO->> Step 398750 run_train: loss = 5.3604  (0.119 sec)
18-06-04 23:11-INFO->> Step 398760 run_train: loss = 5.4273  (0.158 sec)
18-06-04 23:11-INFO->> Step 398770 run_train: loss = 5.3919  (0.148 sec)
18-06-04 23:11-INFO->> Step 398780 run_train: loss = 5.4493  (0.178 sec)
18-06-04 23:11-INFO->> Step 398790 run_train: loss = 5.3707  (0.169 sec)
18-06-04 23:11-INFO->> Step 398800 run_train: loss = 5.4493  (0.135 sec)
18-06-04 23:11-INFO->> Step 398810 run_train: loss = 5.3360  (0.136 sec)
18-06-04 23:11-INFO->> Step 398820 run_train: loss = 5.4003  (0.134 sec)
18-06-04 23:11-INFO->> Step 398830 run_train: loss = 5.4542  (0.178 sec)
18-06-04 23:11-INFO->> Step 398840 run_train: loss = 5.4340  (0.158 sec)
18-06-04 23:11-INFO->> Step 398850 run_train: loss = 5.4838  (0.155 sec)
18-06-04 23:11-INFO->> Step 398860 run_train: loss = 5.4100  (0.146 sec)
18-06-04 23:11-INFO->> Step 398870 run_train: loss = 5.4386  (0.160 sec)
18-06-04 23:11-INFO->> Step 398880 run_train: loss = 5.4105  (0.144 sec)
18-06-04 23:11-INFO->> Step 398890 run_train: loss = 5.4611  (0.130 sec)
18-06-04 23:11-INFO->> Step 398900 run_train: loss = 5.4194  (0.158 sec)
18-06-04 23:11-INFO->> Step 398910 run_train: loss = 5.3751  (0.158 sec)
18-06-04 23:11-INFO->> Step 398920 run_train: loss = 5.4370  (0.116 sec)
18-06-04 23:11-INFO->> Step 398930 run_train: loss = 5.3929  (0.148 sec)
18-06-04 23:11-INFO->> Step 398940 run_train: loss = 5.4624  (0.156 sec)
18-06-04 23:11-INFO->> Step 398950 run_train: loss = 5.4394  (0.134 sec)
18-06-04 23:11-INFO->> Step 398960 run_train: loss = 5.4627  (0.158 sec)
18-06-04 23:11-INFO->> Step 398970 run_train: loss = 5.4045  (0.169 sec)
18-06-04 23:11-INFO->> Step 398980 run_train: loss = 5.3982  (0.157 sec)
18-06-04 23:11-INFO->> Step 398990 run_train: loss = 5.3800  (0.161 sec)
18-06-04 23:11-INFO->> Step 399000 run_train: loss = 5.4449  (0.170 sec)
18-06-04 23:11-INFO->> 2018-06-04 23:11:54.564698 Saving in ckpt
18-06-04 23:11-INFO-Test Data Eval:
18-06-04 23:12-INFO-fpr95 = 0.1767318676939426 and auc = 0.9688037866365035
18-06-04 23:12-INFO->> Step 399010 run_train: loss = 5.4437  (0.155 sec)
18-06-04 23:12-INFO->> Step 399020 run_train: loss = 5.3649  (0.178 sec)
18-06-04 23:12-INFO->> Step 399030 run_train: loss = 5.4607  (0.163 sec)
18-06-04 23:12-INFO->> Step 399040 run_train: loss = 5.3762  (0.170 sec)
18-06-04 23:12-INFO->> Step 399050 run_train: loss = 5.3880  (0.162 sec)
18-06-04 23:12-INFO->> Step 399060 run_train: loss = 5.4079  (0.150 sec)
18-06-04 23:12-INFO->> Step 399070 run_train: loss = 5.4523  (0.175 sec)
18-06-04 23:12-INFO->> Step 399080 run_train: loss = 5.4163  (0.146 sec)
18-06-04 23:12-INFO->> Step 399090 run_train: loss = 5.4725  (0.213 sec)
18-06-04 23:12-INFO->> Step 399100 run_train: loss = 5.4881  (0.132 sec)
18-06-04 23:12-INFO->> Step 399110 run_train: loss = 5.4240  (0.143 sec)
18-06-04 23:12-INFO->> Step 399120 run_train: loss = 5.4793  (0.136 sec)
18-06-04 23:12-INFO->> Step 399130 run_train: loss = 5.4194  (0.150 sec)
18-06-04 23:12-INFO->> Step 399140 run_train: loss = 5.4012  (0.168 sec)
18-06-04 23:13-INFO->> Step 399150 run_train: loss = 5.3800  (0.137 sec)
18-06-04 23:13-INFO->> Step 399160 run_train: loss = 5.3958  (0.126 sec)
18-06-04 23:13-INFO->> Step 399170 run_train: loss = 5.5133  (0.150 sec)
18-06-04 23:13-INFO->> Step 399180 run_train: loss = 5.4874  (0.176 sec)
18-06-04 23:13-INFO->> Step 399190 run_train: loss = 5.4067  (0.195 sec)
18-06-04 23:13-INFO->> Step 399200 run_train: loss = 5.3520  (0.135 sec)
18-06-04 23:13-INFO->> Step 399210 run_train: loss = 5.4119  (0.185 sec)
18-06-04 23:13-INFO->> Step 399220 run_train: loss = 5.4552  (0.144 sec)
18-06-04 23:13-INFO->> Step 399230 run_train: loss = 5.4016  (0.172 sec)
18-06-04 23:13-INFO->> Step 399240 run_train: loss = 5.3841  (0.171 sec)
18-06-04 23:13-INFO->> Step 399250 run_train: loss = 5.4529  (0.169 sec)
18-06-04 23:13-INFO->> Step 399260 run_train: loss = 5.4417  (0.182 sec)
18-06-04 23:13-INFO->> Step 399270 run_train: loss = 5.4234  (0.169 sec)
18-06-04 23:13-INFO->> Step 399280 run_train: loss = 5.4285  (0.152 sec)
18-06-04 23:13-INFO->> Step 399290 run_train: loss = 5.4714  (0.176 sec)
18-06-04 23:13-INFO->> Step 399300 run_train: loss = 5.4857  (0.171 sec)
18-06-04 23:13-INFO->> Step 399310 run_train: loss = 5.3733  (0.162 sec)
18-06-04 23:13-INFO->> Step 399320 run_train: loss = 5.4584  (0.170 sec)
18-06-04 23:13-INFO->> Step 399330 run_train: loss = 5.3713  (0.144 sec)
18-06-04 23:13-INFO->> Step 399340 run_train: loss = 5.4410  (0.155 sec)
18-06-04 23:13-INFO->> Step 399350 run_train: loss = 5.4050  (0.178 sec)
18-06-04 23:13-INFO->> Step 399360 run_train: loss = 5.4366  (0.157 sec)
18-06-04 23:13-INFO->> Step 399370 run_train: loss = 5.4428  (0.181 sec)
18-06-04 23:13-INFO->> Step 399380 run_train: loss = 5.3602  (0.169 sec)
18-06-04 23:13-INFO->> Step 399390 run_train: loss = 5.3546  (0.166 sec)
18-06-04 23:13-INFO->> Step 399400 run_train: loss = 5.3556  (0.169 sec)
18-06-04 23:13-INFO->> Step 399410 run_train: loss = 5.4546  (0.166 sec)
18-06-04 23:13-INFO->> Step 399420 run_train: loss = 5.4218  (0.174 sec)
18-06-04 23:13-INFO->> Step 399430 run_train: loss = 5.4624  (0.160 sec)
18-06-04 23:13-INFO->> Step 399440 run_train: loss = 5.4945  (0.161 sec)
18-06-04 23:13-INFO->> Step 399450 run_train: loss = 5.4644  (0.141 sec)
18-06-04 23:13-INFO->> Step 399460 run_train: loss = 5.4491  (0.164 sec)
18-06-04 23:13-INFO->> Step 399470 run_train: loss = 5.4651  (0.156 sec)
18-06-04 23:13-INFO->> Step 399480 run_train: loss = 5.4109  (0.151 sec)
18-06-04 23:13-INFO->> Step 399490 run_train: loss = 5.4091  (0.190 sec)
18-06-04 23:13-INFO->> Step 399500 run_train: loss = 5.3950  (0.156 sec)
18-06-04 23:13-INFO->> Step 399510 run_train: loss = 5.4512  (0.163 sec)
18-06-04 23:13-INFO->> Step 399520 run_train: loss = 5.3764  (0.181 sec)
18-06-04 23:14-INFO->> Step 399530 run_train: loss = 5.4816  (0.177 sec)
18-06-04 23:14-INFO->> Step 399540 run_train: loss = 5.4547  (0.178 sec)
18-06-04 23:14-INFO->> Step 399550 run_train: loss = 5.4292  (0.147 sec)
18-06-04 23:14-INFO->> Step 399560 run_train: loss = 5.4767  (0.138 sec)
18-06-04 23:14-INFO->> Step 399570 run_train: loss = 5.4233  (0.181 sec)
18-06-04 23:14-INFO->> Step 399580 run_train: loss = 5.4218  (0.154 sec)
18-06-04 23:14-INFO->> Step 399590 run_train: loss = 5.4412  (0.207 sec)
18-06-04 23:14-INFO->> Step 399600 run_train: loss = 5.4525  (0.166 sec)
18-06-04 23:14-INFO->> Step 399610 run_train: loss = 5.3812  (0.172 sec)
18-06-04 23:14-INFO->> Step 399620 run_train: loss = 5.4378  (0.152 sec)
18-06-04 23:14-INFO->> Step 399630 run_train: loss = 5.3572  (0.163 sec)
18-06-04 23:14-INFO->> Step 399640 run_train: loss = 5.4304  (0.151 sec)
18-06-04 23:14-INFO->> Step 399650 run_train: loss = 5.3915  (0.169 sec)
18-06-04 23:14-INFO->> Step 399660 run_train: loss = 5.3881  (0.161 sec)
18-06-04 23:14-INFO->> Step 399670 run_train: loss = 5.4525  (0.185 sec)
18-06-04 23:14-INFO->> Step 399680 run_train: loss = 5.3827  (0.167 sec)
18-06-04 23:14-INFO->> Step 399690 run_train: loss = 5.4433  (0.141 sec)
18-06-04 23:14-INFO->> Step 399700 run_train: loss = 5.4854  (0.143 sec)
18-06-04 23:14-INFO->> Step 399710 run_train: loss = 5.4528  (0.155 sec)
18-06-04 23:14-INFO->> Step 399720 run_train: loss = 5.3884  (0.161 sec)
18-06-04 23:14-INFO->> Step 399730 run_train: loss = 5.5110  (0.200 sec)
18-06-04 23:14-INFO->> Step 399740 run_train: loss = 5.4162  (0.150 sec)
18-06-04 23:14-INFO->> Step 399750 run_train: loss = 5.3857  (0.154 sec)
18-06-04 23:14-INFO->> Step 399760 run_train: loss = 5.4028  (0.157 sec)
18-06-04 23:14-INFO->> Step 399770 run_train: loss = 5.4372  (0.159 sec)
18-06-04 23:14-INFO->> Step 399780 run_train: loss = 5.3771  (0.117 sec)
18-06-04 23:14-INFO->> Step 399790 run_train: loss = 5.3845  (0.143 sec)
18-06-04 23:14-INFO->> Step 399800 run_train: loss = 5.3437  (0.175 sec)
18-06-04 23:14-INFO->> Step 399810 run_train: loss = 5.4163  (0.158 sec)
18-06-04 23:14-INFO->> Step 399820 run_train: loss = 5.4287  (0.160 sec)
18-06-04 23:14-INFO->> Step 399830 run_train: loss = 5.4698  (0.163 sec)
18-06-04 23:14-INFO->> Step 399840 run_train: loss = 5.4207  (0.149 sec)
18-06-04 23:14-INFO->> Step 399850 run_train: loss = 5.3883  (0.209 sec)
18-06-04 23:14-INFO->> Step 399860 run_train: loss = 5.3861  (0.116 sec)
18-06-04 23:14-INFO->> Step 399870 run_train: loss = 5.4654  (0.138 sec)
18-06-04 23:14-INFO->> Step 399880 run_train: loss = 5.3286  (0.180 sec)
18-06-04 23:14-INFO->> Step 399890 run_train: loss = 5.4178  (0.158 sec)
18-06-04 23:14-INFO->> Step 399900 run_train: loss = 5.4777  (0.154 sec)
18-06-04 23:15-INFO->> Step 399910 run_train: loss = 5.4128  (0.156 sec)
18-06-04 23:15-INFO->> Step 399920 run_train: loss = 5.4124  (0.177 sec)
18-06-04 23:15-INFO->> Step 399930 run_train: loss = 5.4232  (0.144 sec)
18-06-04 23:15-INFO->> Step 399940 run_train: loss = 5.3915  (0.146 sec)
18-06-04 23:15-INFO->> Step 399950 run_train: loss = 5.4111  (0.186 sec)
18-06-04 23:15-INFO->> Step 399960 run_train: loss = 5.3985  (0.125 sec)
18-06-04 23:15-INFO->> Step 399970 run_train: loss = 5.4374  (0.133 sec)
18-06-04 23:15-INFO->> Step 399980 run_train: loss = 5.4336  (0.126 sec)
18-06-04 23:15-INFO->> Step 399990 run_train: loss = 5.5157  (0.166 sec)
18-06-04 23:15-INFO->> Step 400000 run_train: loss = 5.4131  (0.181 sec)
18-06-04 23:15-INFO->> 2018-06-04 23:15:15.808833 Saving in ckpt
18-06-04 23:15-INFO-Test Data Eval:
18-06-04 23:15-INFO-fpr95 = 0.168653692879915 and auc = 0.9693282972164392
18-06-04 23:15-INFO->> Step 400010 run_train: loss = 5.3883  (0.165 sec)
18-06-04 23:15-INFO->> Step 400020 run_train: loss = 5.3969  (0.152 sec)
18-06-04 23:16-INFO->> Step 400030 run_train: loss = 5.4695  (0.185 sec)
18-06-04 23:16-INFO->> Step 400040 run_train: loss = 5.4049  (0.175 sec)
18-06-04 23:16-INFO->> Step 400050 run_train: loss = 5.5136  (0.137 sec)
18-06-04 23:16-INFO->> Step 400060 run_train: loss = 5.3966  (0.157 sec)
18-06-04 23:16-INFO->> Step 400070 run_train: loss = 5.4715  (0.150 sec)
18-06-04 23:16-INFO->> Step 400080 run_train: loss = 5.4072  (0.161 sec)
18-06-04 23:16-INFO->> Step 400090 run_train: loss = 5.4217  (0.172 sec)
18-06-04 23:16-INFO->> Step 400100 run_train: loss = 5.4698  (0.156 sec)
18-06-04 23:16-INFO->> Step 400110 run_train: loss = 5.4053  (0.144 sec)
18-06-04 23:16-INFO->> Step 400120 run_train: loss = 5.3963  (0.206 sec)
18-06-04 23:16-INFO->> Step 400130 run_train: loss = 5.4234  (0.146 sec)
18-06-04 23:16-INFO->> Step 400140 run_train: loss = 5.3883  (0.179 sec)
18-06-04 23:16-INFO->> Step 400150 run_train: loss = 5.5159  (0.140 sec)
18-06-04 23:16-INFO->> Step 400160 run_train: loss = 5.4477  (0.170 sec)
18-06-04 23:16-INFO->> Step 400170 run_train: loss = 5.4370  (0.213 sec)
18-06-04 23:16-INFO->> Step 400180 run_train: loss = 5.4444  (0.175 sec)
18-06-04 23:16-INFO->> Step 400190 run_train: loss = 5.4164  (0.164 sec)
18-06-04 23:16-INFO->> Step 400200 run_train: loss = 5.4409  (0.148 sec)
18-06-04 23:16-INFO->> Step 400210 run_train: loss = 5.3589  (0.175 sec)
18-06-04 23:16-INFO->> Step 400220 run_train: loss = 5.4539  (0.160 sec)
18-06-04 23:16-INFO->> Step 400230 run_train: loss = 5.4488  (0.157 sec)
18-06-04 23:16-INFO->> Step 400240 run_train: loss = 5.4186  (0.114 sec)
18-06-04 23:16-INFO->> Step 400250 run_train: loss = 5.4224  (0.150 sec)
18-06-04 23:16-INFO->> Step 400260 run_train: loss = 5.4051  (0.141 sec)
18-06-04 23:16-INFO->> Step 400270 run_train: loss = 5.4740  (0.190 sec)
18-06-04 23:16-INFO->> Step 400280 run_train: loss = 5.3845  (0.161 sec)
18-06-04 23:16-INFO->> Step 400290 run_train: loss = 5.4372  (0.126 sec)
18-06-04 23:16-INFO->> Step 400300 run_train: loss = 5.4031  (0.179 sec)
18-06-04 23:16-INFO->> Step 400310 run_train: loss = 5.4236  (0.169 sec)
18-06-04 23:16-INFO->> Step 400320 run_train: loss = 5.4335  (0.167 sec)
18-06-04 23:16-INFO->> Step 400330 run_train: loss = 5.4189  (0.180 sec)
18-06-04 23:16-INFO->> Step 400340 run_train: loss = 5.3513  (0.149 sec)
18-06-04 23:16-INFO->> Step 400350 run_train: loss = 5.4610  (0.160 sec)
18-06-04 23:16-INFO->> Step 400360 run_train: loss = 5.3953  (0.134 sec)
18-06-04 23:16-INFO->> Step 400370 run_train: loss = 5.4146  (0.142 sec)
18-06-04 23:16-INFO->> Step 400380 run_train: loss = 5.5095  (0.152 sec)
18-06-04 23:16-INFO->> Step 400390 run_train: loss = 5.3985  (0.168 sec)
18-06-04 23:17-INFO->> Step 400400 run_train: loss = 5.4500  (0.161 sec)
18-06-04 23:17-INFO->> Step 400410 run_train: loss = 5.4554  (0.169 sec)
18-06-04 23:17-INFO->> Step 400420 run_train: loss = 5.4388  (0.153 sec)
18-06-04 23:17-INFO->> Step 400430 run_train: loss = 5.3821  (0.149 sec)
18-06-04 23:17-INFO->> Step 400440 run_train: loss = 5.4049  (0.170 sec)
18-06-04 23:17-INFO->> Step 400450 run_train: loss = 5.4248  (0.129 sec)
18-06-04 23:17-INFO->> Step 400460 run_train: loss = 5.3817  (0.140 sec)
18-06-04 23:17-INFO->> Step 400470 run_train: loss = 5.4561  (0.139 sec)
18-06-04 23:17-INFO->> Step 400480 run_train: loss = 5.4605  (0.167 sec)
18-06-04 23:17-INFO->> Step 400490 run_train: loss = 5.5041  (0.154 sec)
18-06-04 23:17-INFO->> Step 400500 run_train: loss = 5.4467  (0.146 sec)
18-06-04 23:17-INFO->> Step 400510 run_train: loss = 5.4437  (0.129 sec)
18-06-04 23:17-INFO->> Step 400520 run_train: loss = 5.4603  (0.160 sec)
18-06-04 23:17-INFO->> Step 400530 run_train: loss = 5.3995  (0.180 sec)
18-06-04 23:17-INFO->> Step 400540 run_train: loss = 5.3589  (0.156 sec)
18-06-04 23:17-INFO->> Step 400550 run_train: loss = 5.4392  (0.126 sec)
18-06-04 23:17-INFO->> Step 400560 run_train: loss = 5.4163  (0.216 sec)
18-06-04 23:17-INFO->> Step 400570 run_train: loss = 5.3820  (0.147 sec)
18-06-04 23:17-INFO->> Step 400580 run_train: loss = 5.4503  (0.151 sec)
18-06-04 23:17-INFO->> Step 400590 run_train: loss = 5.4085  (0.180 sec)
18-06-04 23:17-INFO->> Step 400600 run_train: loss = 5.4757  (0.194 sec)
18-06-04 23:17-INFO->> Step 400610 run_train: loss = 5.4271  (0.122 sec)
18-06-04 23:17-INFO->> Step 400620 run_train: loss = 5.4745  (0.142 sec)
18-06-04 23:17-INFO->> Step 400630 run_train: loss = 5.4248  (0.126 sec)
18-06-04 23:17-INFO->> Step 400640 run_train: loss = 5.4416  (0.215 sec)
18-06-04 23:17-INFO->> Step 400650 run_train: loss = 5.3653  (0.162 sec)
18-06-04 23:17-INFO->> Step 400660 run_train: loss = 5.4093  (0.165 sec)
18-06-04 23:17-INFO->> Step 400670 run_train: loss = 5.3996  (0.133 sec)
18-06-04 23:17-INFO->> Step 400680 run_train: loss = 5.4326  (0.167 sec)
18-06-04 23:17-INFO->> Step 400690 run_train: loss = 5.4457  (0.189 sec)
18-06-04 23:17-INFO->> Step 400700 run_train: loss = 5.4878  (0.174 sec)
18-06-04 23:17-INFO->> Step 400710 run_train: loss = 5.4647  (0.131 sec)
18-06-04 23:17-INFO->> Step 400720 run_train: loss = 5.4219  (0.186 sec)
18-06-04 23:17-INFO->> Step 400730 run_train: loss = 5.4896  (0.142 sec)
18-06-04 23:17-INFO->> Step 400740 run_train: loss = 5.4671  (0.160 sec)
18-06-04 23:17-INFO->> Step 400750 run_train: loss = 5.4073  (0.154 sec)
18-06-04 23:17-INFO->> Step 400760 run_train: loss = 5.4261  (0.155 sec)
18-06-04 23:17-INFO->> Step 400770 run_train: loss = 5.4026  (0.160 sec)
18-06-04 23:18-INFO->> Step 400780 run_train: loss = 5.3936  (0.160 sec)
18-06-04 23:18-INFO->> Step 400790 run_train: loss = 5.4276  (0.161 sec)
18-06-04 23:18-INFO->> Step 400800 run_train: loss = 5.4571  (0.165 sec)
18-06-04 23:18-INFO->> Step 400810 run_train: loss = 5.3876  (0.150 sec)
18-06-04 23:18-INFO->> Step 400820 run_train: loss = 5.4575  (0.174 sec)
18-06-04 23:18-INFO->> Step 400830 run_train: loss = 5.4103  (0.176 sec)
18-06-04 23:18-INFO->> Step 400840 run_train: loss = 5.4671  (0.173 sec)
18-06-04 23:18-INFO->> Step 400850 run_train: loss = 5.4486  (0.133 sec)
18-06-04 23:18-INFO->> Step 400860 run_train: loss = 5.4149  (0.143 sec)
18-06-04 23:18-INFO->> Step 400870 run_train: loss = 5.4056  (0.207 sec)
18-06-04 23:18-INFO->> Step 400880 run_train: loss = 5.4584  (0.146 sec)
18-06-04 23:18-INFO->> Step 400890 run_train: loss = 5.4297  (0.178 sec)
18-06-04 23:18-INFO->> Step 400900 run_train: loss = 5.3224  (0.112 sec)
18-06-04 23:18-INFO->> Step 400910 run_train: loss = 5.3857  (0.144 sec)
18-06-04 23:18-INFO->> Step 400920 run_train: loss = 5.4422  (0.164 sec)
18-06-04 23:18-INFO->> Step 400930 run_train: loss = 5.3797  (0.140 sec)
18-06-04 23:18-INFO->> Step 400940 run_train: loss = 5.4289  (0.155 sec)
18-06-04 23:18-INFO->> Step 400950 run_train: loss = 5.4067  (0.129 sec)
18-06-04 23:18-INFO->> Step 400960 run_train: loss = 5.4565  (0.145 sec)
18-06-04 23:18-INFO->> Step 400970 run_train: loss = 5.4159  (0.196 sec)
18-06-04 23:18-INFO->> Step 400980 run_train: loss = 5.4379  (0.149 sec)
18-06-04 23:18-INFO->> Step 400990 run_train: loss = 5.4002  (0.166 sec)
18-06-04 23:18-INFO->> Step 401000 run_train: loss = 5.4463  (0.159 sec)
18-06-04 23:18-INFO->> 2018-06-04 23:18:35.872053 Saving in ckpt
18-06-04 23:18-INFO-Test Data Eval:
18-06-04 23:19-INFO-fpr95 = 0.17081230074388948 and auc = 0.9692356359688575
18-06-04 23:19-INFO->> Step 401010 run_train: loss = 5.4966  (0.142 sec)
18-06-04 23:19-INFO->> Step 401020 run_train: loss = 5.4213  (0.098 sec)
18-06-04 23:19-INFO->> Step 401030 run_train: loss = 5.3832  (0.151 sec)
18-06-04 23:19-INFO->> Step 401040 run_train: loss = 5.3910  (0.164 sec)
18-06-04 23:19-INFO->> Step 401050 run_train: loss = 5.4469  (0.211 sec)
18-06-04 23:19-INFO->> Step 401060 run_train: loss = 5.3834  (0.170 sec)
18-06-04 23:19-INFO->> Step 401070 run_train: loss = 5.4280  (0.157 sec)
18-06-04 23:19-INFO->> Step 401080 run_train: loss = 5.4167  (0.150 sec)
18-06-04 23:19-INFO->> Step 401090 run_train: loss = 5.4042  (0.160 sec)
18-06-04 23:19-INFO->> Step 401100 run_train: loss = 5.4461  (0.150 sec)
18-06-04 23:19-INFO->> Step 401110 run_train: loss = 5.4772  (0.125 sec)
18-06-04 23:19-INFO->> Step 401120 run_train: loss = 5.3623  (0.169 sec)
18-06-04 23:19-INFO->> Step 401130 run_train: loss = 5.4416  (0.161 sec)
18-06-04 23:19-INFO->> Step 401140 run_train: loss = 5.3874  (0.159 sec)
18-06-04 23:19-INFO->> Step 401150 run_train: loss = 5.4886  (0.167 sec)
18-06-04 23:19-INFO->> Step 401160 run_train: loss = 5.4442  (0.185 sec)
18-06-04 23:19-INFO->> Step 401170 run_train: loss = 5.4420  (0.183 sec)
18-06-04 23:19-INFO->> Step 401180 run_train: loss = 5.4665  (0.187 sec)
18-06-04 23:19-INFO->> Step 401190 run_train: loss = 5.3392  (0.165 sec)
18-06-04 23:19-INFO->> Step 401200 run_train: loss = 5.4080  (0.202 sec)
18-06-04 23:19-INFO->> Step 401210 run_train: loss = 5.4434  (0.171 sec)
18-06-04 23:19-INFO->> Step 401220 run_train: loss = 5.4474  (0.157 sec)
18-06-04 23:19-INFO->> Step 401230 run_train: loss = 5.4502  (0.166 sec)
18-06-04 23:19-INFO->> Step 401240 run_train: loss = 5.4542  (0.145 sec)
18-06-04 23:19-INFO->> Step 401250 run_train: loss = 5.3998  (0.179 sec)
18-06-04 23:19-INFO->> Step 401260 run_train: loss = 5.4022  (0.184 sec)
18-06-04 23:19-INFO->> Step 401270 run_train: loss = 5.4225  (0.134 sec)
18-06-04 23:20-INFO->> Step 401280 run_train: loss = 5.3779  (0.186 sec)
18-06-04 23:20-INFO->> Step 401290 run_train: loss = 5.4447  (0.180 sec)
18-06-04 23:20-INFO->> Step 401300 run_train: loss = 5.4269  (0.138 sec)
18-06-04 23:20-INFO->> Step 401310 run_train: loss = 5.4133  (0.194 sec)
18-06-04 23:20-INFO->> Step 401320 run_train: loss = 5.4498  (0.154 sec)
18-06-04 23:20-INFO->> Step 401330 run_train: loss = 5.3952  (0.130 sec)
18-06-04 23:20-INFO->> Step 401340 run_train: loss = 5.4122  (0.152 sec)
18-06-04 23:20-INFO->> Step 401350 run_train: loss = 5.3876  (0.162 sec)
18-06-04 23:20-INFO->> Step 401360 run_train: loss = 5.4595  (0.155 sec)
18-06-04 23:20-INFO->> Step 401370 run_train: loss = 5.4501  (0.129 sec)
18-06-04 23:20-INFO->> Step 401380 run_train: loss = 5.4341  (0.158 sec)
18-06-04 23:20-INFO->> Step 401390 run_train: loss = 5.4318  (0.167 sec)
18-06-04 23:20-INFO->> Step 401400 run_train: loss = 5.3129  (0.156 sec)
18-06-04 23:20-INFO->> Step 401410 run_train: loss = 5.4053  (0.179 sec)
18-06-04 23:20-INFO->> Step 401420 run_train: loss = 5.3919  (0.153 sec)
18-06-04 23:20-INFO->> Step 401430 run_train: loss = 5.3998  (0.162 sec)
18-06-04 23:20-INFO->> Step 401440 run_train: loss = 5.3750  (0.173 sec)
18-06-04 23:20-INFO->> Step 401450 run_train: loss = 5.4122  (0.165 sec)
18-06-04 23:20-INFO->> Step 401460 run_train: loss = 5.3906  (0.166 sec)
18-06-04 23:20-INFO->> Step 401470 run_train: loss = 5.5007  (0.150 sec)
18-06-04 23:20-INFO->> Step 401480 run_train: loss = 5.3837  (0.205 sec)
18-06-04 23:20-INFO->> Step 401490 run_train: loss = 5.4408  (0.184 sec)
18-06-04 23:20-INFO->> Step 401500 run_train: loss = 5.3913  (0.162 sec)
18-06-04 23:20-INFO->> Step 401510 run_train: loss = 5.3774  (0.147 sec)
18-06-04 23:20-INFO->> Step 401520 run_train: loss = 5.4295  (0.164 sec)
18-06-04 23:20-INFO->> Step 401530 run_train: loss = 5.4171  (0.206 sec)
18-06-04 23:20-INFO->> Step 401540 run_train: loss = 5.4684  (0.166 sec)
18-06-04 23:20-INFO->> Step 401550 run_train: loss = 5.3955  (0.161 sec)
18-06-04 23:20-INFO->> Step 401560 run_train: loss = 5.3755  (0.172 sec)
18-06-04 23:20-INFO->> Step 401570 run_train: loss = 5.4066  (0.160 sec)
18-06-04 23:20-INFO->> Step 401580 run_train: loss = 5.4525  (0.161 sec)
18-06-04 23:20-INFO->> Step 401590 run_train: loss = 5.4109  (0.191 sec)
18-06-04 23:20-INFO->> Step 401600 run_train: loss = 5.5013  (0.170 sec)
18-06-04 23:20-INFO->> Step 401610 run_train: loss = 5.3466  (0.128 sec)
18-06-04 23:20-INFO->> Step 401620 run_train: loss = 5.4875  (0.163 sec)
18-06-04 23:20-INFO->> Step 401630 run_train: loss = 5.4113  (0.184 sec)
18-06-04 23:20-INFO->> Step 401640 run_train: loss = 5.3990  (0.143 sec)
18-06-04 23:20-INFO->> Step 401650 run_train: loss = 5.4029  (0.205 sec)
18-06-04 23:21-INFO->> Step 401660 run_train: loss = 5.4126  (0.180 sec)
18-06-04 23:21-INFO->> Step 401670 run_train: loss = 5.4869  (0.178 sec)
18-06-04 23:21-INFO->> Step 401680 run_train: loss = 5.4189  (0.136 sec)
18-06-04 23:21-INFO->> Step 401690 run_train: loss = 5.4496  (0.161 sec)
18-06-04 23:21-INFO->> Step 401700 run_train: loss = 5.4424  (0.177 sec)
18-06-04 23:21-INFO->> Step 401710 run_train: loss = 5.4150  (0.172 sec)
18-06-04 23:21-INFO->> Step 401720 run_train: loss = 5.4025  (0.180 sec)
18-06-04 23:21-INFO->> Step 401730 run_train: loss = 5.4388  (0.162 sec)
18-06-04 23:21-INFO->> Step 401740 run_train: loss = 5.3506  (0.160 sec)
18-06-04 23:21-INFO->> Step 401750 run_train: loss = 5.3500  (0.171 sec)
18-06-04 23:21-INFO->> Step 401760 run_train: loss = 5.3979  (0.194 sec)
18-06-04 23:21-INFO->> Step 401770 run_train: loss = 5.4231  (0.157 sec)
18-06-04 23:21-INFO->> Step 401780 run_train: loss = 5.5031  (0.166 sec)
18-06-04 23:21-INFO->> Step 401790 run_train: loss = 5.3796  (0.150 sec)
18-06-04 23:21-INFO->> Step 401800 run_train: loss = 5.4901  (0.166 sec)
18-06-04 23:21-INFO->> Step 401810 run_train: loss = 5.4692  (0.174 sec)
18-06-04 23:21-INFO->> Step 401820 run_train: loss = 5.4171  (0.171 sec)
18-06-04 23:21-INFO->> Step 401830 run_train: loss = 5.3798  (0.191 sec)
18-06-04 23:21-INFO->> Step 401840 run_train: loss = 5.4305  (0.150 sec)
18-06-04 23:21-INFO->> Step 401850 run_train: loss = 5.4288  (0.148 sec)
18-06-04 23:21-INFO->> Step 401860 run_train: loss = 5.4787  (0.188 sec)
18-06-04 23:21-INFO->> Step 401870 run_train: loss = 5.4026  (0.148 sec)
18-06-04 23:21-INFO->> Step 401880 run_train: loss = 5.4283  (0.193 sec)
18-06-04 23:21-INFO->> Step 401890 run_train: loss = 5.3225  (0.157 sec)
18-06-04 23:21-INFO->> Step 401900 run_train: loss = 5.3959  (0.185 sec)
18-06-04 23:21-INFO->> Step 401910 run_train: loss = 5.3847  (0.186 sec)
18-06-04 23:21-INFO->> Step 401920 run_train: loss = 5.3993  (0.177 sec)
18-06-04 23:21-INFO->> Step 401930 run_train: loss = 5.4945  (0.131 sec)
18-06-04 23:21-INFO->> Step 401940 run_train: loss = 5.4609  (0.148 sec)
18-06-04 23:21-INFO->> Step 401950 run_train: loss = 5.3815  (0.202 sec)
18-06-04 23:21-INFO->> Step 401960 run_train: loss = 5.4146  (0.153 sec)
18-06-04 23:21-INFO->> Step 401970 run_train: loss = 5.4292  (0.144 sec)
18-06-04 23:21-INFO->> Step 401980 run_train: loss = 5.3992  (0.179 sec)
18-06-04 23:21-INFO->> Step 401990 run_train: loss = 5.4147  (0.171 sec)
18-06-04 23:21-INFO->> Step 402000 run_train: loss = 5.4630  (0.132 sec)
18-06-04 23:21-INFO->> 2018-06-04 23:21:55.219412 Saving in ckpt
18-06-04 23:21-INFO-Test Data Eval:
18-06-04 23:22-INFO-fpr95 = 0.1705134165781084 and auc = 0.9692425452899042
18-06-04 23:22-INFO->> Step 402010 run_train: loss = 5.4029  (0.160 sec)
18-06-04 23:22-INFO->> Step 402020 run_train: loss = 5.4152  (0.148 sec)
18-06-04 23:22-INFO->> Step 402030 run_train: loss = 5.3636  (0.148 sec)
18-06-04 23:22-INFO->> Step 402040 run_train: loss = 5.4175  (0.145 sec)
18-06-04 23:22-INFO->> Step 402050 run_train: loss = 5.4673  (0.172 sec)
18-06-04 23:22-INFO->> Step 402060 run_train: loss = 5.4338  (0.161 sec)
18-06-04 23:22-INFO->> Step 402070 run_train: loss = 5.3768  (0.172 sec)
18-06-04 23:22-INFO->> Step 402080 run_train: loss = 5.5088  (0.170 sec)
18-06-04 23:22-INFO->> Step 402090 run_train: loss = 5.3472  (0.154 sec)
18-06-04 23:22-INFO->> Step 402100 run_train: loss = 5.4005  (0.127 sec)
18-06-04 23:22-INFO->> Step 402110 run_train: loss = 5.4210  (0.154 sec)
18-06-04 23:22-INFO->> Step 402120 run_train: loss = 5.4540  (0.196 sec)
18-06-04 23:22-INFO->> Step 402130 run_train: loss = 5.4815  (0.158 sec)
18-06-04 23:22-INFO->> Step 402140 run_train: loss = 5.4354  (0.144 sec)
18-06-04 23:22-INFO->> Step 402150 run_train: loss = 5.4334  (0.147 sec)
18-06-04 23:23-INFO->> Step 402160 run_train: loss = 5.4379  (0.167 sec)
18-06-04 23:23-INFO->> Step 402170 run_train: loss = 5.4465  (0.164 sec)
18-06-04 23:23-INFO->> Step 402180 run_train: loss = 5.4742  (0.186 sec)
18-06-04 23:23-INFO->> Step 402190 run_train: loss = 5.4481  (0.140 sec)
18-06-04 23:23-INFO->> Step 402200 run_train: loss = 5.4011  (0.148 sec)
18-06-04 23:23-INFO->> Step 402210 run_train: loss = 5.4438  (0.144 sec)
18-06-04 23:23-INFO->> Step 402220 run_train: loss = 5.4941  (0.145 sec)
18-06-04 23:23-INFO->> Step 402230 run_train: loss = 5.3999  (0.157 sec)
18-06-04 23:23-INFO->> Step 402240 run_train: loss = 5.4694  (0.175 sec)
18-06-04 23:23-INFO->> Step 402250 run_train: loss = 5.4324  (0.165 sec)
18-06-04 23:23-INFO->> Step 402260 run_train: loss = 5.5015  (0.155 sec)
18-06-04 23:23-INFO->> Step 402270 run_train: loss = 5.4164  (0.120 sec)
18-06-04 23:23-INFO->> Step 402280 run_train: loss = 5.4677  (0.165 sec)
18-06-04 23:23-INFO->> Step 402290 run_train: loss = 5.4233  (0.172 sec)
18-06-04 23:23-INFO->> Step 402300 run_train: loss = 5.4209  (0.137 sec)
18-06-04 23:23-INFO->> Step 402310 run_train: loss = 5.4312  (0.136 sec)
18-06-04 23:23-INFO->> Step 402320 run_train: loss = 5.4454  (0.166 sec)
18-06-04 23:23-INFO->> Step 402330 run_train: loss = 5.4039  (0.195 sec)
18-06-04 23:23-INFO->> Step 402340 run_train: loss = 5.3637  (0.183 sec)
18-06-04 23:23-INFO->> Step 402350 run_train: loss = 5.4334  (0.198 sec)
18-06-04 23:23-INFO->> Step 402360 run_train: loss = 5.4635  (0.180 sec)
18-06-04 23:23-INFO->> Step 402370 run_train: loss = 5.4494  (0.150 sec)
18-06-04 23:23-INFO->> Step 402380 run_train: loss = 5.4282  (0.125 sec)
18-06-04 23:23-INFO->> Step 402390 run_train: loss = 5.3856  (0.167 sec)
18-06-04 23:23-INFO->> Step 402400 run_train: loss = 5.4323  (0.167 sec)
18-06-04 23:23-INFO->> Step 402410 run_train: loss = 5.3880  (0.160 sec)
18-06-04 23:23-INFO->> Step 402420 run_train: loss = 5.4201  (0.146 sec)
18-06-04 23:23-INFO->> Step 402430 run_train: loss = 5.3923  (0.161 sec)
18-06-04 23:23-INFO->> Step 402440 run_train: loss = 5.4131  (0.172 sec)
18-06-04 23:23-INFO->> Step 402450 run_train: loss = 5.4247  (0.182 sec)
18-06-04 23:23-INFO->> Step 402460 run_train: loss = 5.4544  (0.135 sec)
18-06-04 23:23-INFO->> Step 402470 run_train: loss = 5.4236  (0.142 sec)
18-06-04 23:23-INFO->> Step 402480 run_train: loss = 5.4885  (0.173 sec)
18-06-04 23:23-INFO->> Step 402490 run_train: loss = 5.4399  (0.145 sec)
18-06-04 23:23-INFO->> Step 402500 run_train: loss = 5.4121  (0.169 sec)
18-06-04 23:23-INFO->> Step 402510 run_train: loss = 5.4612  (0.149 sec)
18-06-04 23:23-INFO->> Step 402520 run_train: loss = 5.3946  (0.147 sec)
18-06-04 23:23-INFO->> Step 402530 run_train: loss = 5.4621  (0.150 sec)
18-06-04 23:24-INFO->> Step 402540 run_train: loss = 5.4903  (0.188 sec)
18-06-04 23:24-INFO->> Step 402550 run_train: loss = 5.4073  (0.158 sec)
18-06-04 23:24-INFO->> Step 402560 run_train: loss = 5.3951  (0.165 sec)
18-06-04 23:24-INFO->> Step 402570 run_train: loss = 5.3687  (0.146 sec)
18-06-04 23:24-INFO->> Step 402580 run_train: loss = 5.3875  (0.153 sec)
18-06-04 23:24-INFO->> Step 402590 run_train: loss = 5.4462  (0.162 sec)
18-06-04 23:24-INFO->> Step 402600 run_train: loss = 5.3779  (0.156 sec)
18-06-04 23:24-INFO->> Step 402610 run_train: loss = 5.4170  (0.151 sec)
18-06-04 23:24-INFO->> Step 402620 run_train: loss = 5.3329  (0.160 sec)
18-06-04 23:24-INFO->> Step 402630 run_train: loss = 5.4023  (0.133 sec)
18-06-04 23:24-INFO->> Step 402640 run_train: loss = 5.3930  (0.171 sec)
18-06-04 23:24-INFO->> Step 402650 run_train: loss = 5.3929  (0.146 sec)
18-06-04 23:24-INFO->> Step 402660 run_train: loss = 5.4534  (0.159 sec)
18-06-04 23:24-INFO->> Step 402670 run_train: loss = 5.4034  (0.155 sec)
18-06-04 23:24-INFO->> Step 402680 run_train: loss = 5.4255  (0.180 sec)
18-06-04 23:24-INFO->> Step 402690 run_train: loss = 5.4458  (0.149 sec)
18-06-04 23:24-INFO->> Step 402700 run_train: loss = 5.3275  (0.170 sec)
18-06-04 23:24-INFO->> Step 402710 run_train: loss = 5.4071  (0.138 sec)
18-06-04 23:24-INFO->> Step 402720 run_train: loss = 5.4331  (0.156 sec)
18-06-04 23:24-INFO->> Step 402730 run_train: loss = 5.4453  (0.155 sec)
18-06-04 23:24-INFO->> Step 402740 run_train: loss = 5.4071  (0.168 sec)
18-06-04 23:24-INFO->> Step 402750 run_train: loss = 5.3542  (0.123 sec)
18-06-04 23:24-INFO->> Step 402760 run_train: loss = 5.4239  (0.165 sec)
18-06-04 23:24-INFO->> Step 402770 run_train: loss = 5.3790  (0.121 sec)
18-06-04 23:24-INFO->> Step 402780 run_train: loss = 5.3529  (0.149 sec)
18-06-04 23:24-INFO->> Step 402790 run_train: loss = 5.4006  (0.138 sec)
18-06-04 23:24-INFO->> Step 402800 run_train: loss = 5.4539  (0.154 sec)
18-06-04 23:24-INFO->> Step 402810 run_train: loss = 5.4538  (0.131 sec)
18-06-04 23:24-INFO->> Step 402820 run_train: loss = 5.3952  (0.139 sec)
18-06-04 23:24-INFO->> Step 402830 run_train: loss = 5.4517  (0.173 sec)
18-06-04 23:24-INFO->> Step 402840 run_train: loss = 5.3685  (0.145 sec)
18-06-04 23:24-INFO->> Step 402850 run_train: loss = 5.4203  (0.146 sec)
18-06-04 23:24-INFO->> Step 402860 run_train: loss = 5.4632  (0.190 sec)
18-06-04 23:24-INFO->> Step 402870 run_train: loss = 5.4566  (0.141 sec)
18-06-04 23:24-INFO->> Step 402880 run_train: loss = 5.3785  (0.174 sec)
18-06-04 23:24-INFO->> Step 402890 run_train: loss = 5.4109  (0.146 sec)
18-06-04 23:24-INFO->> Step 402900 run_train: loss = 5.3956  (0.174 sec)
18-06-04 23:25-INFO->> Step 402910 run_train: loss = 5.3672  (0.139 sec)
18-06-04 23:25-INFO->> Step 402920 run_train: loss = 5.4703  (0.154 sec)
18-06-04 23:25-INFO->> Step 402930 run_train: loss = 5.4372  (0.164 sec)
18-06-04 23:25-INFO->> Step 402940 run_train: loss = 5.4475  (0.179 sec)
18-06-04 23:25-INFO->> Step 402950 run_train: loss = 5.4231  (0.184 sec)
18-06-04 23:25-INFO->> Step 402960 run_train: loss = 5.4541  (0.161 sec)
18-06-04 23:25-INFO->> Step 402970 run_train: loss = 5.3791  (0.138 sec)
18-06-04 23:25-INFO->> Step 402980 run_train: loss = 5.4456  (0.152 sec)
18-06-04 23:25-INFO->> Step 402990 run_train: loss = 5.4665  (0.171 sec)
18-06-04 23:25-INFO->> Step 403000 run_train: loss = 5.4267  (0.119 sec)
18-06-04 23:25-INFO->> 2018-06-04 23:25:14.789157 Saving in ckpt
18-06-04 23:25-INFO-Test Data Eval:
18-06-04 23:25-INFO-fpr95 = 0.17630844845908608 and auc = 0.9687841525736612
18-06-04 23:25-INFO->> Step 403010 run_train: loss = 5.3978  (0.142 sec)
18-06-04 23:25-INFO->> Step 403020 run_train: loss = 5.3792  (0.155 sec)
18-06-04 23:25-INFO->> Step 403030 run_train: loss = 5.4441  (0.166 sec)
18-06-04 23:26-INFO->> Step 403040 run_train: loss = 5.4219  (0.149 sec)
18-06-04 23:26-INFO->> Step 403050 run_train: loss = 5.3120  (0.172 sec)
18-06-04 23:26-INFO->> Step 403060 run_train: loss = 5.4880  (0.175 sec)
18-06-04 23:26-INFO->> Step 403070 run_train: loss = 5.3615  (0.174 sec)
18-06-04 23:26-INFO->> Step 403080 run_train: loss = 5.3377  (0.152 sec)
18-06-04 23:26-INFO->> Step 403090 run_train: loss = 5.3750  (0.171 sec)
18-06-04 23:26-INFO->> Step 403100 run_train: loss = 5.4184  (0.127 sec)
18-06-04 23:26-INFO->> Step 403110 run_train: loss = 5.3687  (0.184 sec)
18-06-04 23:26-INFO->> Step 403120 run_train: loss = 5.4418  (0.147 sec)
18-06-04 23:26-INFO->> Step 403130 run_train: loss = 5.4670  (0.126 sec)
18-06-04 23:26-INFO->> Step 403140 run_train: loss = 5.4644  (0.135 sec)
18-06-04 23:26-INFO->> Step 403150 run_train: loss = 5.4169  (0.127 sec)
18-06-04 23:26-INFO->> Step 403160 run_train: loss = 5.3627  (0.156 sec)
18-06-04 23:26-INFO->> Step 403170 run_train: loss = 5.4223  (0.125 sec)
18-06-04 23:26-INFO->> Step 403180 run_train: loss = 5.4607  (0.168 sec)
18-06-04 23:26-INFO->> Step 403190 run_train: loss = 5.4524  (0.172 sec)
18-06-04 23:26-INFO->> Step 403200 run_train: loss = 5.4826  (0.156 sec)
18-06-04 23:26-INFO->> Step 403210 run_train: loss = 5.4183  (0.176 sec)
18-06-04 23:26-INFO->> Step 403220 run_train: loss = 5.3907  (0.154 sec)
18-06-04 23:26-INFO->> Step 403230 run_train: loss = 5.4932  (0.156 sec)
18-06-04 23:26-INFO->> Step 403240 run_train: loss = 5.4416  (0.128 sec)
18-06-04 23:26-INFO->> Step 403250 run_train: loss = 5.4725  (0.131 sec)
18-06-04 23:26-INFO->> Step 403260 run_train: loss = 5.3793  (0.179 sec)
18-06-04 23:26-INFO->> Step 403270 run_train: loss = 5.4535  (0.179 sec)
18-06-04 23:26-INFO->> Step 403280 run_train: loss = 5.3502  (0.170 sec)
18-06-04 23:26-INFO->> Step 403290 run_train: loss = 5.4470  (0.147 sec)
18-06-04 23:26-INFO->> Step 403300 run_train: loss = 5.4327  (0.163 sec)
18-06-04 23:26-INFO->> Step 403310 run_train: loss = 5.4292  (0.154 sec)
18-06-04 23:26-INFO->> Step 403320 run_train: loss = 5.3797  (0.159 sec)
18-06-04 23:26-INFO->> Step 403330 run_train: loss = 5.4433  (0.135 sec)
18-06-04 23:26-INFO->> Step 403340 run_train: loss = 5.3950  (0.152 sec)
18-06-04 23:26-INFO->> Step 403350 run_train: loss = 5.4517  (0.152 sec)
18-06-04 23:26-INFO->> Step 403360 run_train: loss = 5.3830  (0.154 sec)
18-06-04 23:26-INFO->> Step 403370 run_train: loss = 5.4445  (0.136 sec)
18-06-04 23:26-INFO->> Step 403380 run_train: loss = 5.4195  (0.136 sec)
18-06-04 23:26-INFO->> Step 403390 run_train: loss = 5.3296  (0.162 sec)
18-06-04 23:26-INFO->> Step 403400 run_train: loss = 5.4834  (0.153 sec)
18-06-04 23:27-INFO->> Step 403410 run_train: loss = 5.3523  (0.165 sec)
18-06-04 23:27-INFO->> Step 403420 run_train: loss = 5.4285  (0.179 sec)
18-06-04 23:27-INFO->> Step 403430 run_train: loss = 5.4547  (0.166 sec)
18-06-04 23:27-INFO->> Step 403440 run_train: loss = 5.4684  (0.188 sec)
18-06-04 23:27-INFO->> Step 403450 run_train: loss = 5.3815  (0.158 sec)
18-06-04 23:27-INFO->> Step 403460 run_train: loss = 5.3724  (0.199 sec)
18-06-04 23:27-INFO->> Step 403470 run_train: loss = 5.4153  (0.144 sec)
18-06-04 23:27-INFO->> Step 403480 run_train: loss = 5.4468  (0.160 sec)
18-06-04 23:27-INFO->> Step 403490 run_train: loss = 5.3215  (0.168 sec)
18-06-04 23:27-INFO->> Step 403500 run_train: loss = 5.4305  (0.173 sec)
18-06-04 23:27-INFO->> Step 403510 run_train: loss = 5.4584  (0.128 sec)
18-06-04 23:27-INFO->> Step 403520 run_train: loss = 5.3673  (0.139 sec)
18-06-04 23:27-INFO->> Step 403530 run_train: loss = 5.4234  (0.139 sec)
18-06-04 23:27-INFO->> Step 403540 run_train: loss = 5.4205  (0.120 sec)
18-06-04 23:27-INFO->> Step 403550 run_train: loss = 5.3787  (0.185 sec)
18-06-04 23:27-INFO->> Step 403560 run_train: loss = 5.4494  (0.171 sec)
18-06-04 23:27-INFO->> Step 403570 run_train: loss = 5.4406  (0.148 sec)
18-06-04 23:27-INFO->> Step 403580 run_train: loss = 5.4291  (0.152 sec)
18-06-04 23:27-INFO->> Step 403590 run_train: loss = 5.3863  (0.189 sec)
18-06-04 23:27-INFO->> Step 403600 run_train: loss = 5.4523  (0.133 sec)
18-06-04 23:27-INFO->> Step 403610 run_train: loss = 5.4200  (0.179 sec)
18-06-04 23:27-INFO->> Step 403620 run_train: loss = 5.4387  (0.177 sec)
18-06-04 23:27-INFO->> Step 403630 run_train: loss = 5.4785  (0.176 sec)
18-06-04 23:27-INFO->> Step 403640 run_train: loss = 5.4131  (0.128 sec)
18-06-04 23:27-INFO->> Step 403650 run_train: loss = 5.4677  (0.155 sec)
18-06-04 23:27-INFO->> Step 403660 run_train: loss = 5.4085  (0.190 sec)
18-06-04 23:27-INFO->> Step 403670 run_train: loss = 5.3606  (0.156 sec)
18-06-04 23:27-INFO->> Step 403680 run_train: loss = 5.3743  (0.144 sec)
18-06-04 23:27-INFO->> Step 403690 run_train: loss = 5.4922  (0.148 sec)
18-06-04 23:27-INFO->> Step 403700 run_train: loss = 5.3966  (0.158 sec)
18-06-04 23:27-INFO->> Step 403710 run_train: loss = 5.4475  (0.135 sec)
18-06-04 23:27-INFO->> Step 403720 run_train: loss = 5.3981  (0.152 sec)
18-06-04 23:27-INFO->> Step 403730 run_train: loss = 5.3639  (0.190 sec)
18-06-04 23:27-INFO->> Step 403740 run_train: loss = 5.4023  (0.189 sec)
18-06-04 23:27-INFO->> Step 403750 run_train: loss = 5.4403  (0.147 sec)
18-06-04 23:27-INFO->> Step 403760 run_train: loss = 5.3540  (0.160 sec)
18-06-04 23:27-INFO->> Step 403770 run_train: loss = 5.4369  (0.161 sec)
18-06-04 23:27-INFO->> Step 403780 run_train: loss = 5.4497  (0.146 sec)
18-06-04 23:28-INFO->> Step 403790 run_train: loss = 5.4633  (0.143 sec)
18-06-04 23:28-INFO->> Step 403800 run_train: loss = 5.4496  (0.182 sec)
18-06-04 23:28-INFO->> Step 403810 run_train: loss = 5.4641  (0.161 sec)
18-06-04 23:28-INFO->> Step 403820 run_train: loss = 5.4588  (0.145 sec)
18-06-04 23:28-INFO->> Step 403830 run_train: loss = 5.5118  (0.148 sec)
18-06-04 23:28-INFO->> Step 403840 run_train: loss = 5.4314  (0.181 sec)
18-06-04 23:28-INFO->> Step 403850 run_train: loss = 5.4774  (0.162 sec)
18-06-04 23:28-INFO->> Step 403860 run_train: loss = 5.3892  (0.162 sec)
18-06-04 23:28-INFO->> Step 403870 run_train: loss = 5.4374  (0.187 sec)
18-06-04 23:28-INFO->> Step 403880 run_train: loss = 5.4508  (0.192 sec)
18-06-04 23:28-INFO->> Step 403890 run_train: loss = 5.4032  (0.157 sec)
18-06-04 23:28-INFO->> Step 403900 run_train: loss = 5.4209  (0.171 sec)
18-06-04 23:28-INFO->> Step 403910 run_train: loss = 5.4181  (0.182 sec)
18-06-04 23:28-INFO->> Step 403920 run_train: loss = 5.3651  (0.161 sec)
18-06-04 23:28-INFO->> Step 403930 run_train: loss = 5.4254  (0.141 sec)
18-06-04 23:28-INFO->> Step 403940 run_train: loss = 5.3707  (0.136 sec)
18-06-04 23:28-INFO->> Step 403950 run_train: loss = 5.4492  (0.148 sec)
18-06-04 23:28-INFO->> Step 403960 run_train: loss = 5.4567  (0.119 sec)
18-06-04 23:28-INFO->> Step 403970 run_train: loss = 5.4601  (0.182 sec)
18-06-04 23:28-INFO->> Step 403980 run_train: loss = 5.4544  (0.184 sec)
18-06-04 23:28-INFO->> Step 403990 run_train: loss = 5.3811  (0.136 sec)
18-06-04 23:28-INFO->> Step 404000 run_train: loss = 5.4319  (0.173 sec)
18-06-04 23:28-INFO->> 2018-06-04 23:28:34.122322 Saving in ckpt
18-06-04 23:28-INFO-Test Data Eval:
18-06-04 23:29-INFO-fpr95 = 0.17542840063761955 and auc = 0.969061884071407
18-06-04 23:29-INFO->> Step 404010 run_train: loss = 5.3661  (0.150 sec)
18-06-04 23:29-INFO->> Step 404020 run_train: loss = 5.4413  (0.137 sec)
18-06-04 23:29-INFO->> Step 404030 run_train: loss = 5.3915  (0.226 sec)
18-06-04 23:29-INFO->> Step 404040 run_train: loss = 5.3680  (0.148 sec)
18-06-04 23:29-INFO->> Step 404050 run_train: loss = 5.3894  (0.114 sec)
18-06-04 23:29-INFO->> Step 404060 run_train: loss = 5.4145  (0.135 sec)
18-06-04 23:29-INFO->> Step 404070 run_train: loss = 5.4514  (0.169 sec)
18-06-04 23:29-INFO->> Step 404080 run_train: loss = 5.4312  (0.129 sec)
18-06-04 23:29-INFO->> Step 404090 run_train: loss = 5.3961  (0.194 sec)
18-06-04 23:29-INFO->> Step 404100 run_train: loss = 5.3393  (0.148 sec)
18-06-04 23:29-INFO->> Step 404110 run_train: loss = 5.4437  (0.132 sec)
18-06-04 23:29-INFO->> Step 404120 run_train: loss = 5.4298  (0.156 sec)
18-06-04 23:29-INFO->> Step 404130 run_train: loss = 5.4310  (0.157 sec)
18-06-04 23:29-INFO->> Step 404140 run_train: loss = 5.4328  (0.171 sec)
18-06-04 23:29-INFO->> Step 404150 run_train: loss = 5.4091  (0.168 sec)
18-06-04 23:29-INFO->> Step 404160 run_train: loss = 5.3977  (0.130 sec)
18-06-04 23:29-INFO->> Step 404170 run_train: loss = 5.4185  (0.154 sec)
18-06-04 23:29-INFO->> Step 404180 run_train: loss = 5.4924  (0.154 sec)
18-06-04 23:29-INFO->> Step 404190 run_train: loss = 5.3886  (0.175 sec)
18-06-04 23:29-INFO->> Step 404200 run_train: loss = 5.3783  (0.156 sec)
18-06-04 23:29-INFO->> Step 404210 run_train: loss = 5.4038  (0.129 sec)
18-06-04 23:29-INFO->> Step 404220 run_train: loss = 5.3988  (0.136 sec)
18-06-04 23:29-INFO->> Step 404230 run_train: loss = 5.3949  (0.158 sec)
18-06-04 23:29-INFO->> Step 404240 run_train: loss = 5.3089  (0.182 sec)
18-06-04 23:29-INFO->> Step 404250 run_train: loss = 5.3778  (0.141 sec)
18-06-04 23:29-INFO->> Step 404260 run_train: loss = 5.3668  (0.161 sec)
18-06-04 23:29-INFO->> Step 404270 run_train: loss = 5.4357  (0.104 sec)
18-06-04 23:29-INFO->> Step 404280 run_train: loss = 5.4142  (0.185 sec)
18-06-04 23:30-INFO->> Step 404290 run_train: loss = 5.4241  (0.155 sec)
18-06-04 23:30-INFO->> Step 404300 run_train: loss = 5.3866  (0.162 sec)
18-06-04 23:30-INFO->> Step 404310 run_train: loss = 5.4836  (0.165 sec)
18-06-04 23:30-INFO->> Step 404320 run_train: loss = 5.4297  (0.131 sec)
18-06-04 23:30-INFO->> Step 404330 run_train: loss = 5.4356  (0.157 sec)
18-06-04 23:30-INFO->> Step 404340 run_train: loss = 5.4363  (0.150 sec)
18-06-04 23:30-INFO->> Step 404350 run_train: loss = 5.4741  (0.172 sec)
18-06-04 23:30-INFO->> Step 404360 run_train: loss = 5.3772  (0.140 sec)
18-06-04 23:30-INFO->> Step 404370 run_train: loss = 5.4106  (0.142 sec)
18-06-04 23:30-INFO->> Step 404380 run_train: loss = 5.4699  (0.155 sec)
18-06-04 23:30-INFO->> Step 404390 run_train: loss = 5.4334  (0.203 sec)
18-06-04 23:30-INFO->> Step 404400 run_train: loss = 5.4358  (0.148 sec)
18-06-04 23:30-INFO->> Step 404410 run_train: loss = 5.4669  (0.157 sec)
18-06-04 23:30-INFO->> Step 404420 run_train: loss = 5.3966  (0.192 sec)
18-06-04 23:30-INFO->> Step 404430 run_train: loss = 5.4631  (0.127 sec)
18-06-04 23:30-INFO->> Step 404440 run_train: loss = 5.4280  (0.159 sec)
18-06-04 23:30-INFO->> Step 404450 run_train: loss = 5.3641  (0.166 sec)
18-06-04 23:30-INFO->> Step 404460 run_train: loss = 5.3825  (0.142 sec)
18-06-04 23:30-INFO->> Step 404470 run_train: loss = 5.4262  (0.195 sec)
18-06-04 23:30-INFO->> Step 404480 run_train: loss = 5.4442  (0.147 sec)
18-06-04 23:30-INFO->> Step 404490 run_train: loss = 5.4340  (0.156 sec)
18-06-04 23:30-INFO->> Step 404500 run_train: loss = 5.4610  (0.145 sec)
18-06-04 23:30-INFO->> Step 404510 run_train: loss = 5.4279  (0.181 sec)
18-06-04 23:30-INFO->> Step 404520 run_train: loss = 5.3641  (0.186 sec)
18-06-04 23:30-INFO->> Step 404530 run_train: loss = 5.4051  (0.162 sec)
18-06-04 23:30-INFO->> Step 404540 run_train: loss = 5.4608  (0.194 sec)
18-06-04 23:30-INFO->> Step 404550 run_train: loss = 5.3984  (0.191 sec)
18-06-04 23:30-INFO->> Step 404560 run_train: loss = 5.4233  (0.133 sec)
18-06-04 23:30-INFO->> Step 404570 run_train: loss = 5.4729  (0.135 sec)
18-06-04 23:30-INFO->> Step 404580 run_train: loss = 5.4433  (0.151 sec)
18-06-04 23:30-INFO->> Step 404590 run_train: loss = 5.5228  (0.181 sec)
18-06-04 23:30-INFO->> Step 404600 run_train: loss = 5.4586  (0.167 sec)
18-06-04 23:30-INFO->> Step 404610 run_train: loss = 5.4176  (0.172 sec)
18-06-04 23:30-INFO->> Step 404620 run_train: loss = 5.4155  (0.163 sec)
18-06-04 23:30-INFO->> Step 404630 run_train: loss = 5.4363  (0.200 sec)
18-06-04 23:30-INFO->> Step 404640 run_train: loss = 5.4727  (0.138 sec)
18-06-04 23:30-INFO->> Step 404650 run_train: loss = 5.4291  (0.145 sec)
18-06-04 23:30-INFO->> Step 404660 run_train: loss = 5.3555  (0.200 sec)
18-06-04 23:31-INFO->> Step 404670 run_train: loss = 5.4097  (0.160 sec)
18-06-04 23:31-INFO->> Step 404680 run_train: loss = 5.3939  (0.145 sec)
18-06-04 23:31-INFO->> Step 404690 run_train: loss = 5.4532  (0.162 sec)
18-06-04 23:31-INFO->> Step 404700 run_train: loss = 5.4377  (0.153 sec)
18-06-04 23:31-INFO->> Step 404710 run_train: loss = 5.3984  (0.132 sec)
18-06-04 23:31-INFO->> Step 404720 run_train: loss = 5.4239  (0.185 sec)
18-06-04 23:31-INFO->> Step 404730 run_train: loss = 5.4376  (0.178 sec)
18-06-04 23:31-INFO->> Step 404740 run_train: loss = 5.4447  (0.165 sec)
18-06-04 23:31-INFO->> Step 404750 run_train: loss = 5.3583  (0.141 sec)
18-06-04 23:31-INFO->> Step 404760 run_train: loss = 5.4387  (0.150 sec)
18-06-04 23:31-INFO->> Step 404770 run_train: loss = 5.3868  (0.157 sec)
18-06-04 23:31-INFO->> Step 404780 run_train: loss = 5.4318  (0.172 sec)
18-06-04 23:31-INFO->> Step 404790 run_train: loss = 5.4097  (0.157 sec)
18-06-04 23:31-INFO->> Step 404800 run_train: loss = 5.3662  (0.174 sec)
18-06-04 23:31-INFO->> Step 404810 run_train: loss = 5.4620  (0.154 sec)
18-06-04 23:31-INFO->> Step 404820 run_train: loss = 5.4439  (0.152 sec)
18-06-04 23:31-INFO->> Step 404830 run_train: loss = 5.4440  (0.145 sec)
18-06-04 23:31-INFO->> Step 404840 run_train: loss = 5.4598  (0.154 sec)
18-06-04 23:31-INFO->> Step 404850 run_train: loss = 5.4579  (0.148 sec)
18-06-04 23:31-INFO->> Step 404860 run_train: loss = 5.4049  (0.187 sec)
18-06-04 23:31-INFO->> Step 404870 run_train: loss = 5.3898  (0.220 sec)
18-06-04 23:31-INFO->> Step 404880 run_train: loss = 5.4028  (0.150 sec)
18-06-04 23:31-INFO->> Step 404890 run_train: loss = 5.3956  (0.152 sec)
18-06-04 23:31-INFO->> Step 404900 run_train: loss = 5.4363  (0.191 sec)
18-06-04 23:31-INFO->> Step 404910 run_train: loss = 5.3983  (0.149 sec)
18-06-04 23:31-INFO->> Step 404920 run_train: loss = 5.4573  (0.171 sec)
18-06-04 23:31-INFO->> Step 404930 run_train: loss = 5.4441  (0.161 sec)
18-06-04 23:31-INFO->> Step 404940 run_train: loss = 5.4691  (0.167 sec)
18-06-04 23:31-INFO->> Step 404950 run_train: loss = 5.3857  (0.147 sec)
18-06-04 23:31-INFO->> Step 404960 run_train: loss = 5.4160  (0.164 sec)
18-06-04 23:31-INFO->> Step 404970 run_train: loss = 5.4417  (0.157 sec)
18-06-04 23:31-INFO->> Step 404980 run_train: loss = 5.4393  (0.124 sec)
18-06-04 23:31-INFO->> Step 404990 run_train: loss = 5.4653  (0.172 sec)
18-06-04 23:31-INFO->> Step 405000 run_train: loss = 5.4370  (0.130 sec)
18-06-04 23:31-INFO->> 2018-06-04 23:31:53.614564 Saving in ckpt
18-06-04 23:31-INFO-Test Data Eval:
18-06-04 23:32-INFO-fpr95 = 0.17454835281615302 and auc = 0.9690449419545153
18-06-04 23:32-INFO->> Step 405010 run_train: loss = 5.4321  (0.148 sec)
18-06-04 23:32-INFO->> Step 405020 run_train: loss = 5.4820  (0.155 sec)
18-06-04 23:32-INFO->> Step 405030 run_train: loss = 5.3996  (0.176 sec)
18-06-04 23:32-INFO->> Step 405040 run_train: loss = 5.4827  (0.153 sec)
18-06-04 23:32-INFO->> Step 405050 run_train: loss = 5.5019  (0.133 sec)
18-06-04 23:32-INFO->> Step 405060 run_train: loss = 5.4374  (0.163 sec)
18-06-04 23:32-INFO->> Step 405070 run_train: loss = 5.4150  (0.171 sec)
18-06-04 23:32-INFO->> Step 405080 run_train: loss = 5.4157  (0.156 sec)
18-06-04 23:32-INFO->> Step 405090 run_train: loss = 5.4225  (0.157 sec)
18-06-04 23:32-INFO->> Step 405100 run_train: loss = 5.3296  (0.162 sec)
18-06-04 23:32-INFO->> Step 405110 run_train: loss = 5.4021  (0.145 sec)
18-06-04 23:32-INFO->> Step 405120 run_train: loss = 5.3868  (0.130 sec)
18-06-04 23:32-INFO->> Step 405130 run_train: loss = 5.3938  (0.188 sec)
18-06-04 23:32-INFO->> Step 405140 run_train: loss = 5.4557  (0.191 sec)
18-06-04 23:32-INFO->> Step 405150 run_train: loss = 5.4402  (0.202 sec)
18-06-04 23:32-INFO->> Step 405160 run_train: loss = 5.4758  (0.196 sec)
18-06-04 23:33-INFO->> Step 405170 run_train: loss = 5.3797  (0.165 sec)
18-06-04 23:33-INFO->> Step 405180 run_train: loss = 5.4152  (0.128 sec)
18-06-04 23:33-INFO->> Step 405190 run_train: loss = 5.3828  (0.168 sec)
18-06-04 23:33-INFO->> Step 405200 run_train: loss = 5.4508  (0.194 sec)
18-06-04 23:33-INFO->> Step 405210 run_train: loss = 5.4267  (0.160 sec)
18-06-04 23:33-INFO->> Step 405220 run_train: loss = 5.3964  (0.142 sec)
18-06-04 23:33-INFO->> Step 405230 run_train: loss = 5.4042  (0.146 sec)
18-06-04 23:33-INFO->> Step 405240 run_train: loss = 5.4916  (0.172 sec)
18-06-04 23:33-INFO->> Step 405250 run_train: loss = 5.4153  (0.175 sec)
18-06-04 23:33-INFO->> Step 405260 run_train: loss = 5.3416  (0.158 sec)
18-06-04 23:33-INFO->> Step 405270 run_train: loss = 5.4314  (0.170 sec)
18-06-04 23:33-INFO->> Step 405280 run_train: loss = 5.4209  (0.139 sec)
18-06-04 23:33-INFO->> Step 405290 run_train: loss = 5.3808  (0.149 sec)
18-06-04 23:33-INFO->> Step 405300 run_train: loss = 5.4291  (0.159 sec)
18-06-04 23:33-INFO->> Step 405310 run_train: loss = 5.3599  (0.184 sec)
18-06-04 23:33-INFO->> Step 405320 run_train: loss = 5.4701  (0.160 sec)
18-06-04 23:33-INFO->> Step 405330 run_train: loss = 5.3275  (0.149 sec)
18-06-04 23:33-INFO->> Step 405340 run_train: loss = 5.4313  (0.169 sec)
18-06-04 23:33-INFO->> Step 405350 run_train: loss = 5.4048  (0.130 sec)
18-06-04 23:33-INFO->> Step 405360 run_train: loss = 5.5043  (0.166 sec)
18-06-04 23:33-INFO->> Step 405370 run_train: loss = 5.4333  (0.167 sec)
18-06-04 23:33-INFO->> Step 405380 run_train: loss = 5.4134  (0.144 sec)
18-06-04 23:33-INFO->> Step 405390 run_train: loss = 5.4414  (0.145 sec)
18-06-04 23:33-INFO->> Step 405400 run_train: loss = 5.3872  (0.153 sec)
18-06-04 23:33-INFO->> Step 405410 run_train: loss = 5.4317  (0.140 sec)
18-06-04 23:33-INFO->> Step 405420 run_train: loss = 5.4126  (0.177 sec)
18-06-04 23:33-INFO->> Step 405430 run_train: loss = 5.4051  (0.167 sec)
18-06-04 23:33-INFO->> Step 405440 run_train: loss = 5.4478  (0.150 sec)
18-06-04 23:33-INFO->> Step 405450 run_train: loss = 5.3683  (0.143 sec)
18-06-04 23:33-INFO->> Step 405460 run_train: loss = 5.4401  (0.205 sec)
18-06-04 23:33-INFO->> Step 405470 run_train: loss = 5.4733  (0.152 sec)
18-06-04 23:33-INFO->> Step 405480 run_train: loss = 5.3940  (0.159 sec)
18-06-04 23:33-INFO->> Step 405490 run_train: loss = 5.4400  (0.165 sec)
18-06-04 23:33-INFO->> Step 405500 run_train: loss = 5.4362  (0.197 sec)
18-06-04 23:33-INFO->> Step 405510 run_train: loss = 5.4447  (0.152 sec)
18-06-04 23:33-INFO->> Step 405520 run_train: loss = 5.3966  (0.167 sec)
18-06-04 23:33-INFO->> Step 405530 run_train: loss = 5.4852  (0.186 sec)
18-06-04 23:33-INFO->> Step 405540 run_train: loss = 5.4397  (0.188 sec)
18-06-04 23:34-INFO->> Step 405550 run_train: loss = 5.3632  (0.175 sec)
18-06-04 23:34-INFO->> Step 405560 run_train: loss = 5.4315  (0.179 sec)
18-06-04 23:34-INFO->> Step 405570 run_train: loss = 5.3836  (0.184 sec)
18-06-04 23:34-INFO->> Step 405580 run_train: loss = 5.4552  (0.146 sec)
18-06-04 23:34-INFO->> Step 405590 run_train: loss = 5.4127  (0.156 sec)
18-06-04 23:34-INFO->> Step 405600 run_train: loss = 5.4028  (0.158 sec)
18-06-04 23:34-INFO->> Step 405610 run_train: loss = 5.3495  (0.203 sec)
18-06-04 23:34-INFO->> Step 405620 run_train: loss = 5.3694  (0.144 sec)
18-06-04 23:34-INFO->> Step 405630 run_train: loss = 5.4432  (0.152 sec)
18-06-04 23:34-INFO->> Step 405640 run_train: loss = 5.4396  (0.178 sec)
18-06-04 23:34-INFO->> Step 405650 run_train: loss = 5.4735  (0.126 sec)
18-06-04 23:34-INFO->> Step 405660 run_train: loss = 5.3766  (0.145 sec)
18-06-04 23:34-INFO->> Step 405670 run_train: loss = 5.4174  (0.154 sec)
18-06-04 23:34-INFO->> Step 405680 run_train: loss = 5.4117  (0.158 sec)
18-06-04 23:34-INFO->> Step 405690 run_train: loss = 5.3691  (0.169 sec)
18-06-04 23:34-INFO->> Step 405700 run_train: loss = 5.3976  (0.168 sec)
18-06-04 23:34-INFO->> Step 405710 run_train: loss = 5.4565  (0.164 sec)
18-06-04 23:34-INFO->> Step 405720 run_train: loss = 5.3947  (0.153 sec)
18-06-04 23:34-INFO->> Step 405730 run_train: loss = 5.3989  (0.169 sec)
18-06-04 23:34-INFO->> Step 405740 run_train: loss = 5.4333  (0.150 sec)
18-06-04 23:34-INFO->> Step 405750 run_train: loss = 5.4157  (0.174 sec)
18-06-04 23:34-INFO->> Step 405760 run_train: loss = 5.4793  (0.153 sec)
18-06-04 23:34-INFO->> Step 405770 run_train: loss = 5.4482  (0.170 sec)
18-06-04 23:34-INFO->> Step 405780 run_train: loss = 5.4411  (0.151 sec)
18-06-04 23:34-INFO->> Step 405790 run_train: loss = 5.4708  (0.183 sec)
18-06-04 23:34-INFO->> Step 405800 run_train: loss = 5.4121  (0.164 sec)
18-06-04 23:34-INFO->> Step 405810 run_train: loss = 5.3805  (0.161 sec)
18-06-04 23:34-INFO->> Step 405820 run_train: loss = 5.4443  (0.147 sec)
18-06-04 23:34-INFO->> Step 405830 run_train: loss = 5.3962  (0.137 sec)
18-06-04 23:34-INFO->> Step 405840 run_train: loss = 5.4108  (0.164 sec)
18-06-04 23:34-INFO->> Step 405850 run_train: loss = 5.4479  (0.156 sec)
18-06-04 23:34-INFO->> Step 405860 run_train: loss = 5.3241  (0.130 sec)
18-06-04 23:34-INFO->> Step 405870 run_train: loss = 5.4693  (0.178 sec)
18-06-04 23:34-INFO->> Step 405880 run_train: loss = 5.4669  (0.151 sec)
18-06-04 23:34-INFO->> Step 405890 run_train: loss = 5.4675  (0.139 sec)
18-06-04 23:34-INFO->> Step 405900 run_train: loss = 5.3835  (0.181 sec)
18-06-04 23:34-INFO->> Step 405910 run_train: loss = 5.4817  (0.171 sec)
18-06-04 23:35-INFO->> Step 405920 run_train: loss = 5.3740  (0.131 sec)
18-06-04 23:35-INFO->> Step 405930 run_train: loss = 5.4605  (0.142 sec)
18-06-04 23:35-INFO->> Step 405940 run_train: loss = 5.3734  (0.136 sec)
18-06-04 23:35-INFO->> Step 405950 run_train: loss = 5.3973  (0.189 sec)
18-06-04 23:35-INFO->> Step 405960 run_train: loss = 5.4368  (0.148 sec)
18-06-04 23:35-INFO->> Step 405970 run_train: loss = 5.3888  (0.171 sec)
18-06-04 23:35-INFO->> Step 405980 run_train: loss = 5.4709  (0.149 sec)
18-06-04 23:35-INFO->> Step 405990 run_train: loss = 5.4395  (0.170 sec)
18-06-04 23:35-INFO->> Step 406000 run_train: loss = 5.4302  (0.152 sec)
18-06-04 23:35-INFO->> 2018-06-04 23:35:13.206403 Saving in ckpt
18-06-04 23:35-INFO-Test Data Eval:
18-06-04 23:35-INFO-fpr95 = 0.17010660201912858 and auc = 0.9692609817835104
18-06-04 23:35-INFO->> Step 406010 run_train: loss = 5.4572  (0.102 sec)
18-06-04 23:35-INFO->> Step 406020 run_train: loss = 5.3867  (0.157 sec)
18-06-04 23:35-INFO->> Step 406030 run_train: loss = 5.3918  (0.188 sec)
18-06-04 23:35-INFO->> Step 406040 run_train: loss = 5.4368  (0.167 sec)
18-06-04 23:36-INFO->> Step 406050 run_train: loss = 5.4440  (0.191 sec)
18-06-04 23:36-INFO->> Step 406060 run_train: loss = 5.3703  (0.159 sec)
18-06-04 23:36-INFO->> Step 406070 run_train: loss = 5.4281  (0.185 sec)
18-06-04 23:36-INFO->> Step 406080 run_train: loss = 5.4422  (0.174 sec)
18-06-04 23:36-INFO->> Step 406090 run_train: loss = 5.4444  (0.157 sec)
18-06-04 23:36-INFO->> Step 406100 run_train: loss = 5.4125  (0.173 sec)
18-06-04 23:36-INFO->> Step 406110 run_train: loss = 5.3952  (0.137 sec)
18-06-04 23:36-INFO->> Step 406120 run_train: loss = 5.4106  (0.166 sec)
18-06-04 23:36-INFO->> Step 406130 run_train: loss = 5.4442  (0.146 sec)
18-06-04 23:36-INFO->> Step 406140 run_train: loss = 5.4229  (0.140 sec)
18-06-04 23:36-INFO->> Step 406150 run_train: loss = 5.4191  (0.175 sec)
18-06-04 23:36-INFO->> Step 406160 run_train: loss = 5.4268  (0.132 sec)
18-06-04 23:36-INFO->> Step 406170 run_train: loss = 5.4024  (0.175 sec)
18-06-04 23:36-INFO->> Step 406180 run_train: loss = 5.4582  (0.159 sec)
18-06-04 23:36-INFO->> Step 406190 run_train: loss = 5.3942  (0.139 sec)
18-06-04 23:36-INFO->> Step 406200 run_train: loss = 5.3825  (0.123 sec)
18-06-04 23:36-INFO->> Step 406210 run_train: loss = 5.3985  (0.181 sec)
18-06-04 23:36-INFO->> Step 406220 run_train: loss = 5.3940  (0.210 sec)
18-06-04 23:36-INFO->> Step 406230 run_train: loss = 5.4216  (0.196 sec)
18-06-04 23:36-INFO->> Step 406240 run_train: loss = 5.4687  (0.173 sec)
18-06-04 23:36-INFO->> Step 406250 run_train: loss = 5.4688  (0.160 sec)
18-06-04 23:36-INFO->> Step 406260 run_train: loss = 5.4466  (0.150 sec)
18-06-04 23:36-INFO->> Step 406270 run_train: loss = 5.4415  (0.161 sec)
18-06-04 23:36-INFO->> Step 406280 run_train: loss = 5.4249  (0.154 sec)
18-06-04 23:36-INFO->> Step 406290 run_train: loss = 5.4256  (0.163 sec)
18-06-04 23:36-INFO->> Step 406300 run_train: loss = 5.3738  (0.134 sec)
18-06-04 23:36-INFO->> Step 406310 run_train: loss = 5.4844  (0.140 sec)
18-06-04 23:36-INFO->> Step 406320 run_train: loss = 5.3843  (0.137 sec)
18-06-04 23:36-INFO->> Step 406330 run_train: loss = 5.4888  (0.154 sec)
18-06-04 23:36-INFO->> Step 406340 run_train: loss = 5.4506  (0.159 sec)
18-06-04 23:36-INFO->> Step 406350 run_train: loss = 5.3937  (0.138 sec)
18-06-04 23:36-INFO->> Step 406360 run_train: loss = 5.4595  (0.182 sec)
18-06-04 23:36-INFO->> Step 406370 run_train: loss = 5.3797  (0.153 sec)
18-06-04 23:36-INFO->> Step 406380 run_train: loss = 5.4445  (0.154 sec)
18-06-04 23:36-INFO->> Step 406390 run_train: loss = 5.3786  (0.132 sec)
18-06-04 23:36-INFO->> Step 406400 run_train: loss = 5.4039  (0.149 sec)
18-06-04 23:36-INFO->> Step 406410 run_train: loss = 5.4041  (0.166 sec)
18-06-04 23:37-INFO->> Step 406420 run_train: loss = 5.4686  (0.154 sec)
18-06-04 23:37-INFO->> Step 406430 run_train: loss = 5.4284  (0.140 sec)
18-06-04 23:37-INFO->> Step 406440 run_train: loss = 5.3982  (0.174 sec)
18-06-04 23:37-INFO->> Step 406450 run_train: loss = 5.3753  (0.160 sec)
18-06-04 23:37-INFO->> Step 406460 run_train: loss = 5.3947  (0.175 sec)
18-06-04 23:37-INFO->> Step 406470 run_train: loss = 5.4028  (0.120 sec)
18-06-04 23:37-INFO->> Step 406480 run_train: loss = 5.4532  (0.130 sec)
18-06-04 23:37-INFO->> Step 406490 run_train: loss = 5.4066  (0.120 sec)
18-06-04 23:37-INFO->> Step 406500 run_train: loss = 5.3522  (0.171 sec)
18-06-04 23:37-INFO->> Step 406510 run_train: loss = 5.4553  (0.145 sec)
18-06-04 23:37-INFO->> Step 406520 run_train: loss = 5.4284  (0.146 sec)
18-06-04 23:37-INFO->> Step 406530 run_train: loss = 5.4414  (0.158 sec)
18-06-04 23:37-INFO->> Step 406540 run_train: loss = 5.4717  (0.144 sec)
18-06-04 23:37-INFO->> Step 406550 run_train: loss = 5.4673  (0.155 sec)
18-06-04 23:37-INFO->> Step 406560 run_train: loss = 5.3494  (0.182 sec)
18-06-04 23:37-INFO->> Step 406570 run_train: loss = 5.4156  (0.160 sec)
18-06-04 23:37-INFO->> Step 406580 run_train: loss = 5.4245  (0.145 sec)
18-06-04 23:37-INFO->> Step 406590 run_train: loss = 5.4145  (0.162 sec)
18-06-04 23:37-INFO->> Step 406600 run_train: loss = 5.4924  (0.185 sec)
18-06-04 23:37-INFO->> Step 406610 run_train: loss = 5.4464  (0.174 sec)
18-06-04 23:37-INFO->> Step 406620 run_train: loss = 5.4339  (0.136 sec)
18-06-04 23:37-INFO->> Step 406630 run_train: loss = 5.3989  (0.149 sec)
18-06-04 23:37-INFO->> Step 406640 run_train: loss = 5.4073  (0.182 sec)
18-06-04 23:37-INFO->> Step 406650 run_train: loss = 5.4257  (0.152 sec)
18-06-04 23:37-INFO->> Step 406660 run_train: loss = 5.4566  (0.188 sec)
18-06-04 23:37-INFO->> Step 406670 run_train: loss = 5.4632  (0.182 sec)
18-06-04 23:37-INFO->> Step 406680 run_train: loss = 5.3627  (0.181 sec)
18-06-04 23:37-INFO->> Step 406690 run_train: loss = 5.4142  (0.181 sec)
18-06-04 23:37-INFO->> Step 406700 run_train: loss = 5.4670  (0.156 sec)
18-06-04 23:37-INFO->> Step 406710 run_train: loss = 5.3762  (0.160 sec)
18-06-04 23:37-INFO->> Step 406720 run_train: loss = 5.4752  (0.161 sec)
18-06-04 23:37-INFO->> Step 406730 run_train: loss = 5.3538  (0.160 sec)
18-06-04 23:37-INFO->> Step 406740 run_train: loss = 5.4193  (0.150 sec)
18-06-04 23:37-INFO->> Step 406750 run_train: loss = 5.3751  (0.172 sec)
18-06-04 23:37-INFO->> Step 406760 run_train: loss = 5.4146  (0.125 sec)
18-06-04 23:37-INFO->> Step 406770 run_train: loss = 5.4242  (0.135 sec)
18-06-04 23:37-INFO->> Step 406780 run_train: loss = 5.4046  (0.131 sec)
18-06-04 23:37-INFO->> Step 406790 run_train: loss = 5.4355  (0.129 sec)
18-06-04 23:38-INFO->> Step 406800 run_train: loss = 5.4259  (0.165 sec)
18-06-04 23:38-INFO->> Step 406810 run_train: loss = 5.4385  (0.179 sec)
18-06-04 23:38-INFO->> Step 406820 run_train: loss = 5.3657  (0.187 sec)
18-06-04 23:38-INFO->> Step 406830 run_train: loss = 5.3888  (0.159 sec)
18-06-04 23:38-INFO->> Step 406840 run_train: loss = 5.3490  (0.182 sec)
18-06-04 23:38-INFO->> Step 406850 run_train: loss = 5.4380  (0.138 sec)
18-06-04 23:38-INFO->> Step 406860 run_train: loss = 5.4532  (0.142 sec)
18-06-04 23:38-INFO->> Step 406870 run_train: loss = 5.4549  (0.180 sec)
18-06-04 23:38-INFO->> Step 406880 run_train: loss = 5.3890  (0.113 sec)
18-06-04 23:38-INFO->> Step 406890 run_train: loss = 5.4323  (0.183 sec)
18-06-04 23:38-INFO->> Step 406900 run_train: loss = 5.4247  (0.144 sec)
18-06-04 23:38-INFO->> Step 406910 run_train: loss = 5.4523  (0.145 sec)
18-06-04 23:38-INFO->> Step 406920 run_train: loss = 5.4454  (0.177 sec)
18-06-04 23:38-INFO->> Step 406930 run_train: loss = 5.4271  (0.163 sec)
18-06-04 23:38-INFO->> Step 406940 run_train: loss = 5.4483  (0.141 sec)
18-06-04 23:38-INFO->> Step 406950 run_train: loss = 5.4135  (0.178 sec)
18-06-04 23:38-INFO->> Step 406960 run_train: loss = 5.4779  (0.121 sec)
18-06-04 23:38-INFO->> Step 406970 run_train: loss = 5.3450  (0.138 sec)
18-06-04 23:38-INFO->> Step 406980 run_train: loss = 5.4291  (0.173 sec)
18-06-04 23:38-INFO->> Step 406990 run_train: loss = 5.4470  (0.171 sec)
18-06-04 23:38-INFO->> Step 407000 run_train: loss = 5.4206  (0.154 sec)
18-06-04 23:38-INFO->> 2018-06-04 23:38:32.704864 Saving in ckpt
18-06-04 23:38-INFO-Test Data Eval:
18-06-04 23:39-INFO-fpr95 = 0.17372642136025504 and auc = 0.9690433237471843
18-06-04 23:39-INFO->> Step 407010 run_train: loss = 5.4396  (0.161 sec)
18-06-04 23:39-INFO->> Step 407020 run_train: loss = 5.4635  (0.166 sec)
18-06-04 23:39-INFO->> Step 407030 run_train: loss = 5.4373  (0.151 sec)
18-06-04 23:39-INFO->> Step 407040 run_train: loss = 5.3602  (0.159 sec)
18-06-04 23:39-INFO->> Step 407050 run_train: loss = 5.4379  (0.153 sec)
18-06-04 23:39-INFO->> Step 407060 run_train: loss = 5.3794  (0.169 sec)
18-06-04 23:39-INFO->> Step 407070 run_train: loss = 5.4207  (0.140 sec)
18-06-04 23:39-INFO->> Step 407080 run_train: loss = 5.4124  (0.192 sec)
18-06-04 23:39-INFO->> Step 407090 run_train: loss = 5.3983  (0.167 sec)
18-06-04 23:39-INFO->> Step 407100 run_train: loss = 5.3517  (0.230 sec)
18-06-04 23:39-INFO->> Step 407110 run_train: loss = 5.3513  (0.185 sec)
18-06-04 23:39-INFO->> Step 407120 run_train: loss = 5.4570  (0.154 sec)
18-06-04 23:39-INFO->> Step 407130 run_train: loss = 5.3831  (0.122 sec)
18-06-04 23:39-INFO->> Step 407140 run_train: loss = 5.4173  (0.129 sec)
18-06-04 23:39-INFO->> Step 407150 run_train: loss = 5.3794  (0.159 sec)
18-06-04 23:39-INFO->> Step 407160 run_train: loss = 5.4013  (0.174 sec)
18-06-04 23:39-INFO->> Step 407170 run_train: loss = 5.3811  (0.152 sec)
18-06-04 23:39-INFO->> Step 407180 run_train: loss = 5.4468  (0.159 sec)
18-06-04 23:39-INFO->> Step 407190 run_train: loss = 5.3923  (0.156 sec)
18-06-04 23:39-INFO->> Step 407200 run_train: loss = 5.4355  (0.136 sec)
18-06-04 23:39-INFO->> Step 407210 run_train: loss = 5.4191  (0.150 sec)
18-06-04 23:39-INFO->> Step 407220 run_train: loss = 5.4832  (0.171 sec)
18-06-04 23:39-INFO->> Step 407230 run_train: loss = 5.4149  (0.120 sec)
18-06-04 23:39-INFO->> Step 407240 run_train: loss = 5.4471  (0.159 sec)
18-06-04 23:39-INFO->> Step 407250 run_train: loss = 5.3872  (0.173 sec)
18-06-04 23:39-INFO->> Step 407260 run_train: loss = 5.4609  (0.129 sec)
18-06-04 23:39-INFO->> Step 407270 run_train: loss = 5.4271  (0.152 sec)
18-06-04 23:39-INFO->> Step 407280 run_train: loss = 5.4249  (0.155 sec)
18-06-04 23:39-INFO->> Step 407290 run_train: loss = 5.3826  (0.167 sec)
18-06-04 23:40-INFO->> Step 407300 run_train: loss = 5.4327  (0.130 sec)
18-06-04 23:40-INFO->> Step 407310 run_train: loss = 5.4554  (0.162 sec)
18-06-04 23:40-INFO->> Step 407320 run_train: loss = 5.4173  (0.150 sec)
18-06-04 23:40-INFO->> Step 407330 run_train: loss = 5.4440  (0.203 sec)
18-06-04 23:40-INFO->> Step 407340 run_train: loss = 5.4261  (0.156 sec)
18-06-04 23:40-INFO->> Step 407350 run_train: loss = 5.4667  (0.161 sec)
18-06-04 23:40-INFO->> Step 407360 run_train: loss = 5.4936  (0.165 sec)
18-06-04 23:40-INFO->> Step 407370 run_train: loss = 5.3779  (0.185 sec)
18-06-04 23:40-INFO->> Step 407380 run_train: loss = 5.4356  (0.151 sec)
18-06-04 23:40-INFO->> Step 407390 run_train: loss = 5.4038  (0.177 sec)
18-06-04 23:40-INFO->> Step 407400 run_train: loss = 5.4277  (0.120 sec)
18-06-04 23:40-INFO->> Step 407410 run_train: loss = 5.4298  (0.166 sec)
18-06-04 23:40-INFO->> Step 407420 run_train: loss = 5.3672  (0.153 sec)
18-06-04 23:40-INFO->> Step 407430 run_train: loss = 5.4301  (0.159 sec)
18-06-04 23:40-INFO->> Step 407440 run_train: loss = 5.4536  (0.163 sec)
18-06-04 23:40-INFO->> Step 407450 run_train: loss = 5.3886  (0.122 sec)
18-06-04 23:40-INFO->> Step 407460 run_train: loss = 5.4360  (0.181 sec)
18-06-04 23:40-INFO->> Step 407470 run_train: loss = 5.4527  (0.148 sec)
18-06-04 23:40-INFO->> Step 407480 run_train: loss = 5.4421  (0.169 sec)
18-06-04 23:40-INFO->> Step 407490 run_train: loss = 5.4573  (0.129 sec)
18-06-04 23:40-INFO->> Step 407500 run_train: loss = 5.4161  (0.142 sec)
18-06-04 23:40-INFO->> Step 407510 run_train: loss = 5.3852  (0.149 sec)
18-06-04 23:40-INFO->> Step 407520 run_train: loss = 5.4309  (0.141 sec)
18-06-04 23:40-INFO->> Step 407530 run_train: loss = 5.4411  (0.156 sec)
18-06-04 23:40-INFO->> Step 407540 run_train: loss = 5.4550  (0.148 sec)
18-06-04 23:40-INFO->> Step 407550 run_train: loss = 5.4498  (0.177 sec)
18-06-04 23:40-INFO->> Step 407560 run_train: loss = 5.3508  (0.158 sec)
18-06-04 23:40-INFO->> Step 407570 run_train: loss = 5.3828  (0.192 sec)
18-06-04 23:40-INFO->> Step 407580 run_train: loss = 5.4273  (0.158 sec)
18-06-04 23:40-INFO->> Step 407590 run_train: loss = 5.4485  (0.154 sec)
18-06-04 23:40-INFO->> Step 407600 run_train: loss = 5.3892  (0.174 sec)
18-06-04 23:40-INFO->> Step 407610 run_train: loss = 5.4407  (0.134 sec)
18-06-04 23:40-INFO->> Step 407620 run_train: loss = 5.4103  (0.176 sec)
18-06-04 23:40-INFO->> Step 407630 run_train: loss = 5.4717  (0.169 sec)
18-06-04 23:40-INFO->> Step 407640 run_train: loss = 5.4526  (0.175 sec)
18-06-04 23:40-INFO->> Step 407650 run_train: loss = 5.3782  (0.156 sec)
18-06-04 23:40-INFO->> Step 407660 run_train: loss = 5.3844  (0.130 sec)
18-06-04 23:40-INFO->> Step 407670 run_train: loss = 5.4452  (0.195 sec)
18-06-04 23:41-INFO->> Step 407680 run_train: loss = 5.3964  (0.154 sec)
18-06-04 23:41-INFO->> Step 407690 run_train: loss = 5.4000  (0.166 sec)
18-06-04 23:41-INFO->> Step 407700 run_train: loss = 5.3641  (0.184 sec)
18-06-04 23:41-INFO->> Step 407710 run_train: loss = 5.4487  (0.154 sec)
18-06-04 23:41-INFO->> Step 407720 run_train: loss = 5.4293  (0.179 sec)
18-06-04 23:41-INFO->> Step 407730 run_train: loss = 5.3965  (0.172 sec)
18-06-04 23:41-INFO->> Step 407740 run_train: loss = 5.4275  (0.147 sec)
18-06-04 23:41-INFO->> Step 407750 run_train: loss = 5.4828  (0.157 sec)
18-06-04 23:41-INFO->> Step 407760 run_train: loss = 5.3506  (0.158 sec)
18-06-04 23:41-INFO->> Step 407770 run_train: loss = 5.4345  (0.177 sec)
18-06-04 23:41-INFO->> Step 407780 run_train: loss = 5.4275  (0.149 sec)
18-06-04 23:41-INFO->> Step 407790 run_train: loss = 5.4811  (0.194 sec)
18-06-04 23:41-INFO->> Step 407800 run_train: loss = 5.4159  (0.167 sec)
18-06-04 23:41-INFO->> Step 407810 run_train: loss = 5.3913  (0.120 sec)
18-06-04 23:41-INFO->> Step 407820 run_train: loss = 5.4117  (0.160 sec)
18-06-04 23:41-INFO->> Step 407830 run_train: loss = 5.4338  (0.138 sec)
18-06-04 23:41-INFO->> Step 407840 run_train: loss = 5.4722  (0.136 sec)
18-06-04 23:41-INFO->> Step 407850 run_train: loss = 5.4205  (0.156 sec)
18-06-04 23:41-INFO->> Step 407860 run_train: loss = 5.4116  (0.159 sec)
18-06-04 23:41-INFO->> Step 407870 run_train: loss = 5.4269  (0.160 sec)
18-06-04 23:41-INFO->> Step 407880 run_train: loss = 5.3593  (0.161 sec)
18-06-04 23:41-INFO->> Step 407890 run_train: loss = 5.3376  (0.146 sec)
18-06-04 23:41-INFO->> Step 407900 run_train: loss = 5.4588  (0.166 sec)
18-06-04 23:41-INFO->> Step 407910 run_train: loss = 5.4307  (0.174 sec)
18-06-04 23:41-INFO->> Step 407920 run_train: loss = 5.4037  (0.175 sec)
18-06-04 23:41-INFO->> Step 407930 run_train: loss = 5.4348  (0.151 sec)
18-06-04 23:41-INFO->> Step 407940 run_train: loss = 5.4536  (0.127 sec)
18-06-04 23:41-INFO->> Step 407950 run_train: loss = 5.4736  (0.157 sec)
18-06-04 23:41-INFO->> Step 407960 run_train: loss = 5.4124  (0.150 sec)
18-06-04 23:41-INFO->> Step 407970 run_train: loss = 5.3877  (0.170 sec)
18-06-04 23:41-INFO->> Step 407980 run_train: loss = 5.3856  (0.168 sec)
18-06-04 23:41-INFO->> Step 407990 run_train: loss = 5.3895  (0.139 sec)
18-06-04 23:41-INFO->> Step 408000 run_train: loss = 5.3797  (0.168 sec)
18-06-04 23:41-INFO->> 2018-06-04 23:41:51.671168 Saving in ckpt
18-06-04 23:41-INFO-Test Data Eval:
18-06-04 23:42-INFO-fpr95 = 0.1734441418703507 and auc = 0.9692107486342191
18-06-04 23:42-INFO->> Step 408010 run_train: loss = 5.3775  (0.175 sec)
18-06-04 23:42-INFO->> Step 408020 run_train: loss = 5.4654  (0.190 sec)
18-06-04 23:42-INFO->> Step 408030 run_train: loss = 5.4267  (0.182 sec)
18-06-04 23:42-INFO->> Step 408040 run_train: loss = 5.4504  (0.163 sec)
18-06-04 23:42-INFO->> Step 408050 run_train: loss = 5.3917  (0.199 sec)
18-06-04 23:42-INFO->> Step 408060 run_train: loss = 5.4274  (0.134 sec)
18-06-04 23:42-INFO->> Step 408070 run_train: loss = 5.4589  (0.155 sec)
18-06-04 23:42-INFO->> Step 408080 run_train: loss = 5.4441  (0.154 sec)
18-06-04 23:42-INFO->> Step 408090 run_train: loss = 5.4336  (0.149 sec)
18-06-04 23:42-INFO->> Step 408100 run_train: loss = 5.4548  (0.166 sec)
18-06-04 23:42-INFO->> Step 408110 run_train: loss = 5.3868  (0.154 sec)
18-06-04 23:42-INFO->> Step 408120 run_train: loss = 5.3727  (0.131 sec)
18-06-04 23:42-INFO->> Step 408130 run_train: loss = 5.4453  (0.157 sec)
18-06-04 23:42-INFO->> Step 408140 run_train: loss = 5.3982  (0.173 sec)
18-06-04 23:42-INFO->> Step 408150 run_train: loss = 5.3919  (0.203 sec)
18-06-04 23:42-INFO->> Step 408160 run_train: loss = 5.4445  (0.168 sec)
18-06-04 23:42-INFO->> Step 408170 run_train: loss = 5.3715  (0.142 sec)
18-06-04 23:43-INFO->> Step 408180 run_train: loss = 5.4525  (0.151 sec)
18-06-04 23:43-INFO->> Step 408190 run_train: loss = 5.3866  (0.127 sec)
18-06-04 23:43-INFO->> Step 408200 run_train: loss = 5.4230  (0.187 sec)
18-06-04 23:43-INFO->> Step 408210 run_train: loss = 5.3747  (0.174 sec)
18-06-04 23:43-INFO->> Step 408220 run_train: loss = 5.4776  (0.162 sec)
18-06-04 23:43-INFO->> Step 408230 run_train: loss = 5.5103  (0.151 sec)
18-06-04 23:43-INFO->> Step 408240 run_train: loss = 5.4640  (0.150 sec)
18-06-04 23:43-INFO->> Step 408250 run_train: loss = 5.4381  (0.158 sec)
18-06-04 23:43-INFO->> Step 408260 run_train: loss = 5.3677  (0.192 sec)
18-06-04 23:43-INFO->> Step 408270 run_train: loss = 5.4536  (0.140 sec)
18-06-04 23:43-INFO->> Step 408280 run_train: loss = 5.4276  (0.154 sec)
18-06-04 23:43-INFO->> Step 408290 run_train: loss = 5.4073  (0.204 sec)
18-06-04 23:43-INFO->> Step 408300 run_train: loss = 5.4285  (0.169 sec)
18-06-04 23:43-INFO->> Step 408310 run_train: loss = 5.4279  (0.163 sec)
18-06-04 23:43-INFO->> Step 408320 run_train: loss = 5.4386  (0.162 sec)
18-06-04 23:43-INFO->> Step 408330 run_train: loss = 5.4314  (0.152 sec)
18-06-04 23:43-INFO->> Step 408340 run_train: loss = 5.4083  (0.179 sec)
18-06-04 23:43-INFO->> Step 408350 run_train: loss = 5.3981  (0.132 sec)
18-06-04 23:43-INFO->> Step 408360 run_train: loss = 5.4438  (0.156 sec)
18-06-04 23:43-INFO->> Step 408370 run_train: loss = 5.4485  (0.144 sec)
18-06-04 23:43-INFO->> Step 408380 run_train: loss = 5.4340  (0.162 sec)
18-06-04 23:43-INFO->> Step 408390 run_train: loss = 5.4732  (0.163 sec)
18-06-04 23:43-INFO->> Step 408400 run_train: loss = 5.4225  (0.154 sec)
18-06-04 23:43-INFO->> Step 408410 run_train: loss = 5.3750  (0.156 sec)
18-06-04 23:43-INFO->> Step 408420 run_train: loss = 5.4137  (0.169 sec)
18-06-04 23:43-INFO->> Step 408430 run_train: loss = 5.4575  (0.132 sec)
18-06-04 23:43-INFO->> Step 408440 run_train: loss = 5.4152  (0.152 sec)
18-06-04 23:43-INFO->> Step 408450 run_train: loss = 5.4295  (0.133 sec)
18-06-04 23:43-INFO->> Step 408460 run_train: loss = 5.3677  (0.148 sec)
18-06-04 23:43-INFO->> Step 408470 run_train: loss = 5.3191  (0.167 sec)
18-06-04 23:43-INFO->> Step 408480 run_train: loss = 5.4177  (0.151 sec)
18-06-04 23:43-INFO->> Step 408490 run_train: loss = 5.3278  (0.172 sec)
18-06-04 23:43-INFO->> Step 408500 run_train: loss = 5.4262  (0.181 sec)
18-06-04 23:43-INFO->> Step 408510 run_train: loss = 5.4494  (0.154 sec)
18-06-04 23:43-INFO->> Step 408520 run_train: loss = 5.4334  (0.147 sec)
18-06-04 23:43-INFO->> Step 408530 run_train: loss = 5.3552  (0.165 sec)
18-06-04 23:43-INFO->> Step 408540 run_train: loss = 5.4395  (0.167 sec)
18-06-04 23:43-INFO->> Step 408550 run_train: loss = 5.4042  (0.166 sec)
18-06-04 23:44-INFO->> Step 408560 run_train: loss = 5.4471  (0.131 sec)
18-06-04 23:44-INFO->> Step 408570 run_train: loss = 5.3932  (0.140 sec)
18-06-04 23:44-INFO->> Step 408580 run_train: loss = 5.3963  (0.171 sec)
18-06-04 23:44-INFO->> Step 408590 run_train: loss = 5.4462  (0.177 sec)
18-06-04 23:44-INFO->> Step 408600 run_train: loss = 5.3772  (0.143 sec)
18-06-04 23:44-INFO->> Step 408610 run_train: loss = 5.4227  (0.155 sec)
18-06-04 23:44-INFO->> Step 408620 run_train: loss = 5.3919  (0.135 sec)
18-06-04 23:44-INFO->> Step 408630 run_train: loss = 5.3777  (0.131 sec)
18-06-04 23:44-INFO->> Step 408640 run_train: loss = 5.4173  (0.170 sec)
18-06-04 23:44-INFO->> Step 408650 run_train: loss = 5.4546  (0.165 sec)
18-06-04 23:44-INFO->> Step 408660 run_train: loss = 5.3890  (0.139 sec)
18-06-04 23:44-INFO->> Step 408670 run_train: loss = 5.4212  (0.122 sec)
18-06-04 23:44-INFO->> Step 408680 run_train: loss = 5.4204  (0.158 sec)
18-06-04 23:44-INFO->> Step 408690 run_train: loss = 5.4376  (0.159 sec)
18-06-04 23:44-INFO->> Step 408700 run_train: loss = 5.3520  (0.152 sec)
18-06-04 23:44-INFO->> Step 408710 run_train: loss = 5.3913  (0.165 sec)
18-06-04 23:44-INFO->> Step 408720 run_train: loss = 5.4364  (0.109 sec)
18-06-04 23:44-INFO->> Step 408730 run_train: loss = 5.4266  (0.172 sec)
18-06-04 23:44-INFO->> Step 408740 run_train: loss = 5.4089  (0.151 sec)
18-06-04 23:44-INFO->> Step 408750 run_train: loss = 5.4129  (0.185 sec)
18-06-04 23:44-INFO->> Step 408760 run_train: loss = 5.4614  (0.142 sec)
18-06-04 23:44-INFO->> Step 408770 run_train: loss = 5.4151  (0.151 sec)
18-06-04 23:44-INFO->> Step 408780 run_train: loss = 5.4163  (0.167 sec)
18-06-04 23:44-INFO->> Step 408790 run_train: loss = 5.4276  (0.179 sec)
18-06-04 23:44-INFO->> Step 408800 run_train: loss = 5.4226  (0.154 sec)
18-06-04 23:44-INFO->> Step 408810 run_train: loss = 5.4928  (0.187 sec)
18-06-04 23:44-INFO->> Step 408820 run_train: loss = 5.4687  (0.153 sec)
18-06-04 23:44-INFO->> Step 408830 run_train: loss = 5.4411  (0.161 sec)
18-06-04 23:44-INFO->> Step 408840 run_train: loss = 5.3940  (0.181 sec)
18-06-04 23:44-INFO->> Step 408850 run_train: loss = 5.4267  (0.183 sec)
18-06-04 23:44-INFO->> Step 408860 run_train: loss = 5.4449  (0.157 sec)
18-06-04 23:44-INFO->> Step 408870 run_train: loss = 5.4370  (0.111 sec)
18-06-04 23:44-INFO->> Step 408880 run_train: loss = 5.4528  (0.156 sec)
18-06-04 23:44-INFO->> Step 408890 run_train: loss = 5.4143  (0.159 sec)
18-06-04 23:44-INFO->> Step 408900 run_train: loss = 5.4473  (0.144 sec)
18-06-04 23:44-INFO->> Step 408910 run_train: loss = 5.4460  (0.124 sec)
18-06-04 23:44-INFO->> Step 408920 run_train: loss = 5.4256  (0.147 sec)
18-06-04 23:44-INFO->> Step 408930 run_train: loss = 5.3788  (0.142 sec)
18-06-04 23:45-INFO->> Step 408940 run_train: loss = 5.4084  (0.135 sec)
18-06-04 23:45-INFO->> Step 408950 run_train: loss = 5.4538  (0.158 sec)
18-06-04 23:45-INFO->> Step 408960 run_train: loss = 5.3997  (0.128 sec)
18-06-04 23:45-INFO->> Step 408970 run_train: loss = 5.4290  (0.185 sec)
18-06-04 23:45-INFO->> Step 408980 run_train: loss = 5.4054  (0.155 sec)
18-06-04 23:45-INFO->> Step 408990 run_train: loss = 5.4006  (0.176 sec)
18-06-04 23:45-INFO->> Step 409000 run_train: loss = 5.4243  (0.128 sec)
18-06-04 23:45-INFO->> 2018-06-04 23:45:10.688717 Saving in ckpt
18-06-04 23:45-INFO-Test Data Eval:
18-06-04 23:45-INFO-fpr95 = 0.17483893464399575 and auc = 0.9690940781706407
18-06-04 23:45-INFO->> Step 409010 run_train: loss = 5.4748  (0.122 sec)
18-06-04 23:45-INFO->> Step 409020 run_train: loss = 5.4397  (0.196 sec)
18-06-04 23:45-INFO->> Step 409030 run_train: loss = 5.4410  (0.184 sec)
18-06-04 23:45-INFO->> Step 409040 run_train: loss = 5.3076  (0.166 sec)
18-06-04 23:45-INFO->> Step 409050 run_train: loss = 5.3991  (0.167 sec)
18-06-04 23:46-INFO->> Step 409060 run_train: loss = 5.3998  (0.142 sec)
18-06-04 23:46-INFO->> Step 409070 run_train: loss = 5.4686  (0.154 sec)
18-06-04 23:46-INFO->> Step 409080 run_train: loss = 5.4457  (0.152 sec)
18-06-04 23:46-INFO->> Step 409090 run_train: loss = 5.4010  (0.171 sec)
18-06-04 23:46-INFO->> Step 409100 run_train: loss = 5.4226  (0.163 sec)
18-06-04 23:46-INFO->> Step 409110 run_train: loss = 5.4423  (0.158 sec)
18-06-04 23:46-INFO->> Step 409120 run_train: loss = 5.3980  (0.195 sec)
18-06-04 23:46-INFO->> Step 409130 run_train: loss = 5.3917  (0.176 sec)
18-06-04 23:46-INFO->> Step 409140 run_train: loss = 5.3487  (0.147 sec)
18-06-04 23:46-INFO->> Step 409150 run_train: loss = 5.4557  (0.131 sec)
18-06-04 23:46-INFO->> Step 409160 run_train: loss = 5.4183  (0.115 sec)
18-06-04 23:46-INFO->> Step 409170 run_train: loss = 5.4293  (0.160 sec)
18-06-04 23:46-INFO->> Step 409180 run_train: loss = 5.4030  (0.159 sec)
18-06-04 23:46-INFO->> Step 409190 run_train: loss = 5.4648  (0.176 sec)
18-06-04 23:46-INFO->> Step 409200 run_train: loss = 5.4127  (0.160 sec)
18-06-04 23:46-INFO->> Step 409210 run_train: loss = 5.4399  (0.164 sec)
18-06-04 23:46-INFO->> Step 409220 run_train: loss = 5.4645  (0.168 sec)
18-06-04 23:46-INFO->> Step 409230 run_train: loss = 5.4503  (0.163 sec)
18-06-04 23:46-INFO->> Step 409240 run_train: loss = 5.4097  (0.161 sec)
18-06-04 23:46-INFO->> Step 409250 run_train: loss = 5.4130  (0.139 sec)
18-06-04 23:46-INFO->> Step 409260 run_train: loss = 5.3843  (0.172 sec)
18-06-04 23:46-INFO->> Step 409270 run_train: loss = 5.3888  (0.169 sec)
18-06-04 23:46-INFO->> Step 409280 run_train: loss = 5.4453  (0.133 sec)
18-06-04 23:46-INFO->> Step 409290 run_train: loss = 5.4227  (0.189 sec)
18-06-04 23:46-INFO->> Step 409300 run_train: loss = 5.4301  (0.180 sec)
18-06-04 23:46-INFO->> Step 409310 run_train: loss = 5.4772  (0.158 sec)
18-06-04 23:46-INFO->> Step 409320 run_train: loss = 5.4614  (0.147 sec)
18-06-04 23:46-INFO->> Step 409330 run_train: loss = 5.4428  (0.118 sec)
18-06-04 23:46-INFO->> Step 409340 run_train: loss = 5.3940  (0.145 sec)
18-06-04 23:46-INFO->> Step 409350 run_train: loss = 5.3688  (0.148 sec)
18-06-04 23:46-INFO->> Step 409360 run_train: loss = 5.3992  (0.148 sec)
18-06-04 23:46-INFO->> Step 409370 run_train: loss = 5.4096  (0.169 sec)
18-06-04 23:46-INFO->> Step 409380 run_train: loss = 5.4296  (0.117 sec)
18-06-04 23:46-INFO->> Step 409390 run_train: loss = 5.4018  (0.147 sec)
18-06-04 23:46-INFO->> Step 409400 run_train: loss = 5.4089  (0.160 sec)
18-06-04 23:46-INFO->> Step 409410 run_train: loss = 5.4491  (0.147 sec)
18-06-04 23:46-INFO->> Step 409420 run_train: loss = 5.4555  (0.165 sec)
18-06-04 23:46-INFO->> Step 409430 run_train: loss = 5.4385  (0.153 sec)
18-06-04 23:47-INFO->> Step 409440 run_train: loss = 5.4077  (0.142 sec)
18-06-04 23:47-INFO->> Step 409450 run_train: loss = 5.3790  (0.195 sec)
18-06-04 23:47-INFO->> Step 409460 run_train: loss = 5.4006  (0.141 sec)
18-06-04 23:47-INFO->> Step 409470 run_train: loss = 5.4433  (0.159 sec)
18-06-04 23:47-INFO->> Step 409480 run_train: loss = 5.3748  (0.155 sec)
18-06-04 23:47-INFO->> Step 409490 run_train: loss = 5.3703  (0.165 sec)
18-06-04 23:47-INFO->> Step 409500 run_train: loss = 5.4286  (0.147 sec)
18-06-04 23:47-INFO->> Step 409510 run_train: loss = 5.4147  (0.123 sec)
18-06-04 23:47-INFO->> Step 409520 run_train: loss = 5.4315  (0.171 sec)
18-06-04 23:47-INFO->> Step 409530 run_train: loss = 5.4396  (0.177 sec)
18-06-04 23:47-INFO->> Step 409540 run_train: loss = 5.3785  (0.202 sec)
18-06-04 23:47-INFO->> Step 409550 run_train: loss = 5.4171  (0.171 sec)
18-06-04 23:47-INFO->> Step 409560 run_train: loss = 5.3668  (0.134 sec)
18-06-04 23:47-INFO->> Step 409570 run_train: loss = 5.4380  (0.153 sec)
18-06-04 23:47-INFO->> Step 409580 run_train: loss = 5.4516  (0.175 sec)
18-06-04 23:47-INFO->> Step 409590 run_train: loss = 5.4111  (0.138 sec)
18-06-04 23:47-INFO->> Step 409600 run_train: loss = 5.4325  (0.147 sec)
18-06-04 23:47-INFO->> Step 409610 run_train: loss = 5.3900  (0.134 sec)
18-06-04 23:47-INFO->> Step 409620 run_train: loss = 5.3455  (0.122 sec)
18-06-04 23:47-INFO->> Step 409630 run_train: loss = 5.3842  (0.148 sec)
18-06-04 23:47-INFO->> Step 409640 run_train: loss = 5.4498  (0.187 sec)
18-06-04 23:47-INFO->> Step 409650 run_train: loss = 5.3784  (0.126 sec)
18-06-04 23:47-INFO->> Step 409660 run_train: loss = 5.4318  (0.167 sec)
18-06-04 23:47-INFO->> Step 409670 run_train: loss = 5.4135  (0.160 sec)
18-06-04 23:47-INFO->> Step 409680 run_train: loss = 5.3486  (0.147 sec)
18-06-04 23:47-INFO->> Step 409690 run_train: loss = 5.4648  (0.203 sec)
18-06-04 23:47-INFO->> Step 409700 run_train: loss = 5.4472  (0.151 sec)
18-06-04 23:47-INFO->> Step 409710 run_train: loss = 5.4859  (0.138 sec)
18-06-04 23:47-INFO->> Step 409720 run_train: loss = 5.3962  (0.140 sec)
18-06-04 23:47-INFO->> Step 409730 run_train: loss = 5.4639  (0.150 sec)
18-06-04 23:47-INFO->> Step 409740 run_train: loss = 5.4912  (0.172 sec)
18-06-04 23:47-INFO->> Step 409750 run_train: loss = 5.4945  (0.197 sec)
18-06-04 23:47-INFO->> Step 409760 run_train: loss = 5.4367  (0.152 sec)
18-06-04 23:47-INFO->> Step 409770 run_train: loss = 5.4269  (0.189 sec)
18-06-04 23:47-INFO->> Step 409780 run_train: loss = 5.4434  (0.155 sec)
18-06-04 23:47-INFO->> Step 409790 run_train: loss = 5.4413  (0.159 sec)
18-06-04 23:47-INFO->> Step 409800 run_train: loss = 5.4675  (0.160 sec)
18-06-04 23:47-INFO->> Step 409810 run_train: loss = 5.4093  (0.134 sec)
18-06-04 23:48-INFO->> Step 409820 run_train: loss = 5.4045  (0.160 sec)
18-06-04 23:48-INFO->> Step 409830 run_train: loss = 5.4453  (0.151 sec)
18-06-04 23:48-INFO->> Step 409840 run_train: loss = 5.3541  (0.186 sec)
18-06-04 23:48-INFO->> Step 409850 run_train: loss = 5.4007  (0.160 sec)
18-06-04 23:48-INFO->> Step 409860 run_train: loss = 5.3780  (0.131 sec)
18-06-04 23:48-INFO->> Step 409870 run_train: loss = 5.4223  (0.164 sec)
18-06-04 23:48-INFO->> Step 409880 run_train: loss = 5.3808  (0.153 sec)
18-06-04 23:48-INFO->> Step 409890 run_train: loss = 5.3903  (0.165 sec)
18-06-04 23:48-INFO->> Step 409900 run_train: loss = 5.4540  (0.127 sec)
18-06-04 23:48-INFO->> Step 409910 run_train: loss = 5.4939  (0.149 sec)
18-06-04 23:48-INFO->> Step 409920 run_train: loss = 5.3999  (0.166 sec)
18-06-04 23:48-INFO->> Step 409930 run_train: loss = 5.4021  (0.152 sec)
18-06-04 23:48-INFO->> Step 409940 run_train: loss = 5.4295  (0.162 sec)
18-06-04 23:48-INFO->> Step 409950 run_train: loss = 5.4009  (0.147 sec)
18-06-04 23:48-INFO->> Step 409960 run_train: loss = 5.3936  (0.172 sec)
18-06-04 23:48-INFO->> Step 409970 run_train: loss = 5.3939  (0.152 sec)
18-06-04 23:48-INFO->> Step 409980 run_train: loss = 5.4334  (0.154 sec)
18-06-04 23:48-INFO->> Step 409990 run_train: loss = 5.4534  (0.179 sec)
18-06-04 23:48-INFO->> Step 410000 run_train: loss = 5.4698  (0.138 sec)
18-06-04 23:48-INFO->> 2018-06-04 23:48:29.902816 Saving in ckpt
18-06-04 23:48-INFO-Test Data Eval:
18-06-04 23:49-INFO-fpr95 = 0.17255579171094582 and auc = 0.9693302440074325
18-06-04 23:49-INFO->> Step 410010 run_train: loss = 5.4324  (0.180 sec)
18-06-04 23:49-INFO->> Step 410020 run_train: loss = 5.4290  (0.165 sec)
18-06-04 23:49-INFO->> Step 410030 run_train: loss = 5.4365  (0.172 sec)
18-06-04 23:49-INFO->> Step 410040 run_train: loss = 5.4737  (0.171 sec)
18-06-04 23:49-INFO->> Step 410050 run_train: loss = 5.4409  (0.181 sec)
18-06-04 23:49-INFO->> Step 410060 run_train: loss = 5.4301  (0.128 sec)
18-06-04 23:49-INFO->> Step 410070 run_train: loss = 5.3857  (0.175 sec)
18-06-04 23:49-INFO->> Step 410080 run_train: loss = 5.4150  (0.162 sec)
18-06-04 23:49-INFO->> Step 410090 run_train: loss = 5.4516  (0.187 sec)
18-06-04 23:49-INFO->> Step 410100 run_train: loss = 5.4342  (0.160 sec)
18-06-04 23:49-INFO->> Step 410110 run_train: loss = 5.4709  (0.174 sec)
18-06-04 23:49-INFO->> Step 410120 run_train: loss = 5.4116  (0.154 sec)
18-06-04 23:49-INFO->> Step 410130 run_train: loss = 5.4824  (0.193 sec)
18-06-04 23:49-INFO->> Step 410140 run_train: loss = 5.4553  (0.172 sec)
18-06-04 23:49-INFO->> Step 410150 run_train: loss = 5.4586  (0.145 sec)
18-06-04 23:49-INFO->> Step 410160 run_train: loss = 5.4102  (0.158 sec)
18-06-04 23:49-INFO->> Step 410170 run_train: loss = 5.4449  (0.164 sec)
18-06-04 23:49-INFO->> Step 410180 run_train: loss = 5.4349  (0.150 sec)
18-06-04 23:49-INFO->> Step 410190 run_train: loss = 5.3857  (0.141 sec)
18-06-04 23:49-INFO->> Step 410200 run_train: loss = 5.3558  (0.167 sec)
18-06-04 23:49-INFO->> Step 410210 run_train: loss = 5.4278  (0.159 sec)
18-06-04 23:49-INFO->> Step 410220 run_train: loss = 5.3556  (0.164 sec)
18-06-04 23:49-INFO->> Step 410230 run_train: loss = 5.4303  (0.152 sec)
18-06-04 23:49-INFO->> Step 410240 run_train: loss = 5.4413  (0.144 sec)
18-06-04 23:49-INFO->> Step 410250 run_train: loss = 5.4439  (0.152 sec)
18-06-04 23:49-INFO->> Step 410260 run_train: loss = 5.4647  (0.184 sec)
18-06-04 23:49-INFO->> Step 410270 run_train: loss = 5.4308  (0.179 sec)
18-06-04 23:49-INFO->> Step 410280 run_train: loss = 5.4090  (0.147 sec)
18-06-04 23:49-INFO->> Step 410290 run_train: loss = 5.3927  (0.153 sec)
18-06-04 23:49-INFO->> Step 410300 run_train: loss = 5.4381  (0.166 sec)
18-06-04 23:49-INFO->> Step 410310 run_train: loss = 5.3607  (0.206 sec)
18-06-04 23:50-INFO->> Step 410320 run_train: loss = 5.4777  (0.199 sec)
18-06-04 23:50-INFO->> Step 410330 run_train: loss = 5.4832  (0.118 sec)
18-06-04 23:50-INFO->> Step 410340 run_train: loss = 5.3818  (0.172 sec)
18-06-04 23:50-INFO->> Step 410350 run_train: loss = 5.4452  (0.134 sec)
18-06-04 23:50-INFO->> Step 410360 run_train: loss = 5.4289  (0.145 sec)
18-06-04 23:50-INFO->> Step 410370 run_train: loss = 5.3902  (0.174 sec)
18-06-04 23:50-INFO->> Step 410380 run_train: loss = 5.4482  (0.175 sec)
18-06-04 23:50-INFO->> Step 410390 run_train: loss = 5.4480  (0.125 sec)
18-06-04 23:50-INFO->> Step 410400 run_train: loss = 5.4671  (0.138 sec)
18-06-04 23:50-INFO->> Step 410410 run_train: loss = 5.3580  (0.171 sec)
18-06-04 23:50-INFO->> Step 410420 run_train: loss = 5.4631  (0.128 sec)
18-06-04 23:50-INFO->> Step 410430 run_train: loss = 5.4464  (0.165 sec)
18-06-04 23:50-INFO->> Step 410440 run_train: loss = 5.4460  (0.156 sec)
18-06-04 23:50-INFO->> Step 410450 run_train: loss = 5.4577  (0.141 sec)
18-06-04 23:50-INFO->> Step 410460 run_train: loss = 5.3511  (0.129 sec)
18-06-04 23:50-INFO->> Step 410470 run_train: loss = 5.4381  (0.182 sec)
18-06-04 23:50-INFO->> Step 410480 run_train: loss = 5.4115  (0.155 sec)
18-06-04 23:50-INFO->> Step 410490 run_train: loss = 5.4212  (0.170 sec)
18-06-04 23:50-INFO->> Step 410500 run_train: loss = 5.3767  (0.172 sec)
18-06-04 23:50-INFO->> Step 410510 run_train: loss = 5.4649  (0.173 sec)
18-06-04 23:50-INFO->> Step 410520 run_train: loss = 5.4621  (0.135 sec)
18-06-04 23:50-INFO->> Step 410530 run_train: loss = 5.4372  (0.171 sec)
18-06-04 23:50-INFO->> Step 410540 run_train: loss = 5.4225  (0.146 sec)
18-06-04 23:50-INFO->> Step 410550 run_train: loss = 5.4130  (0.140 sec)
18-06-04 23:50-INFO->> Step 410560 run_train: loss = 5.4033  (0.184 sec)
18-06-04 23:50-INFO->> Step 410570 run_train: loss = 5.4441  (0.162 sec)
18-06-04 23:50-INFO->> Step 410580 run_train: loss = 5.3610  (0.112 sec)
18-06-04 23:50-INFO->> Step 410590 run_train: loss = 5.4812  (0.110 sec)
18-06-04 23:50-INFO->> Step 410600 run_train: loss = 5.3977  (0.143 sec)
18-06-04 23:50-INFO->> Step 410610 run_train: loss = 5.4554  (0.137 sec)
18-06-04 23:50-INFO->> Step 410620 run_train: loss = 5.3612  (0.127 sec)
18-06-04 23:50-INFO->> Step 410630 run_train: loss = 5.3945  (0.172 sec)
18-06-04 23:50-INFO->> Step 410640 run_train: loss = 5.4629  (0.120 sec)
18-06-04 23:50-INFO->> Step 410650 run_train: loss = 5.4665  (0.187 sec)
18-06-04 23:50-INFO->> Step 410660 run_train: loss = 5.3574  (0.150 sec)
18-06-04 23:50-INFO->> Step 410670 run_train: loss = 5.4305  (0.167 sec)
18-06-04 23:50-INFO->> Step 410680 run_train: loss = 5.4844  (0.163 sec)
18-06-04 23:51-INFO->> Step 410690 run_train: loss = 5.4182  (0.151 sec)
18-06-04 23:51-INFO->> Step 410700 run_train: loss = 5.3684  (0.153 sec)
18-06-04 23:51-INFO->> Step 410710 run_train: loss = 5.3707  (0.149 sec)
18-06-04 23:51-INFO->> Step 410720 run_train: loss = 5.4291  (0.200 sec)
18-06-04 23:51-INFO->> Step 410730 run_train: loss = 5.3449  (0.136 sec)
18-06-04 23:51-INFO->> Step 410740 run_train: loss = 5.3865  (0.162 sec)
18-06-04 23:51-INFO->> Step 410750 run_train: loss = 5.4369  (0.162 sec)
18-06-04 23:51-INFO->> Step 410760 run_train: loss = 5.4397  (0.160 sec)
18-06-04 23:51-INFO->> Step 410770 run_train: loss = 5.4327  (0.142 sec)
18-06-04 23:51-INFO->> Step 410780 run_train: loss = 5.3923  (0.124 sec)
18-06-04 23:51-INFO->> Step 410790 run_train: loss = 5.3874  (0.149 sec)
18-06-04 23:51-INFO->> Step 410800 run_train: loss = 5.4002  (0.179 sec)
18-06-04 23:51-INFO->> Step 410810 run_train: loss = 5.4084  (0.182 sec)
18-06-04 23:51-INFO->> Step 410820 run_train: loss = 5.3772  (0.158 sec)
18-06-04 23:51-INFO->> Step 410830 run_train: loss = 5.3880  (0.161 sec)
18-06-04 23:51-INFO->> Step 410840 run_train: loss = 5.4374  (0.181 sec)
18-06-04 23:51-INFO->> Step 410850 run_train: loss = 5.4318  (0.174 sec)
18-06-04 23:51-INFO->> Step 410860 run_train: loss = 5.4631  (0.153 sec)
18-06-04 23:51-INFO->> Step 410870 run_train: loss = 5.4225  (0.168 sec)
18-06-04 23:51-INFO->> Step 410880 run_train: loss = 5.3416  (0.160 sec)
18-06-04 23:51-INFO->> Step 410890 run_train: loss = 5.4473  (0.193 sec)
18-06-04 23:51-INFO->> Step 410900 run_train: loss = 5.4297  (0.144 sec)
18-06-04 23:51-INFO->> Step 410910 run_train: loss = 5.4058  (0.151 sec)
18-06-04 23:51-INFO->> Step 410920 run_train: loss = 5.3955  (0.153 sec)
18-06-04 23:51-INFO->> Step 410930 run_train: loss = 5.4337  (0.135 sec)
18-06-04 23:51-INFO->> Step 410940 run_train: loss = 5.4363  (0.156 sec)
18-06-04 23:51-INFO->> Step 410950 run_train: loss = 5.3842  (0.180 sec)
18-06-04 23:51-INFO->> Step 410960 run_train: loss = 5.3773  (0.162 sec)
18-06-04 23:51-INFO->> Step 410970 run_train: loss = 5.4206  (0.158 sec)
18-06-04 23:51-INFO->> Step 410980 run_train: loss = 5.4729  (0.166 sec)
18-06-04 23:51-INFO->> Step 410990 run_train: loss = 5.4132  (0.154 sec)
18-06-04 23:51-INFO->> Step 411000 run_train: loss = 5.4403  (0.132 sec)
18-06-04 23:51-INFO->> 2018-06-04 23:51:49.413770 Saving in ckpt
18-06-04 23:51-INFO-Test Data Eval:
18-06-04 23:52-INFO-fpr95 = 0.1697412991498406 and auc = 0.969481863930705
18-06-04 23:52-INFO->> Step 411010 run_train: loss = 5.4006  (0.151 sec)
18-06-04 23:52-INFO->> Step 411020 run_train: loss = 5.3780  (0.151 sec)
18-06-04 23:52-INFO->> Step 411030 run_train: loss = 5.5065  (0.162 sec)
18-06-04 23:52-INFO->> Step 411040 run_train: loss = 5.3718  (0.131 sec)
18-06-04 23:52-INFO->> Step 411050 run_train: loss = 5.4407  (0.111 sec)
18-06-04 23:52-INFO->> Step 411060 run_train: loss = 5.4450  (0.160 sec)
18-06-04 23:52-INFO->> Step 411070 run_train: loss = 5.4107  (0.154 sec)
18-06-04 23:52-INFO->> Step 411080 run_train: loss = 5.3201  (0.200 sec)
18-06-04 23:52-INFO->> Step 411090 run_train: loss = 5.4052  (0.143 sec)
18-06-04 23:52-INFO->> Step 411100 run_train: loss = 5.4331  (0.196 sec)
18-06-04 23:52-INFO->> Step 411110 run_train: loss = 5.4353  (0.160 sec)
18-06-04 23:52-INFO->> Step 411120 run_train: loss = 5.4627  (0.182 sec)
18-06-04 23:52-INFO->> Step 411130 run_train: loss = 5.4164  (0.162 sec)
18-06-04 23:52-INFO->> Step 411140 run_train: loss = 5.4158  (0.141 sec)
18-06-04 23:52-INFO->> Step 411150 run_train: loss = 5.4772  (0.189 sec)
18-06-04 23:52-INFO->> Step 411160 run_train: loss = 5.4220  (0.191 sec)
18-06-04 23:52-INFO->> Step 411170 run_train: loss = 5.4334  (0.160 sec)
18-06-04 23:52-INFO->> Step 411180 run_train: loss = 5.4231  (0.141 sec)
18-06-04 23:52-INFO->> Step 411190 run_train: loss = 5.3850  (0.174 sec)
18-06-04 23:53-INFO->> Step 411200 run_train: loss = 5.3960  (0.153 sec)
18-06-04 23:53-INFO->> Step 411210 run_train: loss = 5.4403  (0.149 sec)
18-06-04 23:53-INFO->> Step 411220 run_train: loss = 5.3981  (0.139 sec)
18-06-04 23:53-INFO->> Step 411230 run_train: loss = 5.4401  (0.162 sec)
18-06-04 23:53-INFO->> Step 411240 run_train: loss = 5.4296  (0.179 sec)
18-06-04 23:53-INFO->> Step 411250 run_train: loss = 5.4274  (0.187 sec)
18-06-04 23:53-INFO->> Step 411260 run_train: loss = 5.4526  (0.169 sec)
18-06-04 23:53-INFO->> Step 411270 run_train: loss = 5.4185  (0.187 sec)
18-06-04 23:53-INFO->> Step 411280 run_train: loss = 5.4031  (0.164 sec)
18-06-04 23:53-INFO->> Step 411290 run_train: loss = 5.4548  (0.188 sec)
18-06-04 23:53-INFO->> Step 411300 run_train: loss = 5.4328  (0.157 sec)
18-06-04 23:53-INFO->> Step 411310 run_train: loss = 5.3609  (0.169 sec)
18-06-04 23:53-INFO->> Step 411320 run_train: loss = 5.4167  (0.154 sec)
18-06-04 23:53-INFO->> Step 411330 run_train: loss = 5.4157  (0.135 sec)
18-06-04 23:53-INFO->> Step 411340 run_train: loss = 5.4352  (0.196 sec)
18-06-04 23:53-INFO->> Step 411350 run_train: loss = 5.5186  (0.162 sec)
18-06-04 23:53-INFO->> Step 411360 run_train: loss = 5.4459  (0.184 sec)
18-06-04 23:53-INFO->> Step 411370 run_train: loss = 5.3602  (0.156 sec)
18-06-04 23:53-INFO->> Step 411380 run_train: loss = 5.3893  (0.158 sec)
18-06-04 23:53-INFO->> Step 411390 run_train: loss = 5.4231  (0.177 sec)
18-06-04 23:53-INFO->> Step 411400 run_train: loss = 5.4684  (0.178 sec)
18-06-04 23:53-INFO->> Step 411410 run_train: loss = 5.4710  (0.187 sec)
18-06-04 23:53-INFO->> Step 411420 run_train: loss = 5.4065  (0.148 sec)
18-06-04 23:53-INFO->> Step 411430 run_train: loss = 5.3604  (0.208 sec)
18-06-04 23:53-INFO->> Step 411440 run_train: loss = 5.4230  (0.158 sec)
18-06-04 23:53-INFO->> Step 411450 run_train: loss = 5.4262  (0.162 sec)
18-06-04 23:53-INFO->> Step 411460 run_train: loss = 5.4039  (0.146 sec)
18-06-04 23:53-INFO->> Step 411470 run_train: loss = 5.4145  (0.186 sec)
18-06-04 23:53-INFO->> Step 411480 run_train: loss = 5.4269  (0.122 sec)
18-06-04 23:53-INFO->> Step 411490 run_train: loss = 5.4380  (0.141 sec)
18-06-04 23:53-INFO->> Step 411500 run_train: loss = 5.3699  (0.176 sec)
18-06-04 23:53-INFO->> Step 411510 run_train: loss = 5.4506  (0.165 sec)
18-06-04 23:53-INFO->> Step 411520 run_train: loss = 5.4273  (0.178 sec)
18-06-04 23:53-INFO->> Step 411530 run_train: loss = 5.4138  (0.177 sec)
18-06-04 23:53-INFO->> Step 411540 run_train: loss = 5.4129  (0.176 sec)
18-06-04 23:53-INFO->> Step 411550 run_train: loss = 5.4717  (0.162 sec)
18-06-04 23:53-INFO->> Step 411560 run_train: loss = 5.4082  (0.165 sec)
18-06-04 23:54-INFO->> Step 411570 run_train: loss = 5.5033  (0.174 sec)
18-06-04 23:54-INFO->> Step 411580 run_train: loss = 5.3713  (0.186 sec)
18-06-04 23:54-INFO->> Step 411590 run_train: loss = 5.4456  (0.160 sec)
18-06-04 23:54-INFO->> Step 411600 run_train: loss = 5.4153  (0.183 sec)
18-06-04 23:54-INFO->> Step 411610 run_train: loss = 5.4362  (0.188 sec)
18-06-04 23:54-INFO->> Step 411620 run_train: loss = 5.4277  (0.160 sec)
18-06-04 23:54-INFO->> Step 411630 run_train: loss = 5.4323  (0.193 sec)
18-06-04 23:54-INFO->> Step 411640 run_train: loss = 5.4261  (0.178 sec)
18-06-04 23:54-INFO->> Step 411650 run_train: loss = 5.4076  (0.153 sec)
18-06-04 23:54-INFO->> Step 411660 run_train: loss = 5.4300  (0.152 sec)
18-06-04 23:54-INFO->> Step 411670 run_train: loss = 5.4132  (0.167 sec)
18-06-04 23:54-INFO->> Step 411680 run_train: loss = 5.4365  (0.139 sec)
18-06-04 23:54-INFO->> Step 411690 run_train: loss = 5.3779  (0.172 sec)
18-06-04 23:54-INFO->> Step 411700 run_train: loss = 5.4562  (0.158 sec)
18-06-04 23:54-INFO->> Step 411710 run_train: loss = 5.4732  (0.149 sec)
18-06-04 23:54-INFO->> Step 411720 run_train: loss = 5.4257  (0.172 sec)
18-06-04 23:54-INFO->> Step 411730 run_train: loss = 5.3952  (0.164 sec)
18-06-04 23:54-INFO->> Step 411740 run_train: loss = 5.4112  (0.166 sec)
18-06-04 23:54-INFO->> Step 411750 run_train: loss = 5.4190  (0.162 sec)
18-06-04 23:54-INFO->> Step 411760 run_train: loss = 5.4492  (0.151 sec)
18-06-04 23:54-INFO->> Step 411770 run_train: loss = 5.4849  (0.180 sec)
18-06-04 23:54-INFO->> Step 411780 run_train: loss = 5.4453  (0.171 sec)
18-06-04 23:54-INFO->> Step 411790 run_train: loss = 5.3958  (0.158 sec)
18-06-04 23:54-INFO->> Step 411800 run_train: loss = 5.4340  (0.178 sec)
18-06-04 23:54-INFO->> Step 411810 run_train: loss = 5.4238  (0.130 sec)
18-06-04 23:54-INFO->> Step 411820 run_train: loss = 5.4100  (0.197 sec)
18-06-04 23:54-INFO->> Step 411830 run_train: loss = 5.3559  (0.163 sec)
18-06-04 23:54-INFO->> Step 411840 run_train: loss = 5.3681  (0.145 sec)
18-06-04 23:54-INFO->> Step 411850 run_train: loss = 5.4243  (0.197 sec)
18-06-04 23:54-INFO->> Step 411860 run_train: loss = 5.3225  (0.161 sec)
18-06-04 23:54-INFO->> Step 411870 run_train: loss = 5.4126  (0.164 sec)
18-06-04 23:54-INFO->> Step 411880 run_train: loss = 5.3796  (0.165 sec)
18-06-04 23:54-INFO->> Step 411890 run_train: loss = 5.4468  (0.154 sec)
18-06-04 23:54-INFO->> Step 411900 run_train: loss = 5.4007  (0.160 sec)
18-06-04 23:54-INFO->> Step 411910 run_train: loss = 5.4034  (0.167 sec)
18-06-04 23:54-INFO->> Step 411920 run_train: loss = 5.4407  (0.145 sec)
18-06-04 23:54-INFO->> Step 411930 run_train: loss = 5.4755  (0.131 sec)
18-06-04 23:54-INFO->> Step 411940 run_train: loss = 5.3935  (0.137 sec)
18-06-04 23:55-INFO->> Step 411950 run_train: loss = 5.3332  (0.152 sec)
18-06-04 23:55-INFO->> Step 411960 run_train: loss = 5.4501  (0.137 sec)
18-06-04 23:55-INFO->> Step 411970 run_train: loss = 5.3741  (0.153 sec)
18-06-04 23:55-INFO->> Step 411980 run_train: loss = 5.4063  (0.195 sec)
18-06-04 23:55-INFO->> Step 411990 run_train: loss = 5.4151  (0.193 sec)
18-06-04 23:55-INFO->> Step 412000 run_train: loss = 5.4130  (0.159 sec)
18-06-04 23:55-INFO->> 2018-06-04 23:55:08.552509 Saving in ckpt
18-06-04 23:55-INFO-Test Data Eval:
18-06-04 23:55-INFO-fpr95 = 0.17438230605738575 and auc = 0.9690084643085227
18-06-04 23:55-INFO->> Step 412010 run_train: loss = 5.3994  (0.176 sec)
18-06-04 23:55-INFO->> Step 412020 run_train: loss = 5.4363  (0.131 sec)
18-06-04 23:55-INFO->> Step 412030 run_train: loss = 5.4616  (0.165 sec)
18-06-04 23:55-INFO->> Step 412040 run_train: loss = 5.4328  (0.149 sec)
18-06-04 23:55-INFO->> Step 412050 run_train: loss = 5.4065  (0.179 sec)
18-06-04 23:55-INFO->> Step 412060 run_train: loss = 5.3628  (0.194 sec)
18-06-04 23:55-INFO->> Step 412070 run_train: loss = 5.4812  (0.121 sec)
18-06-04 23:56-INFO->> Step 412080 run_train: loss = 5.4311  (0.145 sec)
18-06-04 23:56-INFO->> Step 412090 run_train: loss = 5.4810  (0.175 sec)
18-06-04 23:56-INFO->> Step 412100 run_train: loss = 5.4353  (0.159 sec)
18-06-04 23:56-INFO->> Step 412110 run_train: loss = 5.4317  (0.139 sec)
18-06-04 23:56-INFO->> Step 412120 run_train: loss = 5.3798  (0.165 sec)
18-06-04 23:56-INFO->> Step 412130 run_train: loss = 5.4142  (0.181 sec)
18-06-04 23:56-INFO->> Step 412140 run_train: loss = 5.3946  (0.167 sec)
18-06-04 23:56-INFO->> Step 412150 run_train: loss = 5.3799  (0.137 sec)
18-06-04 23:56-INFO->> Step 412160 run_train: loss = 5.4209  (0.135 sec)
18-06-04 23:56-INFO->> Step 412170 run_train: loss = 5.4865  (0.180 sec)
18-06-04 23:56-INFO->> Step 412180 run_train: loss = 5.4450  (0.143 sec)
18-06-04 23:56-INFO->> Step 412190 run_train: loss = 5.3876  (0.171 sec)
18-06-04 23:56-INFO->> Step 412200 run_train: loss = 5.4200  (0.117 sec)
18-06-04 23:56-INFO->> Step 412210 run_train: loss = 5.4014  (0.164 sec)
18-06-04 23:56-INFO->> Step 412220 run_train: loss = 5.4233  (0.150 sec)
18-06-04 23:56-INFO->> Step 412230 run_train: loss = 5.4805  (0.186 sec)
18-06-04 23:56-INFO->> Step 412240 run_train: loss = 5.3411  (0.155 sec)
18-06-04 23:56-INFO->> Step 412250 run_train: loss = 5.4860  (0.173 sec)
18-06-04 23:56-INFO->> Step 412260 run_train: loss = 5.3671  (0.143 sec)
18-06-04 23:56-INFO->> Step 412270 run_train: loss = 5.4177  (0.188 sec)
18-06-04 23:56-INFO->> Step 412280 run_train: loss = 5.4247  (0.178 sec)
18-06-04 23:56-INFO->> Step 412290 run_train: loss = 5.4263  (0.145 sec)
18-06-04 23:56-INFO->> Step 412300 run_train: loss = 5.5049  (0.124 sec)
18-06-04 23:56-INFO->> Step 412310 run_train: loss = 5.4280  (0.141 sec)
18-06-04 23:56-INFO->> Step 412320 run_train: loss = 5.3877  (0.170 sec)
18-06-04 23:56-INFO->> Step 412330 run_train: loss = 5.4992  (0.149 sec)
18-06-04 23:56-INFO->> Step 412340 run_train: loss = 5.4388  (0.145 sec)
18-06-04 23:56-INFO->> Step 412350 run_train: loss = 5.3652  (0.178 sec)
18-06-04 23:56-INFO->> Step 412360 run_train: loss = 5.4414  (0.153 sec)
18-06-04 23:56-INFO->> Step 412370 run_train: loss = 5.3825  (0.144 sec)
18-06-04 23:56-INFO->> Step 412380 run_train: loss = 5.4319  (0.159 sec)
18-06-04 23:56-INFO->> Step 412390 run_train: loss = 5.5145  (0.175 sec)
18-06-04 23:56-INFO->> Step 412400 run_train: loss = 5.4797  (0.158 sec)
18-06-04 23:56-INFO->> Step 412410 run_train: loss = 5.4056  (0.155 sec)
18-06-04 23:56-INFO->> Step 412420 run_train: loss = 5.3660  (0.183 sec)
18-06-04 23:56-INFO->> Step 412430 run_train: loss = 5.4111  (0.158 sec)
18-06-04 23:56-INFO->> Step 412440 run_train: loss = 5.3768  (0.153 sec)
18-06-04 23:57-INFO->> Step 412450 run_train: loss = 5.4240  (0.142 sec)
18-06-04 23:57-INFO->> Step 412460 run_train: loss = 5.3617  (0.188 sec)
18-06-04 23:57-INFO->> Step 412470 run_train: loss = 5.4577  (0.198 sec)
18-06-04 23:57-INFO->> Step 412480 run_train: loss = 5.3748  (0.152 sec)
18-06-04 23:57-INFO->> Step 412490 run_train: loss = 5.3550  (0.152 sec)
18-06-04 23:57-INFO->> Step 412500 run_train: loss = 5.4331  (0.157 sec)
18-06-04 23:57-INFO->> Step 412510 run_train: loss = 5.3861  (0.155 sec)
18-06-04 23:57-INFO->> Step 412520 run_train: loss = 5.4335  (0.124 sec)
18-06-04 23:57-INFO->> Step 412530 run_train: loss = 5.3857  (0.154 sec)
18-06-04 23:57-INFO->> Step 412540 run_train: loss = 5.3655  (0.155 sec)
18-06-04 23:57-INFO->> Step 412550 run_train: loss = 5.4449  (0.167 sec)
18-06-04 23:57-INFO->> Step 412560 run_train: loss = 5.4555  (0.163 sec)
18-06-04 23:57-INFO->> Step 412570 run_train: loss = 5.4114  (0.172 sec)
18-06-04 23:57-INFO->> Step 412580 run_train: loss = 5.4071  (0.148 sec)
18-06-04 23:57-INFO->> Step 412590 run_train: loss = 5.4047  (0.171 sec)
18-06-04 23:57-INFO->> Step 412600 run_train: loss = 5.4095  (0.151 sec)
18-06-04 23:57-INFO->> Step 412610 run_train: loss = 5.4033  (0.160 sec)
18-06-04 23:57-INFO->> Step 412620 run_train: loss = 5.3874  (0.133 sec)
18-06-04 23:57-INFO->> Step 412630 run_train: loss = 5.4536  (0.156 sec)
18-06-04 23:57-INFO->> Step 412640 run_train: loss = 5.4302  (0.160 sec)
18-06-04 23:57-INFO->> Step 412650 run_train: loss = 5.4728  (0.146 sec)
18-06-04 23:57-INFO->> Step 412660 run_train: loss = 5.4460  (0.126 sec)
18-06-04 23:57-INFO->> Step 412670 run_train: loss = 5.4969  (0.163 sec)
18-06-04 23:57-INFO->> Step 412680 run_train: loss = 5.4424  (0.144 sec)
18-06-04 23:57-INFO->> Step 412690 run_train: loss = 5.4004  (0.153 sec)
18-06-04 23:57-INFO->> Step 412700 run_train: loss = 5.4687  (0.176 sec)
18-06-04 23:57-INFO->> Step 412710 run_train: loss = 5.4865  (0.159 sec)
18-06-04 23:57-INFO->> Step 412720 run_train: loss = 5.4352  (0.140 sec)
18-06-04 23:57-INFO->> Step 412730 run_train: loss = 5.4286  (0.155 sec)
18-06-04 23:57-INFO->> Step 412740 run_train: loss = 5.4404  (0.184 sec)
18-06-04 23:57-INFO->> Step 412750 run_train: loss = 5.4129  (0.190 sec)
18-06-04 23:57-INFO->> Step 412760 run_train: loss = 5.4173  (0.146 sec)
18-06-04 23:57-INFO->> Step 412770 run_train: loss = 5.4026  (0.169 sec)
18-06-04 23:57-INFO->> Step 412780 run_train: loss = 5.3957  (0.174 sec)
18-06-04 23:57-INFO->> Step 412790 run_train: loss = 5.4393  (0.147 sec)
18-06-04 23:57-INFO->> Step 412800 run_train: loss = 5.3568  (0.139 sec)
18-06-04 23:57-INFO->> Step 412810 run_train: loss = 5.4429  (0.144 sec)
18-06-04 23:57-INFO->> Step 412820 run_train: loss = 5.3802  (0.138 sec)
18-06-04 23:58-INFO->> Step 412830 run_train: loss = 5.4214  (0.154 sec)
18-06-04 23:58-INFO->> Step 412840 run_train: loss = 5.4584  (0.134 sec)
18-06-04 23:58-INFO->> Step 412850 run_train: loss = 5.3357  (0.136 sec)
18-06-04 23:58-INFO->> Step 412860 run_train: loss = 5.4278  (0.159 sec)
18-06-04 23:58-INFO->> Step 412870 run_train: loss = 5.4264  (0.190 sec)
18-06-04 23:58-INFO->> Step 412880 run_train: loss = 5.4093  (0.151 sec)
18-06-04 23:58-INFO->> Step 412890 run_train: loss = 5.4468  (0.159 sec)
18-06-04 23:58-INFO->> Step 412900 run_train: loss = 5.4154  (0.160 sec)
18-06-04 23:58-INFO->> Step 412910 run_train: loss = 5.4732  (0.151 sec)
18-06-04 23:58-INFO->> Step 412920 run_train: loss = 5.4669  (0.160 sec)
18-06-04 23:58-INFO->> Step 412930 run_train: loss = 5.4458  (0.200 sec)
18-06-04 23:58-INFO->> Step 412940 run_train: loss = 5.3827  (0.138 sec)
18-06-04 23:58-INFO->> Step 412950 run_train: loss = 5.4136  (0.158 sec)
18-06-04 23:58-INFO->> Step 412960 run_train: loss = 5.4121  (0.162 sec)
18-06-04 23:58-INFO->> Step 412970 run_train: loss = 5.3714  (0.178 sec)
18-06-04 23:58-INFO->> Step 412980 run_train: loss = 5.4135  (0.174 sec)
18-06-04 23:58-INFO->> Step 412990 run_train: loss = 5.3839  (0.155 sec)
18-06-04 23:58-INFO->> Step 413000 run_train: loss = 5.4128  (0.203 sec)
18-06-04 23:58-INFO->> 2018-06-04 23:58:27.857326 Saving in ckpt
18-06-04 23:58-INFO-Test Data Eval:
18-06-04 23:59-INFO-fpr95 = 0.17006509032943676 and auc = 0.9693181731243783
18-06-04 23:59-INFO->> Step 413010 run_train: loss = 5.3896  (0.161 sec)
18-06-04 23:59-INFO->> Step 413020 run_train: loss = 5.4339  (0.128 sec)
18-06-04 23:59-INFO->> Step 413030 run_train: loss = 5.4419  (0.122 sec)
18-06-04 23:59-INFO->> Step 413040 run_train: loss = 5.4133  (0.164 sec)
18-06-04 23:59-INFO->> Step 413050 run_train: loss = 5.4045  (0.152 sec)
18-06-04 23:59-INFO->> Step 413060 run_train: loss = 5.3983  (0.188 sec)
18-06-04 23:59-INFO->> Step 413070 run_train: loss = 5.4052  (0.149 sec)
18-06-04 23:59-INFO->> Step 413080 run_train: loss = 5.4101  (0.176 sec)
18-06-04 23:59-INFO->> Step 413090 run_train: loss = 5.4092  (0.156 sec)
18-06-04 23:59-INFO->> Step 413100 run_train: loss = 5.3948  (0.156 sec)
18-06-04 23:59-INFO->> Step 413110 run_train: loss = 5.4447  (0.155 sec)
18-06-04 23:59-INFO->> Step 413120 run_train: loss = 5.4437  (0.154 sec)
18-06-04 23:59-INFO->> Step 413130 run_train: loss = 5.4756  (0.162 sec)
18-06-04 23:59-INFO->> Step 413140 run_train: loss = 5.4203  (0.151 sec)
18-06-04 23:59-INFO->> Step 413150 run_train: loss = 5.3738  (0.209 sec)
18-06-04 23:59-INFO->> Step 413160 run_train: loss = 5.3921  (0.136 sec)
18-06-04 23:59-INFO->> Step 413170 run_train: loss = 5.4409  (0.165 sec)
18-06-04 23:59-INFO->> Step 413180 run_train: loss = 5.3602  (0.154 sec)
18-06-04 23:59-INFO->> Step 413190 run_train: loss = 5.4670  (0.153 sec)
18-06-04 23:59-INFO->> Step 413200 run_train: loss = 5.3876  (0.143 sec)
18-06-04 23:59-INFO->> Step 413210 run_train: loss = 5.4296  (0.191 sec)
18-06-04 23:59-INFO->> Step 413220 run_train: loss = 5.3230  (0.164 sec)
18-06-04 23:59-INFO->> Step 413230 run_train: loss = 5.4214  (0.188 sec)
18-06-04 23:59-INFO->> Step 413240 run_train: loss = 5.4915  (0.155 sec)
18-06-04 23:59-INFO->> Step 413250 run_train: loss = 5.3911  (0.149 sec)
18-06-04 23:59-INFO->> Step 413260 run_train: loss = 5.4474  (0.143 sec)
18-06-04 23:59-INFO->> Step 413270 run_train: loss = 5.4561  (0.163 sec)
18-06-04 23:59-INFO->> Step 413280 run_train: loss = 5.3852  (0.163 sec)
18-06-04 23:59-INFO->> Step 413290 run_train: loss = 5.4469  (0.171 sec)
18-06-04 23:59-INFO->> Step 413300 run_train: loss = 5.3604  (0.174 sec)
18-06-04 23:59-INFO->> Step 413310 run_train: loss = 5.4894  (0.183 sec)
18-06-04 23:59-INFO->> Step 413320 run_train: loss = 5.3999  (0.153 sec)
18-06-05 00:00-INFO->> Step 413330 run_train: loss = 5.4060  (0.156 sec)
18-06-05 00:00-INFO->> Step 413340 run_train: loss = 5.4568  (0.144 sec)
18-06-05 00:00-INFO->> Step 413350 run_train: loss = 5.3826  (0.119 sec)
18-06-05 00:00-INFO->> Step 413360 run_train: loss = 5.5025  (0.141 sec)
18-06-05 00:00-INFO->> Step 413370 run_train: loss = 5.3841  (0.160 sec)
18-06-05 00:00-INFO->> Step 413380 run_train: loss = 5.4087  (0.160 sec)
18-06-05 00:00-INFO->> Step 413390 run_train: loss = 5.3901  (0.150 sec)
18-06-05 00:00-INFO->> Step 413400 run_train: loss = 5.4233  (0.143 sec)
18-06-05 00:00-INFO->> Step 413410 run_train: loss = 5.3928  (0.146 sec)
18-06-05 00:00-INFO->> Step 413420 run_train: loss = 5.3797  (0.162 sec)
18-06-05 00:00-INFO->> Step 413430 run_train: loss = 5.3510  (0.155 sec)
18-06-05 00:00-INFO->> Step 413440 run_train: loss = 5.4008  (0.126 sec)
18-06-05 00:00-INFO->> Step 413450 run_train: loss = 5.4317  (0.156 sec)
18-06-05 00:00-INFO->> Step 413460 run_train: loss = 5.5139  (0.251 sec)
18-06-05 00:00-INFO->> Step 413470 run_train: loss = 5.4894  (0.108 sec)
18-06-05 00:00-INFO->> Step 413480 run_train: loss = 5.4907  (0.148 sec)
18-06-05 00:00-INFO->> Step 413490 run_train: loss = 5.4065  (0.161 sec)
18-06-05 00:00-INFO->> Step 413500 run_train: loss = 5.4204  (0.159 sec)
18-06-05 00:00-INFO->> Step 413510 run_train: loss = 5.4552  (0.184 sec)
18-06-05 00:00-INFO->> Step 413520 run_train: loss = 5.4367  (0.179 sec)
18-06-05 00:00-INFO->> Step 413530 run_train: loss = 5.4694  (0.151 sec)
18-06-05 00:00-INFO->> Step 413540 run_train: loss = 5.3082  (0.143 sec)
18-06-05 00:00-INFO->> Step 413550 run_train: loss = 5.3831  (0.168 sec)
18-06-05 00:00-INFO->> Step 413560 run_train: loss = 5.4234  (0.153 sec)
18-06-05 00:00-INFO->> Step 413570 run_train: loss = 5.4053  (0.175 sec)
18-06-05 00:00-INFO->> Step 413580 run_train: loss = 5.3739  (0.167 sec)
18-06-05 00:00-INFO->> Step 413590 run_train: loss = 5.4743  (0.186 sec)
18-06-05 00:00-INFO->> Step 413600 run_train: loss = 5.4073  (0.150 sec)
18-06-05 00:00-INFO->> Step 413610 run_train: loss = 5.4134  (0.136 sec)
18-06-05 00:00-INFO->> Step 413620 run_train: loss = 5.4638  (0.187 sec)
18-06-05 00:00-INFO->> Step 413630 run_train: loss = 5.3537  (0.154 sec)
18-06-05 00:00-INFO->> Step 413640 run_train: loss = 5.3806  (0.147 sec)
18-06-05 00:00-INFO->> Step 413650 run_train: loss = 5.4330  (0.149 sec)
18-06-05 00:00-INFO->> Step 413660 run_train: loss = 5.4260  (0.145 sec)
18-06-05 00:00-INFO->> Step 413670 run_train: loss = 5.4049  (0.126 sec)
18-06-05 00:00-INFO->> Step 413680 run_train: loss = 5.3933  (0.146 sec)
18-06-05 00:00-INFO->> Step 413690 run_train: loss = 5.4194  (0.123 sec)
18-06-05 00:00-INFO->> Step 413700 run_train: loss = 5.4229  (0.163 sec)
18-06-05 00:01-INFO->> Step 413710 run_train: loss = 5.4334  (0.148 sec)
18-06-05 00:01-INFO->> Step 413720 run_train: loss = 5.4222  (0.175 sec)
18-06-05 00:01-INFO->> Step 413730 run_train: loss = 5.4488  (0.160 sec)
18-06-05 00:01-INFO->> Step 413740 run_train: loss = 5.4418  (0.174 sec)
18-06-05 00:01-INFO->> Step 413750 run_train: loss = 5.3656  (0.136 sec)
18-06-05 00:01-INFO->> Step 413760 run_train: loss = 5.4251  (0.185 sec)
18-06-05 00:01-INFO->> Step 413770 run_train: loss = 5.4206  (0.154 sec)
18-06-05 00:01-INFO->> Step 413780 run_train: loss = 5.4297  (0.169 sec)
18-06-05 00:01-INFO->> Step 413790 run_train: loss = 5.3400  (0.164 sec)
18-06-05 00:01-INFO->> Step 413800 run_train: loss = 5.4565  (0.175 sec)
18-06-05 00:01-INFO->> Step 413810 run_train: loss = 5.3665  (0.157 sec)
18-06-05 00:01-INFO->> Step 413820 run_train: loss = 5.4242  (0.112 sec)
18-06-05 00:01-INFO->> Step 413830 run_train: loss = 5.4051  (0.170 sec)
18-06-05 00:01-INFO->> Step 413840 run_train: loss = 5.4096  (0.189 sec)
18-06-05 00:01-INFO->> Step 413850 run_train: loss = 5.4968  (0.153 sec)
18-06-05 00:01-INFO->> Step 413860 run_train: loss = 5.3821  (0.187 sec)
18-06-05 00:01-INFO->> Step 413870 run_train: loss = 5.3637  (0.133 sec)
18-06-05 00:01-INFO->> Step 413880 run_train: loss = 5.4024  (0.154 sec)
18-06-05 00:01-INFO->> Step 413890 run_train: loss = 5.3700  (0.161 sec)
18-06-05 00:01-INFO->> Step 413900 run_train: loss = 5.4547  (0.164 sec)
18-06-05 00:01-INFO->> Step 413910 run_train: loss = 5.3291  (0.134 sec)
18-06-05 00:01-INFO->> Step 413920 run_train: loss = 5.4730  (0.165 sec)
18-06-05 00:01-INFO->> Step 413930 run_train: loss = 5.4457  (0.156 sec)
18-06-05 00:01-INFO->> Step 413940 run_train: loss = 5.3725  (0.155 sec)
18-06-05 00:01-INFO->> Step 413950 run_train: loss = 5.3809  (0.153 sec)
18-06-05 00:01-INFO->> Step 413960 run_train: loss = 5.4393  (0.122 sec)
18-06-05 00:01-INFO->> Step 413970 run_train: loss = 5.4147  (0.177 sec)
18-06-05 00:01-INFO->> Step 413980 run_train: loss = 5.3662  (0.143 sec)
18-06-05 00:01-INFO->> Step 413990 run_train: loss = 5.4524  (0.175 sec)
18-06-05 00:01-INFO->> Step 414000 run_train: loss = 5.4460  (0.182 sec)
18-06-05 00:01-INFO->> 2018-06-05 00:01:47.124897 Saving in ckpt
18-06-05 00:01-INFO-Test Data Eval:
18-06-05 00:02-INFO-fpr95 = 0.17254748937300743 and auc = 0.9692776830286571
18-06-05 00:02-INFO->> Step 414010 run_train: loss = 5.4015  (0.160 sec)
18-06-05 00:02-INFO->> Step 414020 run_train: loss = 5.3871  (0.142 sec)
18-06-05 00:02-INFO->> Step 414030 run_train: loss = 5.3868  (0.152 sec)
18-06-05 00:02-INFO->> Step 414040 run_train: loss = 5.3491  (0.141 sec)
18-06-05 00:02-INFO->> Step 414050 run_train: loss = 5.4163  (0.143 sec)
18-06-05 00:02-INFO->> Step 414060 run_train: loss = 5.4218  (0.154 sec)
18-06-05 00:02-INFO->> Step 414070 run_train: loss = 5.3782  (0.174 sec)
18-06-05 00:02-INFO->> Step 414080 run_train: loss = 5.4077  (0.137 sec)
18-06-05 00:02-INFO->> Step 414090 run_train: loss = 5.4424  (0.129 sec)
18-06-05 00:02-INFO->> Step 414100 run_train: loss = 5.4426  (0.129 sec)
18-06-05 00:02-INFO->> Step 414110 run_train: loss = 5.4069  (0.159 sec)
18-06-05 00:02-INFO->> Step 414120 run_train: loss = 5.4147  (0.166 sec)
18-06-05 00:02-INFO->> Step 414130 run_train: loss = 5.4512  (0.187 sec)
18-06-05 00:02-INFO->> Step 414140 run_train: loss = 5.3797  (0.152 sec)
18-06-05 00:02-INFO->> Step 414150 run_train: loss = 5.4392  (0.157 sec)
18-06-05 00:02-INFO->> Step 414160 run_train: loss = 5.4011  (0.187 sec)
18-06-05 00:02-INFO->> Step 414170 run_train: loss = 5.4252  (0.171 sec)
18-06-05 00:02-INFO->> Step 414180 run_train: loss = 5.4395  (0.149 sec)
18-06-05 00:02-INFO->> Step 414190 run_train: loss = 5.3914  (0.158 sec)
18-06-05 00:02-INFO->> Step 414200 run_train: loss = 5.4417  (0.152 sec)
18-06-05 00:03-INFO->> Step 414210 run_train: loss = 5.4412  (0.164 sec)
18-06-05 00:03-INFO->> Step 414220 run_train: loss = 5.4463  (0.123 sec)
18-06-05 00:03-INFO->> Step 414230 run_train: loss = 5.4334  (0.139 sec)
18-06-05 00:03-INFO->> Step 414240 run_train: loss = 5.4194  (0.136 sec)
18-06-05 00:03-INFO->> Step 414250 run_train: loss = 5.4663  (0.151 sec)
18-06-05 00:03-INFO->> Step 414260 run_train: loss = 5.4435  (0.162 sec)
18-06-05 00:03-INFO->> Step 414270 run_train: loss = 5.5132  (0.176 sec)
18-06-05 00:03-INFO->> Step 414280 run_train: loss = 5.4310  (0.179 sec)
18-06-05 00:03-INFO->> Step 414290 run_train: loss = 5.3479  (0.156 sec)
18-06-05 00:03-INFO->> Step 414300 run_train: loss = 5.4086  (0.181 sec)
18-06-05 00:03-INFO->> Step 414310 run_train: loss = 5.4665  (0.122 sec)
18-06-05 00:03-INFO->> Step 414320 run_train: loss = 5.4579  (0.176 sec)
18-06-05 00:03-INFO->> Step 414330 run_train: loss = 5.4346  (0.136 sec)
18-06-05 00:03-INFO->> Step 414340 run_train: loss = 5.3555  (0.146 sec)
18-06-05 00:03-INFO->> Step 414350 run_train: loss = 5.5090  (0.134 sec)
18-06-05 00:03-INFO->> Step 414360 run_train: loss = 5.3675  (0.118 sec)
18-06-05 00:03-INFO->> Step 414370 run_train: loss = 5.3626  (0.162 sec)
18-06-05 00:03-INFO->> Step 414380 run_train: loss = 5.4414  (0.162 sec)
18-06-05 00:03-INFO->> Step 414390 run_train: loss = 5.4326  (0.194 sec)
18-06-05 00:03-INFO->> Step 414400 run_train: loss = 5.5046  (0.173 sec)
18-06-05 00:03-INFO->> Step 414410 run_train: loss = 5.4302  (0.130 sec)
18-06-05 00:03-INFO->> Step 414420 run_train: loss = 5.4314  (0.169 sec)
18-06-05 00:03-INFO->> Step 414430 run_train: loss = 5.3591  (0.172 sec)
18-06-05 00:03-INFO->> Step 414440 run_train: loss = 5.4138  (0.150 sec)
18-06-05 00:03-INFO->> Step 414450 run_train: loss = 5.4715  (0.164 sec)
18-06-05 00:03-INFO->> Step 414460 run_train: loss = 5.4040  (0.195 sec)
18-06-05 00:03-INFO->> Step 414470 run_train: loss = 5.4132  (0.167 sec)
18-06-05 00:03-INFO->> Step 414480 run_train: loss = 5.3511  (0.165 sec)
18-06-05 00:03-INFO->> Step 414490 run_train: loss = 5.3476  (0.153 sec)
18-06-05 00:03-INFO->> Step 414500 run_train: loss = 5.5011  (0.144 sec)
18-06-05 00:03-INFO->> Step 414510 run_train: loss = 5.3811  (0.159 sec)
18-06-05 00:03-INFO->> Step 414520 run_train: loss = 5.3701  (0.153 sec)
18-06-05 00:03-INFO->> Step 414530 run_train: loss = 5.3916  (0.150 sec)
18-06-05 00:03-INFO->> Step 414540 run_train: loss = 5.4281  (0.145 sec)
18-06-05 00:03-INFO->> Step 414550 run_train: loss = 5.4625  (0.213 sec)
18-06-05 00:03-INFO->> Step 414560 run_train: loss = 5.3928  (0.146 sec)
18-06-05 00:03-INFO->> Step 414570 run_train: loss = 5.3649  (0.158 sec)
18-06-05 00:03-INFO->> Step 414580 run_train: loss = 5.3774  (0.161 sec)
18-06-05 00:04-INFO->> Step 414590 run_train: loss = 5.4709  (0.171 sec)
18-06-05 00:04-INFO->> Step 414600 run_train: loss = 5.4423  (0.167 sec)
18-06-05 00:04-INFO->> Step 414610 run_train: loss = 5.5131  (0.200 sec)
18-06-05 00:04-INFO->> Step 414620 run_train: loss = 5.4000  (0.170 sec)
18-06-05 00:04-INFO->> Step 414630 run_train: loss = 5.4368  (0.146 sec)
18-06-05 00:04-INFO->> Step 414640 run_train: loss = 5.3803  (0.140 sec)
18-06-05 00:04-INFO->> Step 414650 run_train: loss = 5.4730  (0.154 sec)
18-06-05 00:04-INFO->> Step 414660 run_train: loss = 5.4063  (0.156 sec)
18-06-05 00:04-INFO->> Step 414670 run_train: loss = 5.4510  (0.151 sec)
18-06-05 00:04-INFO->> Step 414680 run_train: loss = 5.3778  (0.159 sec)
18-06-05 00:04-INFO->> Step 414690 run_train: loss = 5.4459  (0.159 sec)
18-06-05 00:04-INFO->> Step 414700 run_train: loss = 5.4222  (0.191 sec)
18-06-05 00:04-INFO->> Step 414710 run_train: loss = 5.3848  (0.154 sec)
18-06-05 00:04-INFO->> Step 414720 run_train: loss = 5.3805  (0.162 sec)
18-06-05 00:04-INFO->> Step 414730 run_train: loss = 5.4870  (0.128 sec)
18-06-05 00:04-INFO->> Step 414740 run_train: loss = 5.4760  (0.176 sec)
18-06-05 00:04-INFO->> Step 414750 run_train: loss = 5.4718  (0.181 sec)
18-06-05 00:04-INFO->> Step 414760 run_train: loss = 5.4227  (0.130 sec)
18-06-05 00:04-INFO->> Step 414770 run_train: loss = 5.3702  (0.170 sec)
18-06-05 00:04-INFO->> Step 414780 run_train: loss = 5.3686  (0.154 sec)
18-06-05 00:04-INFO->> Step 414790 run_train: loss = 5.4186  (0.166 sec)
18-06-05 00:04-INFO->> Step 414800 run_train: loss = 5.4376  (0.158 sec)
18-06-05 00:04-INFO->> Step 414810 run_train: loss = 5.4041  (0.133 sec)
18-06-05 00:04-INFO->> Step 414820 run_train: loss = 5.4281  (0.152 sec)
18-06-05 00:04-INFO->> Step 414830 run_train: loss = 5.3846  (0.147 sec)
18-06-05 00:04-INFO->> Step 414840 run_train: loss = 5.4368  (0.163 sec)
18-06-05 00:04-INFO->> Step 414850 run_train: loss = 5.4330  (0.167 sec)
18-06-05 00:04-INFO->> Step 414860 run_train: loss = 5.5170  (0.163 sec)
18-06-05 00:04-INFO->> Step 414870 run_train: loss = 5.3716  (0.142 sec)
18-06-05 00:04-INFO->> Step 414880 run_train: loss = 5.4548  (0.140 sec)
18-06-05 00:04-INFO->> Step 414890 run_train: loss = 5.4492  (0.164 sec)
18-06-05 00:04-INFO->> Step 414900 run_train: loss = 5.4501  (0.149 sec)
18-06-05 00:04-INFO->> Step 414910 run_train: loss = 5.3921  (0.153 sec)
18-06-05 00:04-INFO->> Step 414920 run_train: loss = 5.4061  (0.146 sec)
18-06-05 00:04-INFO->> Step 414930 run_train: loss = 5.4384  (0.182 sec)
18-06-05 00:04-INFO->> Step 414940 run_train: loss = 5.4099  (0.143 sec)
18-06-05 00:04-INFO->> Step 414950 run_train: loss = 5.3997  (0.182 sec)
18-06-05 00:05-INFO->> Step 414960 run_train: loss = 5.4604  (0.102 sec)
18-06-05 00:05-INFO->> Step 414970 run_train: loss = 5.4535  (0.156 sec)
18-06-05 00:05-INFO->> Step 414980 run_train: loss = 5.4143  (0.205 sec)
18-06-05 00:05-INFO->> Step 414990 run_train: loss = 5.3862  (0.168 sec)
18-06-05 00:05-INFO->> Step 415000 run_train: loss = 5.4203  (0.198 sec)
18-06-05 00:05-INFO->> 2018-06-05 00:05:06.877549 Saving in ckpt
18-06-05 00:05-INFO-Test Data Eval:
18-06-05 00:05-INFO-fpr95 = 0.17380114240170033 and auc = 0.9690897364824262
18-06-05 00:05-INFO->> Step 415010 run_train: loss = 5.4429  (0.192 sec)
18-06-05 00:05-INFO->> Step 415020 run_train: loss = 5.3877  (0.154 sec)
18-06-05 00:05-INFO->> Step 415030 run_train: loss = 5.4314  (0.157 sec)
18-06-05 00:05-INFO->> Step 415040 run_train: loss = 5.3846  (0.168 sec)
18-06-05 00:05-INFO->> Step 415050 run_train: loss = 5.4967  (0.159 sec)
18-06-05 00:05-INFO->> Step 415060 run_train: loss = 5.4244  (0.182 sec)
18-06-05 00:05-INFO->> Step 415070 run_train: loss = 5.4546  (0.151 sec)
18-06-05 00:05-INFO->> Step 415080 run_train: loss = 5.3962  (0.141 sec)
18-06-05 00:06-INFO->> Step 415090 run_train: loss = 5.3832  (0.159 sec)
18-06-05 00:06-INFO->> Step 415100 run_train: loss = 5.4523  (0.138 sec)
18-06-05 00:06-INFO->> Step 415110 run_train: loss = 5.4437  (0.194 sec)
18-06-05 00:06-INFO->> Step 415120 run_train: loss = 5.4463  (0.169 sec)
18-06-05 00:06-INFO->> Step 415130 run_train: loss = 5.3725  (0.153 sec)
18-06-05 00:06-INFO->> Step 415140 run_train: loss = 5.3886  (0.166 sec)
18-06-05 00:06-INFO->> Step 415150 run_train: loss = 5.4448  (0.156 sec)
18-06-05 00:06-INFO->> Step 415160 run_train: loss = 5.4201  (0.160 sec)
18-06-05 00:06-INFO->> Step 415170 run_train: loss = 5.3779  (0.151 sec)
18-06-05 00:06-INFO->> Step 415180 run_train: loss = 5.4128  (0.152 sec)
18-06-05 00:06-INFO->> Step 415190 run_train: loss = 5.4379  (0.207 sec)
18-06-05 00:06-INFO->> Step 415200 run_train: loss = 5.3957  (0.143 sec)
18-06-05 00:06-INFO->> Step 415210 run_train: loss = 5.3700  (0.144 sec)
18-06-05 00:06-INFO->> Step 415220 run_train: loss = 5.3968  (0.148 sec)
18-06-05 00:06-INFO->> Step 415230 run_train: loss = 5.4964  (0.155 sec)
18-06-05 00:06-INFO->> Step 415240 run_train: loss = 5.4673  (0.173 sec)
18-06-05 00:06-INFO->> Step 415250 run_train: loss = 5.4362  (0.171 sec)
18-06-05 00:06-INFO->> Step 415260 run_train: loss = 5.4199  (0.167 sec)
18-06-05 00:06-INFO->> Step 415270 run_train: loss = 5.4055  (0.189 sec)
18-06-05 00:06-INFO->> Step 415280 run_train: loss = 5.4709  (0.156 sec)
18-06-05 00:06-INFO->> Step 415290 run_train: loss = 5.4523  (0.166 sec)
18-06-05 00:06-INFO->> Step 415300 run_train: loss = 5.3762  (0.141 sec)
18-06-05 00:06-INFO->> Step 415310 run_train: loss = 5.3867  (0.178 sec)
18-06-05 00:06-INFO->> Step 415320 run_train: loss = 5.4334  (0.141 sec)
18-06-05 00:06-INFO->> Step 415330 run_train: loss = 5.3777  (0.137 sec)
18-06-05 00:06-INFO->> Step 415340 run_train: loss = 5.4103  (0.164 sec)
18-06-05 00:06-INFO->> Step 415350 run_train: loss = 5.3943  (0.161 sec)
18-06-05 00:06-INFO->> Step 415360 run_train: loss = 5.4047  (0.163 sec)
18-06-05 00:06-INFO->> Step 415370 run_train: loss = 5.4602  (0.130 sec)
18-06-05 00:06-INFO->> Step 415380 run_train: loss = 5.4540  (0.169 sec)
18-06-05 00:06-INFO->> Step 415390 run_train: loss = 5.4473  (0.142 sec)
18-06-05 00:06-INFO->> Step 415400 run_train: loss = 5.3505  (0.133 sec)
18-06-05 00:06-INFO->> Step 415410 run_train: loss = 5.4113  (0.177 sec)
18-06-05 00:06-INFO->> Step 415420 run_train: loss = 5.4368  (0.171 sec)
18-06-05 00:06-INFO->> Step 415430 run_train: loss = 5.4447  (0.151 sec)
18-06-05 00:06-INFO->> Step 415440 run_train: loss = 5.4339  (0.174 sec)
18-06-05 00:06-INFO->> Step 415450 run_train: loss = 5.3895  (0.186 sec)
18-06-05 00:06-INFO->> Step 415460 run_train: loss = 5.4810  (0.144 sec)
18-06-05 00:07-INFO->> Step 415470 run_train: loss = 5.3769  (0.175 sec)
18-06-05 00:07-INFO->> Step 415480 run_train: loss = 5.4017  (0.170 sec)
18-06-05 00:07-INFO->> Step 415490 run_train: loss = 5.4333  (0.130 sec)
18-06-05 00:07-INFO->> Step 415500 run_train: loss = 5.3854  (0.142 sec)
18-06-05 00:07-INFO->> Step 415510 run_train: loss = 5.4322  (0.147 sec)
18-06-05 00:07-INFO->> Step 415520 run_train: loss = 5.3656  (0.176 sec)
18-06-05 00:07-INFO->> Step 415530 run_train: loss = 5.4353  (0.187 sec)
18-06-05 00:07-INFO->> Step 415540 run_train: loss = 5.4469  (0.142 sec)
18-06-05 00:07-INFO->> Step 415550 run_train: loss = 5.4898  (0.140 sec)
18-06-05 00:07-INFO->> Step 415560 run_train: loss = 5.4570  (0.161 sec)
18-06-05 00:07-INFO->> Step 415570 run_train: loss = 5.3838  (0.151 sec)
18-06-05 00:07-INFO->> Step 415580 run_train: loss = 5.4915  (0.174 sec)
18-06-05 00:07-INFO->> Step 415590 run_train: loss = 5.4657  (0.173 sec)
18-06-05 00:07-INFO->> Step 415600 run_train: loss = 5.3659  (0.177 sec)
18-06-05 00:07-INFO->> Step 415610 run_train: loss = 5.4828  (0.148 sec)
18-06-05 00:07-INFO->> Step 415620 run_train: loss = 5.4376  (0.150 sec)
18-06-05 00:07-INFO->> Step 415630 run_train: loss = 5.4305  (0.153 sec)
18-06-05 00:07-INFO->> Step 415640 run_train: loss = 5.4151  (0.166 sec)
18-06-05 00:07-INFO->> Step 415650 run_train: loss = 5.4408  (0.134 sec)
18-06-05 00:07-INFO->> Step 415660 run_train: loss = 5.4373  (0.164 sec)
18-06-05 00:07-INFO->> Step 415670 run_train: loss = 5.4435  (0.190 sec)
18-06-05 00:07-INFO->> Step 415680 run_train: loss = 5.3625  (0.159 sec)
18-06-05 00:07-INFO->> Step 415690 run_train: loss = 5.4109  (0.214 sec)
18-06-05 00:07-INFO->> Step 415700 run_train: loss = 5.4312  (0.154 sec)
18-06-05 00:07-INFO->> Step 415710 run_train: loss = 5.4120  (0.178 sec)
18-06-05 00:07-INFO->> Step 415720 run_train: loss = 5.4741  (0.193 sec)
18-06-05 00:07-INFO->> Step 415730 run_train: loss = 5.3982  (0.143 sec)
18-06-05 00:07-INFO->> Step 415740 run_train: loss = 5.4402  (0.175 sec)
18-06-05 00:07-INFO->> Step 415750 run_train: loss = 5.4042  (0.157 sec)
18-06-05 00:07-INFO->> Step 415760 run_train: loss = 5.3840  (0.151 sec)
18-06-05 00:07-INFO->> Step 415770 run_train: loss = 5.4209  (0.164 sec)
18-06-05 00:07-INFO->> Step 415780 run_train: loss = 5.4086  (0.143 sec)
18-06-05 00:07-INFO->> Step 415790 run_train: loss = 5.4161  (0.159 sec)
18-06-05 00:07-INFO->> Step 415800 run_train: loss = 5.3944  (0.155 sec)
18-06-05 00:07-INFO->> Step 415810 run_train: loss = 5.3980  (0.184 sec)
18-06-05 00:07-INFO->> Step 415820 run_train: loss = 5.4131  (0.167 sec)
18-06-05 00:07-INFO->> Step 415830 run_train: loss = 5.3960  (0.124 sec)
18-06-05 00:08-INFO->> Step 415840 run_train: loss = 5.4336  (0.159 sec)
18-06-05 00:08-INFO->> Step 415850 run_train: loss = 5.5002  (0.148 sec)
18-06-05 00:08-INFO->> Step 415860 run_train: loss = 5.3949  (0.156 sec)
18-06-05 00:08-INFO->> Step 415870 run_train: loss = 5.3882  (0.141 sec)
18-06-05 00:08-INFO->> Step 415880 run_train: loss = 5.4332  (0.134 sec)
18-06-05 00:08-INFO->> Step 415890 run_train: loss = 5.4572  (0.164 sec)
18-06-05 00:08-INFO->> Step 415900 run_train: loss = 5.4194  (0.153 sec)
18-06-05 00:08-INFO->> Step 415910 run_train: loss = 5.4797  (0.162 sec)
18-06-05 00:08-INFO->> Step 415920 run_train: loss = 5.4439  (0.145 sec)
18-06-05 00:08-INFO->> Step 415930 run_train: loss = 5.4430  (0.183 sec)
18-06-05 00:08-INFO->> Step 415940 run_train: loss = 5.4299  (0.154 sec)
18-06-05 00:08-INFO->> Step 415950 run_train: loss = 5.4122  (0.129 sec)
18-06-05 00:08-INFO->> Step 415960 run_train: loss = 5.3828  (0.189 sec)
18-06-05 00:08-INFO->> Step 415970 run_train: loss = 5.4036  (0.172 sec)
18-06-05 00:08-INFO->> Step 415980 run_train: loss = 5.4110  (0.128 sec)
18-06-05 00:08-INFO->> Step 415990 run_train: loss = 5.4540  (0.160 sec)
18-06-05 00:08-INFO->> Step 416000 run_train: loss = 5.4366  (0.177 sec)
18-06-05 00:08-INFO->> 2018-06-05 00:08:25.873111 Saving in ckpt
18-06-05 00:08-INFO-Test Data Eval:
18-06-05 00:09-INFO-fpr95 = 0.1719414187035069 and auc = 0.9693065142977817
18-06-05 00:09-INFO->> Step 416010 run_train: loss = 5.4447  (0.152 sec)
18-06-05 00:09-INFO->> Step 416020 run_train: loss = 5.3724  (0.186 sec)
18-06-05 00:09-INFO->> Step 416030 run_train: loss = 5.4036  (0.152 sec)
18-06-05 00:09-INFO->> Step 416040 run_train: loss = 5.4798  (0.162 sec)
18-06-05 00:09-INFO->> Step 416050 run_train: loss = 5.3945  (0.120 sec)
18-06-05 00:09-INFO->> Step 416060 run_train: loss = 5.4296  (0.191 sec)
18-06-05 00:09-INFO->> Step 416070 run_train: loss = 5.4305  (0.180 sec)
18-06-05 00:09-INFO->> Step 416080 run_train: loss = 5.3924  (0.148 sec)
18-06-05 00:09-INFO->> Step 416090 run_train: loss = 5.3705  (0.165 sec)
18-06-05 00:09-INFO->> Step 416100 run_train: loss = 5.4649  (0.155 sec)
18-06-05 00:09-INFO->> Step 416110 run_train: loss = 5.3689  (0.151 sec)
18-06-05 00:09-INFO->> Step 416120 run_train: loss = 5.3753  (0.139 sec)
18-06-05 00:09-INFO->> Step 416130 run_train: loss = 5.4355  (0.167 sec)
18-06-05 00:09-INFO->> Step 416140 run_train: loss = 5.4401  (0.199 sec)
18-06-05 00:09-INFO->> Step 416150 run_train: loss = 5.4099  (0.161 sec)
18-06-05 00:09-INFO->> Step 416160 run_train: loss = 5.4652  (0.164 sec)
18-06-05 00:09-INFO->> Step 416170 run_train: loss = 5.4661  (0.162 sec)
18-06-05 00:09-INFO->> Step 416180 run_train: loss = 5.4499  (0.189 sec)
18-06-05 00:09-INFO->> Step 416190 run_train: loss = 5.3763  (0.126 sec)
18-06-05 00:09-INFO->> Step 416200 run_train: loss = 5.4274  (0.141 sec)
18-06-05 00:09-INFO->> Step 416210 run_train: loss = 5.4130  (0.179 sec)
18-06-05 00:09-INFO->> Step 416220 run_train: loss = 5.3837  (0.158 sec)
18-06-05 00:09-INFO->> Step 416230 run_train: loss = 5.4649  (0.197 sec)
18-06-05 00:09-INFO->> Step 416240 run_train: loss = 5.5207  (0.144 sec)
18-06-05 00:09-INFO->> Step 416250 run_train: loss = 5.4798  (0.183 sec)
18-06-05 00:09-INFO->> Step 416260 run_train: loss = 5.4797  (0.135 sec)
18-06-05 00:09-INFO->> Step 416270 run_train: loss = 5.4136  (0.185 sec)
18-06-05 00:09-INFO->> Step 416280 run_train: loss = 5.4387  (0.160 sec)
18-06-05 00:09-INFO->> Step 416290 run_train: loss = 5.4358  (0.160 sec)
18-06-05 00:09-INFO->> Step 416300 run_train: loss = 5.3769  (0.165 sec)
18-06-05 00:09-INFO->> Step 416310 run_train: loss = 5.4262  (0.163 sec)
18-06-05 00:09-INFO->> Step 416320 run_train: loss = 5.4241  (0.172 sec)
18-06-05 00:09-INFO->> Step 416330 run_train: loss = 5.4193  (0.160 sec)
18-06-05 00:09-INFO->> Step 416340 run_train: loss = 5.3513  (0.150 sec)
18-06-05 00:10-INFO->> Step 416350 run_train: loss = 5.3964  (0.146 sec)
18-06-05 00:10-INFO->> Step 416360 run_train: loss = 5.4818  (0.153 sec)
18-06-05 00:10-INFO->> Step 416370 run_train: loss = 5.4318  (0.150 sec)
18-06-05 00:10-INFO->> Step 416380 run_train: loss = 5.4000  (0.160 sec)
18-06-05 00:10-INFO->> Step 416390 run_train: loss = 5.3996  (0.144 sec)
18-06-05 00:10-INFO->> Step 416400 run_train: loss = 5.4067  (0.175 sec)
18-06-05 00:10-INFO->> Step 416410 run_train: loss = 5.4655  (0.145 sec)
18-06-05 00:10-INFO->> Step 416420 run_train: loss = 5.3886  (0.139 sec)
18-06-05 00:10-INFO->> Step 416430 run_train: loss = 5.4476  (0.154 sec)
18-06-05 00:10-INFO->> Step 416440 run_train: loss = 5.3192  (0.162 sec)
18-06-05 00:10-INFO->> Step 416450 run_train: loss = 5.4053  (0.166 sec)
18-06-05 00:10-INFO->> Step 416460 run_train: loss = 5.4364  (0.145 sec)
18-06-05 00:10-INFO->> Step 416470 run_train: loss = 5.4255  (0.179 sec)
18-06-05 00:10-INFO->> Step 416480 run_train: loss = 5.4115  (0.117 sec)
18-06-05 00:10-INFO->> Step 416490 run_train: loss = 5.3800  (0.166 sec)
18-06-05 00:10-INFO->> Step 416500 run_train: loss = 5.4656  (0.141 sec)
18-06-05 00:10-INFO->> Step 416510 run_train: loss = 5.4218  (0.155 sec)
18-06-05 00:10-INFO->> Step 416520 run_train: loss = 5.3661  (0.154 sec)
18-06-05 00:10-INFO->> Step 416530 run_train: loss = 5.4109  (0.142 sec)
18-06-05 00:10-INFO->> Step 416540 run_train: loss = 5.4155  (0.148 sec)
18-06-05 00:10-INFO->> Step 416550 run_train: loss = 5.4107  (0.151 sec)
18-06-05 00:10-INFO->> Step 416560 run_train: loss = 5.4012  (0.145 sec)
18-06-05 00:10-INFO->> Step 416570 run_train: loss = 5.3963  (0.173 sec)
18-06-05 00:10-INFO->> Step 416580 run_train: loss = 5.4417  (0.164 sec)
18-06-05 00:10-INFO->> Step 416590 run_train: loss = 5.4591  (0.172 sec)
18-06-05 00:10-INFO->> Step 416600 run_train: loss = 5.4655  (0.149 sec)
18-06-05 00:10-INFO->> Step 416610 run_train: loss = 5.4091  (0.141 sec)
18-06-05 00:10-INFO->> Step 416620 run_train: loss = 5.4420  (0.146 sec)
18-06-05 00:10-INFO->> Step 416630 run_train: loss = 5.4282  (0.128 sec)
18-06-05 00:10-INFO->> Step 416640 run_train: loss = 5.4433  (0.155 sec)
18-06-05 00:10-INFO->> Step 416650 run_train: loss = 5.4188  (0.164 sec)
18-06-05 00:10-INFO->> Step 416660 run_train: loss = 5.4217  (0.156 sec)
18-06-05 00:10-INFO->> Step 416670 run_train: loss = 5.4127  (0.190 sec)
18-06-05 00:10-INFO->> Step 416680 run_train: loss = 5.4095  (0.150 sec)
18-06-05 00:10-INFO->> Step 416690 run_train: loss = 5.4654  (0.129 sec)
18-06-05 00:10-INFO->> Step 416700 run_train: loss = 5.4406  (0.158 sec)
18-06-05 00:10-INFO->> Step 416710 run_train: loss = 5.4413  (0.155 sec)
18-06-05 00:11-INFO->> Step 416720 run_train: loss = 5.4263  (0.175 sec)
18-06-05 00:11-INFO->> Step 416730 run_train: loss = 5.4519  (0.161 sec)
18-06-05 00:11-INFO->> Step 416740 run_train: loss = 5.4749  (0.143 sec)
18-06-05 00:11-INFO->> Step 416750 run_train: loss = 5.4797  (0.181 sec)
18-06-05 00:11-INFO->> Step 416760 run_train: loss = 5.4368  (0.183 sec)
18-06-05 00:11-INFO->> Step 416770 run_train: loss = 5.4185  (0.150 sec)
18-06-05 00:11-INFO->> Step 416780 run_train: loss = 5.4002  (0.188 sec)
18-06-05 00:11-INFO->> Step 416790 run_train: loss = 5.4149  (0.158 sec)
18-06-05 00:11-INFO->> Step 416800 run_train: loss = 5.4129  (0.128 sec)
18-06-05 00:11-INFO->> Step 416810 run_train: loss = 5.4560  (0.153 sec)
18-06-05 00:11-INFO->> Step 416820 run_train: loss = 5.4719  (0.116 sec)
18-06-05 00:11-INFO->> Step 416830 run_train: loss = 5.4278  (0.130 sec)
18-06-05 00:11-INFO->> Step 416840 run_train: loss = 5.4343  (0.155 sec)
18-06-05 00:11-INFO->> Step 416850 run_train: loss = 5.4739  (0.194 sec)
18-06-05 00:11-INFO->> Step 416860 run_train: loss = 5.3873  (0.162 sec)
18-06-05 00:11-INFO->> Step 416870 run_train: loss = 5.4066  (0.123 sec)
18-06-05 00:11-INFO->> Step 416880 run_train: loss = 5.4147  (0.221 sec)
18-06-05 00:11-INFO->> Step 416890 run_train: loss = 5.4356  (0.153 sec)
18-06-05 00:11-INFO->> Step 416900 run_train: loss = 5.3512  (0.204 sec)
18-06-05 00:11-INFO->> Step 416910 run_train: loss = 5.3785  (0.162 sec)
18-06-05 00:11-INFO->> Step 416920 run_train: loss = 5.3881  (0.160 sec)
18-06-05 00:11-INFO->> Step 416930 run_train: loss = 5.3817  (0.188 sec)
18-06-05 00:11-INFO->> Step 416940 run_train: loss = 5.4511  (0.165 sec)
18-06-05 00:11-INFO->> Step 416950 run_train: loss = 5.3966  (0.143 sec)
18-06-05 00:11-INFO->> Step 416960 run_train: loss = 5.4373  (0.130 sec)
18-06-05 00:11-INFO->> Step 416970 run_train: loss = 5.4015  (0.155 sec)
18-06-05 00:11-INFO->> Step 416980 run_train: loss = 5.4706  (0.159 sec)
18-06-05 00:11-INFO->> Step 416990 run_train: loss = 5.4516  (0.142 sec)
18-06-05 00:11-INFO->> Step 417000 run_train: loss = 5.4385  (0.142 sec)
18-06-05 00:11-INFO->> 2018-06-05 00:11:45.053479 Saving in ckpt
18-06-05 00:11-INFO-Test Data Eval:
18-06-05 00:12-INFO-fpr95 = 0.17204104675876727 and auc = 0.9692589407668265
18-06-05 00:12-INFO->> Step 417010 run_train: loss = 5.4120  (0.145 sec)
18-06-05 00:12-INFO->> Step 417020 run_train: loss = 5.4085  (0.156 sec)
18-06-05 00:12-INFO->> Step 417030 run_train: loss = 5.3966  (0.149 sec)
18-06-05 00:12-INFO->> Step 417040 run_train: loss = 5.4834  (0.156 sec)
18-06-05 00:12-INFO->> Step 417050 run_train: loss = 5.4508  (0.189 sec)
18-06-05 00:12-INFO->> Step 417060 run_train: loss = 5.3904  (0.176 sec)
18-06-05 00:12-INFO->> Step 417070 run_train: loss = 5.4440  (0.155 sec)
18-06-05 00:12-INFO->> Step 417080 run_train: loss = 5.4056  (0.205 sec)
18-06-05 00:12-INFO->> Step 417090 run_train: loss = 5.4495  (0.161 sec)
18-06-05 00:12-INFO->> Step 417100 run_train: loss = 5.4192  (0.161 sec)
18-06-05 00:12-INFO->> Step 417110 run_train: loss = 5.3509  (0.130 sec)
18-06-05 00:12-INFO->> Step 417120 run_train: loss = 5.3922  (0.153 sec)
18-06-05 00:12-INFO->> Step 417130 run_train: loss = 5.4293  (0.115 sec)
18-06-05 00:12-INFO->> Step 417140 run_train: loss = 5.4581  (0.183 sec)
18-06-05 00:12-INFO->> Step 417150 run_train: loss = 5.4198  (0.168 sec)
18-06-05 00:12-INFO->> Step 417160 run_train: loss = 5.3755  (0.124 sec)
18-06-05 00:12-INFO->> Step 417170 run_train: loss = 5.3944  (0.189 sec)
18-06-05 00:12-INFO->> Step 417180 run_train: loss = 5.3998  (0.158 sec)
18-06-05 00:12-INFO->> Step 417190 run_train: loss = 5.3990  (0.134 sec)
18-06-05 00:12-INFO->> Step 417200 run_train: loss = 5.4144  (0.205 sec)
18-06-05 00:12-INFO->> Step 417210 run_train: loss = 5.3701  (0.141 sec)
18-06-05 00:13-INFO->> Step 417220 run_train: loss = 5.4231  (0.188 sec)
18-06-05 00:13-INFO->> Step 417230 run_train: loss = 5.4078  (0.151 sec)
18-06-05 00:13-INFO->> Step 417240 run_train: loss = 5.4414  (0.159 sec)
18-06-05 00:13-INFO->> Step 417250 run_train: loss = 5.4463  (0.158 sec)
18-06-05 00:13-INFO->> Step 417260 run_train: loss = 5.4437  (0.183 sec)
18-06-05 00:13-INFO->> Step 417270 run_train: loss = 5.3908  (0.128 sec)
18-06-05 00:13-INFO->> Step 417280 run_train: loss = 5.4815  (0.131 sec)
18-06-05 00:13-INFO->> Step 417290 run_train: loss = 5.4291  (0.171 sec)
18-06-05 00:13-INFO->> Step 417300 run_train: loss = 5.4044  (0.130 sec)
18-06-05 00:13-INFO->> Step 417310 run_train: loss = 5.3831  (0.152 sec)
18-06-05 00:13-INFO->> Step 417320 run_train: loss = 5.3572  (0.170 sec)
18-06-05 00:13-INFO->> Step 417330 run_train: loss = 5.4578  (0.137 sec)
18-06-05 00:13-INFO->> Step 417340 run_train: loss = 5.3994  (0.162 sec)
18-06-05 00:13-INFO->> Step 417350 run_train: loss = 5.4123  (0.152 sec)
18-06-05 00:13-INFO->> Step 417360 run_train: loss = 5.4397  (0.147 sec)
18-06-05 00:13-INFO->> Step 417370 run_train: loss = 5.4662  (0.122 sec)
18-06-05 00:13-INFO->> Step 417380 run_train: loss = 5.3628  (0.161 sec)
18-06-05 00:13-INFO->> Step 417390 run_train: loss = 5.4665  (0.171 sec)
18-06-05 00:13-INFO->> Step 417400 run_train: loss = 5.3373  (0.182 sec)
18-06-05 00:13-INFO->> Step 417410 run_train: loss = 5.4329  (0.143 sec)
18-06-05 00:13-INFO->> Step 417420 run_train: loss = 5.4157  (0.190 sec)
18-06-05 00:13-INFO->> Step 417430 run_train: loss = 5.4479  (0.181 sec)
18-06-05 00:13-INFO->> Step 417440 run_train: loss = 5.4111  (0.169 sec)
18-06-05 00:13-INFO->> Step 417450 run_train: loss = 5.4489  (0.163 sec)
18-06-05 00:13-INFO->> Step 417460 run_train: loss = 5.3639  (0.154 sec)
18-06-05 00:13-INFO->> Step 417470 run_train: loss = 5.4309  (0.156 sec)
18-06-05 00:13-INFO->> Step 417480 run_train: loss = 5.3973  (0.195 sec)
18-06-05 00:13-INFO->> Step 417490 run_train: loss = 5.4693  (0.166 sec)
18-06-05 00:13-INFO->> Step 417500 run_train: loss = 5.4649  (0.142 sec)
18-06-05 00:13-INFO->> Step 417510 run_train: loss = 5.3940  (0.191 sec)
18-06-05 00:13-INFO->> Step 417520 run_train: loss = 5.4507  (0.127 sec)
18-06-05 00:13-INFO->> Step 417530 run_train: loss = 5.4572  (0.149 sec)
18-06-05 00:13-INFO->> Step 417540 run_train: loss = 5.4297  (0.134 sec)
18-06-05 00:13-INFO->> Step 417550 run_train: loss = 5.3643  (0.219 sec)
18-06-05 00:13-INFO->> Step 417560 run_train: loss = 5.4198  (0.174 sec)
18-06-05 00:13-INFO->> Step 417570 run_train: loss = 5.4367  (0.129 sec)
18-06-05 00:13-INFO->> Step 417580 run_train: loss = 5.3716  (0.162 sec)
18-06-05 00:13-INFO->> Step 417590 run_train: loss = 5.4133  (0.155 sec)
18-06-05 00:14-INFO->> Step 417600 run_train: loss = 5.4928  (0.134 sec)
18-06-05 00:14-INFO->> Step 417610 run_train: loss = 5.4251  (0.154 sec)
18-06-05 00:14-INFO->> Step 417620 run_train: loss = 5.4277  (0.156 sec)
18-06-05 00:14-INFO->> Step 417630 run_train: loss = 5.4308  (0.159 sec)
18-06-05 00:14-INFO->> Step 417640 run_train: loss = 5.4523  (0.121 sec)
18-06-05 00:14-INFO->> Step 417650 run_train: loss = 5.3937  (0.137 sec)
18-06-05 00:14-INFO->> Step 417660 run_train: loss = 5.4547  (0.153 sec)
18-06-05 00:14-INFO->> Step 417670 run_train: loss = 5.4634  (0.156 sec)
18-06-05 00:14-INFO->> Step 417680 run_train: loss = 5.4735  (0.155 sec)
18-06-05 00:14-INFO->> Step 417690 run_train: loss = 5.3102  (0.131 sec)
18-06-05 00:14-INFO->> Step 417700 run_train: loss = 5.3657  (0.153 sec)
18-06-05 00:14-INFO->> Step 417710 run_train: loss = 5.3870  (0.187 sec)
18-06-05 00:14-INFO->> Step 417720 run_train: loss = 5.4341  (0.170 sec)
18-06-05 00:14-INFO->> Step 417730 run_train: loss = 5.4076  (0.172 sec)
18-06-05 00:14-INFO->> Step 417740 run_train: loss = 5.4252  (0.161 sec)
18-06-05 00:14-INFO->> Step 417750 run_train: loss = 5.3812  (0.172 sec)
18-06-05 00:14-INFO->> Step 417760 run_train: loss = 5.4633  (0.156 sec)
18-06-05 00:14-INFO->> Step 417770 run_train: loss = 5.3856  (0.150 sec)
18-06-05 00:14-INFO->> Step 417780 run_train: loss = 5.3847  (0.157 sec)
18-06-05 00:14-INFO->> Step 417790 run_train: loss = 5.4064  (0.156 sec)
18-06-05 00:14-INFO->> Step 417800 run_train: loss = 5.4091  (0.210 sec)
18-06-05 00:14-INFO->> Step 417810 run_train: loss = 5.4260  (0.165 sec)
18-06-05 00:14-INFO->> Step 417820 run_train: loss = 5.4549  (0.128 sec)
18-06-05 00:14-INFO->> Step 417830 run_train: loss = 5.3861  (0.155 sec)
18-06-05 00:14-INFO->> Step 417840 run_train: loss = 5.4608  (0.165 sec)
18-06-05 00:14-INFO->> Step 417850 run_train: loss = 5.4805  (0.164 sec)
18-06-05 00:14-INFO->> Step 417860 run_train: loss = 5.4747  (0.149 sec)
18-06-05 00:14-INFO->> Step 417870 run_train: loss = 5.4051  (0.158 sec)
18-06-05 00:14-INFO->> Step 417880 run_train: loss = 5.3530  (0.164 sec)
18-06-05 00:14-INFO->> Step 417890 run_train: loss = 5.3832  (0.177 sec)
18-06-05 00:14-INFO->> Step 417900 run_train: loss = 5.4707  (0.150 sec)
18-06-05 00:14-INFO->> Step 417910 run_train: loss = 5.3940  (0.136 sec)
18-06-05 00:14-INFO->> Step 417920 run_train: loss = 5.4704  (0.137 sec)
18-06-05 00:14-INFO->> Step 417930 run_train: loss = 5.3260  (0.127 sec)
18-06-05 00:14-INFO->> Step 417940 run_train: loss = 5.4116  (0.161 sec)
18-06-05 00:14-INFO->> Step 417950 run_train: loss = 5.3764  (0.137 sec)
18-06-05 00:14-INFO->> Step 417960 run_train: loss = 5.4464  (0.155 sec)
18-06-05 00:14-INFO->> Step 417970 run_train: loss = 5.3461  (0.159 sec)
18-06-05 00:15-INFO->> Step 417980 run_train: loss = 5.3749  (0.141 sec)
18-06-05 00:15-INFO->> Step 417990 run_train: loss = 5.3848  (0.139 sec)
18-06-05 00:15-INFO->> Step 418000 run_train: loss = 5.4023  (0.176 sec)
18-06-05 00:15-INFO->> 2018-06-05 00:15:04.704013 Saving in ckpt
18-06-05 00:15-INFO-Test Data Eval:
18-06-05 00:15-INFO-fpr95 = 0.1706960680127524 and auc = 0.9694441082410845
18-06-05 00:15-INFO->> Step 418010 run_train: loss = 5.4316  (0.147 sec)
18-06-05 00:15-INFO->> Step 418020 run_train: loss = 5.4169  (0.122 sec)
18-06-05 00:15-INFO->> Step 418030 run_train: loss = 5.4884  (0.146 sec)
18-06-05 00:15-INFO->> Step 418040 run_train: loss = 5.4196  (0.148 sec)
18-06-05 00:15-INFO->> Step 418050 run_train: loss = 5.4036  (0.171 sec)
18-06-05 00:15-INFO->> Step 418060 run_train: loss = 5.4423  (0.167 sec)
18-06-05 00:15-INFO->> Step 418070 run_train: loss = 5.4147  (0.154 sec)
18-06-05 00:15-INFO->> Step 418080 run_train: loss = 5.4314  (0.139 sec)
18-06-05 00:15-INFO->> Step 418090 run_train: loss = 5.4339  (0.167 sec)
18-06-05 00:16-INFO->> Step 418100 run_train: loss = 5.4828  (0.167 sec)
18-06-05 00:16-INFO->> Step 418110 run_train: loss = 5.4818  (0.173 sec)
18-06-05 00:16-INFO->> Step 418120 run_train: loss = 5.3641  (0.183 sec)
18-06-05 00:16-INFO->> Step 418130 run_train: loss = 5.4332  (0.159 sec)
18-06-05 00:16-INFO->> Step 418140 run_train: loss = 5.3943  (0.172 sec)
18-06-05 00:16-INFO->> Step 418150 run_train: loss = 5.4150  (0.134 sec)
18-06-05 00:16-INFO->> Step 418160 run_train: loss = 5.4313  (0.142 sec)
18-06-05 00:16-INFO->> Step 418170 run_train: loss = 5.3764  (0.158 sec)
18-06-05 00:16-INFO->> Step 418180 run_train: loss = 5.4458  (0.181 sec)
18-06-05 00:16-INFO->> Step 418190 run_train: loss = 5.3637  (0.129 sec)
18-06-05 00:16-INFO->> Step 418200 run_train: loss = 5.3701  (0.146 sec)
18-06-05 00:16-INFO->> Step 418210 run_train: loss = 5.4306  (0.156 sec)
18-06-05 00:16-INFO->> Step 418220 run_train: loss = 5.5076  (0.155 sec)
18-06-05 00:16-INFO->> Step 418230 run_train: loss = 5.3741  (0.179 sec)
18-06-05 00:16-INFO->> Step 418240 run_train: loss = 5.4289  (0.181 sec)
18-06-05 00:16-INFO->> Step 418250 run_train: loss = 5.4666  (0.183 sec)
18-06-05 00:16-INFO->> Step 418260 run_train: loss = 5.3870  (0.202 sec)
18-06-05 00:16-INFO->> Step 418270 run_train: loss = 5.3978  (0.136 sec)
18-06-05 00:16-INFO->> Step 418280 run_train: loss = 5.4357  (0.171 sec)
18-06-05 00:16-INFO->> Step 418290 run_train: loss = 5.3538  (0.127 sec)
18-06-05 00:16-INFO->> Step 418300 run_train: loss = 5.4612  (0.142 sec)
18-06-05 00:16-INFO->> Step 418310 run_train: loss = 5.3986  (0.144 sec)
18-06-05 00:16-INFO->> Step 418320 run_train: loss = 5.4727  (0.154 sec)
18-06-05 00:16-INFO->> Step 418330 run_train: loss = 5.4239  (0.190 sec)
18-06-05 00:16-INFO->> Step 418340 run_train: loss = 5.3854  (0.150 sec)
18-06-05 00:16-INFO->> Step 418350 run_train: loss = 5.4986  (0.122 sec)
18-06-05 00:16-INFO->> Step 418360 run_train: loss = 5.4368  (0.132 sec)
18-06-05 00:16-INFO->> Step 418370 run_train: loss = 5.4303  (0.165 sec)
18-06-05 00:16-INFO->> Step 418380 run_train: loss = 5.4176  (0.167 sec)
18-06-05 00:16-INFO->> Step 418390 run_train: loss = 5.4549  (0.162 sec)
18-06-05 00:16-INFO->> Step 418400 run_train: loss = 5.4294  (0.195 sec)
18-06-05 00:16-INFO->> Step 418410 run_train: loss = 5.3851  (0.180 sec)
18-06-05 00:16-INFO->> Step 418420 run_train: loss = 5.4097  (0.154 sec)
18-06-05 00:16-INFO->> Step 418430 run_train: loss = 5.4195  (0.160 sec)
18-06-05 00:16-INFO->> Step 418440 run_train: loss = 5.3921  (0.161 sec)
18-06-05 00:16-INFO->> Step 418450 run_train: loss = 5.4072  (0.155 sec)
18-06-05 00:16-INFO->> Step 418460 run_train: loss = 5.4147  (0.170 sec)
18-06-05 00:16-INFO->> Step 418470 run_train: loss = 5.4354  (0.196 sec)
18-06-05 00:17-INFO->> Step 418480 run_train: loss = 5.3890  (0.181 sec)
18-06-05 00:17-INFO->> Step 418490 run_train: loss = 5.4632  (0.141 sec)
18-06-05 00:17-INFO->> Step 418500 run_train: loss = 5.4114  (0.143 sec)
18-06-05 00:17-INFO->> Step 418510 run_train: loss = 5.4310  (0.152 sec)
18-06-05 00:17-INFO->> Step 418520 run_train: loss = 5.4625  (0.157 sec)
18-06-05 00:17-INFO->> Step 418530 run_train: loss = 5.4309  (0.171 sec)
18-06-05 00:17-INFO->> Step 418540 run_train: loss = 5.4624  (0.211 sec)
18-06-05 00:17-INFO->> Step 418550 run_train: loss = 5.4294  (0.166 sec)
18-06-05 00:17-INFO->> Step 418560 run_train: loss = 5.4626  (0.164 sec)
18-06-05 00:17-INFO->> Step 418570 run_train: loss = 5.4172  (0.158 sec)
18-06-05 00:17-INFO->> Step 418580 run_train: loss = 5.3908  (0.171 sec)
18-06-05 00:17-INFO->> Step 418590 run_train: loss = 5.5051  (0.195 sec)
18-06-05 00:17-INFO->> Step 418600 run_train: loss = 5.3806  (0.146 sec)
18-06-05 00:17-INFO->> Step 418610 run_train: loss = 5.3487  (0.180 sec)
18-06-05 00:17-INFO->> Step 418620 run_train: loss = 5.4447  (0.098 sec)
18-06-05 00:17-INFO->> Step 418630 run_train: loss = 5.4601  (0.160 sec)
18-06-05 00:17-INFO->> Step 418640 run_train: loss = 5.3984  (0.165 sec)
18-06-05 00:17-INFO->> Step 418650 run_train: loss = 5.3823  (0.139 sec)
18-06-05 00:17-INFO->> Step 418660 run_train: loss = 5.4240  (0.142 sec)
18-06-05 00:17-INFO->> Step 418670 run_train: loss = 5.4092  (0.123 sec)
18-06-05 00:17-INFO->> Step 418680 run_train: loss = 5.4580  (0.151 sec)
18-06-05 00:17-INFO->> Step 418690 run_train: loss = 5.4791  (0.158 sec)
18-06-05 00:17-INFO->> Step 418700 run_train: loss = 5.3858  (0.147 sec)
18-06-05 00:17-INFO->> Step 418710 run_train: loss = 5.4305  (0.143 sec)
18-06-05 00:17-INFO->> Step 418720 run_train: loss = 5.4221  (0.170 sec)
18-06-05 00:17-INFO->> Step 418730 run_train: loss = 5.3851  (0.169 sec)
18-06-05 00:17-INFO->> Step 418740 run_train: loss = 5.3642  (0.146 sec)
18-06-05 00:17-INFO->> Step 418750 run_train: loss = 5.4379  (0.176 sec)
18-06-05 00:17-INFO->> Step 418760 run_train: loss = 5.4865  (0.164 sec)
18-06-05 00:17-INFO->> Step 418770 run_train: loss = 5.3859  (0.188 sec)
18-06-05 00:17-INFO->> Step 418780 run_train: loss = 5.4458  (0.159 sec)
18-06-05 00:17-INFO->> Step 418790 run_train: loss = 5.4554  (0.166 sec)
18-06-05 00:17-INFO->> Step 418800 run_train: loss = 5.4037  (0.151 sec)
18-06-05 00:17-INFO->> Step 418810 run_train: loss = 5.4173  (0.165 sec)
18-06-05 00:17-INFO->> Step 418820 run_train: loss = 5.4040  (0.170 sec)
18-06-05 00:17-INFO->> Step 418830 run_train: loss = 5.4771  (0.170 sec)
18-06-05 00:17-INFO->> Step 418840 run_train: loss = 5.4145  (0.215 sec)
18-06-05 00:18-INFO->> Step 418850 run_train: loss = 5.4284  (0.135 sec)
18-06-05 00:18-INFO->> Step 418860 run_train: loss = 5.4139  (0.134 sec)
18-06-05 00:18-INFO->> Step 418870 run_train: loss = 5.4469  (0.185 sec)
18-06-05 00:18-INFO->> Step 418880 run_train: loss = 5.3791  (0.156 sec)
18-06-05 00:18-INFO->> Step 418890 run_train: loss = 5.4250  (0.161 sec)
18-06-05 00:18-INFO->> Step 418900 run_train: loss = 5.4064  (0.188 sec)
18-06-05 00:18-INFO->> Step 418910 run_train: loss = 5.4017  (0.154 sec)
18-06-05 00:18-INFO->> Step 418920 run_train: loss = 5.4921  (0.178 sec)
18-06-05 00:18-INFO->> Step 418930 run_train: loss = 5.4398  (0.179 sec)
18-06-05 00:18-INFO->> Step 418940 run_train: loss = 5.3758  (0.177 sec)
18-06-05 00:18-INFO->> Step 418950 run_train: loss = 5.3813  (0.149 sec)
18-06-05 00:18-INFO->> Step 418960 run_train: loss = 5.4633  (0.155 sec)
18-06-05 00:18-INFO->> Step 418970 run_train: loss = 5.3965  (0.174 sec)
18-06-05 00:18-INFO->> Step 418980 run_train: loss = 5.4236  (0.173 sec)
18-06-05 00:18-INFO->> Step 418990 run_train: loss = 5.4266  (0.162 sec)
18-06-05 00:18-INFO->> Step 419000 run_train: loss = 5.4360  (0.180 sec)
18-06-05 00:18-INFO->> 2018-06-05 00:18:24.356620 Saving in ckpt
18-06-05 00:18-INFO-Test Data Eval:
18-06-05 00:19-INFO-fpr95 = 0.171692348565356 and auc = 0.9693177205377774
18-06-05 00:19-INFO->> Step 419010 run_train: loss = 5.4385  (0.183 sec)
18-06-05 00:19-INFO->> Step 419020 run_train: loss = 5.4084  (0.143 sec)
18-06-05 00:19-INFO->> Step 419030 run_train: loss = 5.4027  (0.119 sec)
18-06-05 00:19-INFO->> Step 419040 run_train: loss = 5.4713  (0.197 sec)
18-06-05 00:19-INFO->> Step 419050 run_train: loss = 5.4481  (0.152 sec)
18-06-05 00:19-INFO->> Step 419060 run_train: loss = 5.3917  (0.176 sec)
18-06-05 00:19-INFO->> Step 419070 run_train: loss = 5.4472  (0.156 sec)
18-06-05 00:19-INFO->> Step 419080 run_train: loss = 5.4576  (0.157 sec)
18-06-05 00:19-INFO->> Step 419090 run_train: loss = 5.4026  (0.211 sec)
18-06-05 00:19-INFO->> Step 419100 run_train: loss = 5.4418  (0.165 sec)
18-06-05 00:19-INFO->> Step 419110 run_train: loss = 5.4076  (0.134 sec)
18-06-05 00:19-INFO->> Step 419120 run_train: loss = 5.4526  (0.158 sec)
18-06-05 00:19-INFO->> Step 419130 run_train: loss = 5.4035  (0.133 sec)
18-06-05 00:19-INFO->> Step 419140 run_train: loss = 5.4430  (0.129 sec)
18-06-05 00:19-INFO->> Step 419150 run_train: loss = 5.3714  (0.144 sec)
18-06-05 00:19-INFO->> Step 419160 run_train: loss = 5.4603  (0.128 sec)
18-06-05 00:19-INFO->> Step 419170 run_train: loss = 5.4302  (0.159 sec)
18-06-05 00:19-INFO->> Step 419180 run_train: loss = 5.4319  (0.151 sec)
18-06-05 00:19-INFO->> Step 419190 run_train: loss = 5.4476  (0.159 sec)
18-06-05 00:19-INFO->> Step 419200 run_train: loss = 5.3796  (0.167 sec)
18-06-05 00:19-INFO->> Step 419210 run_train: loss = 5.4045  (0.155 sec)
18-06-05 00:19-INFO->> Step 419220 run_train: loss = 5.3366  (0.189 sec)
18-06-05 00:19-INFO->> Step 419230 run_train: loss = 5.4728  (0.159 sec)
18-06-05 00:19-INFO->> Step 419240 run_train: loss = 5.4197  (0.152 sec)
18-06-05 00:19-INFO->> Step 419250 run_train: loss = 5.3941  (0.202 sec)
18-06-05 00:19-INFO->> Step 419260 run_train: loss = 5.4003  (0.167 sec)
18-06-05 00:19-INFO->> Step 419270 run_train: loss = 5.4085  (0.189 sec)
18-06-05 00:19-INFO->> Step 419280 run_train: loss = 5.4017  (0.145 sec)
18-06-05 00:19-INFO->> Step 419290 run_train: loss = 5.4227  (0.158 sec)
18-06-05 00:19-INFO->> Step 419300 run_train: loss = 5.4406  (0.155 sec)
18-06-05 00:19-INFO->> Step 419310 run_train: loss = 5.3999  (0.164 sec)
18-06-05 00:19-INFO->> Step 419320 run_train: loss = 5.3846  (0.163 sec)
18-06-05 00:19-INFO->> Step 419330 run_train: loss = 5.3721  (0.159 sec)
18-06-05 00:19-INFO->> Step 419340 run_train: loss = 5.3921  (0.134 sec)
18-06-05 00:20-INFO->> Step 419350 run_train: loss = 5.4955  (0.145 sec)
18-06-05 00:20-INFO->> Step 419360 run_train: loss = 5.3721  (0.145 sec)
18-06-05 00:20-INFO->> Step 419370 run_train: loss = 5.4621  (0.185 sec)
18-06-05 00:20-INFO->> Step 419380 run_train: loss = 5.4734  (0.185 sec)
18-06-05 00:20-INFO->> Step 419390 run_train: loss = 5.3992  (0.156 sec)
18-06-05 00:20-INFO->> Step 419400 run_train: loss = 5.3984  (0.144 sec)
18-06-05 00:20-INFO->> Step 419410 run_train: loss = 5.4656  (0.144 sec)
18-06-05 00:20-INFO->> Step 419420 run_train: loss = 5.4647  (0.142 sec)
18-06-05 00:20-INFO->> Step 419430 run_train: loss = 5.2795  (0.170 sec)
18-06-05 00:20-INFO->> Step 419440 run_train: loss = 5.3973  (0.136 sec)
18-06-05 00:20-INFO->> Step 419450 run_train: loss = 5.4203  (0.148 sec)
18-06-05 00:20-INFO->> Step 419460 run_train: loss = 5.4360  (0.143 sec)
18-06-05 00:20-INFO->> Step 419470 run_train: loss = 5.4315  (0.158 sec)
18-06-05 00:20-INFO->> Step 419480 run_train: loss = 5.3861  (0.129 sec)
18-06-05 00:20-INFO->> Step 419490 run_train: loss = 5.4909  (0.185 sec)
18-06-05 00:20-INFO->> Step 419500 run_train: loss = 5.4038  (0.163 sec)
18-06-05 00:20-INFO->> Step 419510 run_train: loss = 5.4274  (0.124 sec)
18-06-05 00:20-INFO->> Step 419520 run_train: loss = 5.3762  (0.154 sec)
18-06-05 00:20-INFO->> Step 419530 run_train: loss = 5.4653  (0.166 sec)
18-06-05 00:20-INFO->> Step 419540 run_train: loss = 5.4109  (0.189 sec)
18-06-05 00:20-INFO->> Step 419550 run_train: loss = 5.3440  (0.144 sec)
18-06-05 00:20-INFO->> Step 419560 run_train: loss = 5.4361  (0.139 sec)
18-06-05 00:20-INFO->> Step 419570 run_train: loss = 5.4358  (0.119 sec)
18-06-05 00:20-INFO->> Step 419580 run_train: loss = 5.3481  (0.132 sec)
18-06-05 00:20-INFO->> Step 419590 run_train: loss = 5.4398  (0.194 sec)
18-06-05 00:20-INFO->> Step 419600 run_train: loss = 5.3591  (0.174 sec)
18-06-05 00:20-INFO->> Step 419610 run_train: loss = 5.4528  (0.143 sec)
18-06-05 00:20-INFO->> Step 419620 run_train: loss = 5.3935  (0.147 sec)
18-06-05 00:20-INFO->> Step 419630 run_train: loss = 5.4299  (0.138 sec)
18-06-05 00:20-INFO->> Step 419640 run_train: loss = 5.4093  (0.174 sec)
18-06-05 00:20-INFO->> Step 419650 run_train: loss = 5.3875  (0.152 sec)
18-06-05 00:20-INFO->> Step 419660 run_train: loss = 5.3956  (0.154 sec)
18-06-05 00:20-INFO->> Step 419670 run_train: loss = 5.4713  (0.121 sec)
18-06-05 00:20-INFO->> Step 419680 run_train: loss = 5.4065  (0.183 sec)
18-06-05 00:20-INFO->> Step 419690 run_train: loss = 5.4022  (0.167 sec)
18-06-05 00:20-INFO->> Step 419700 run_train: loss = 5.4638  (0.163 sec)
18-06-05 00:20-INFO->> Step 419710 run_train: loss = 5.4281  (0.131 sec)
18-06-05 00:20-INFO->> Step 419720 run_train: loss = 5.4074  (0.146 sec)
18-06-05 00:21-INFO->> Step 419730 run_train: loss = 5.3883  (0.135 sec)
18-06-05 00:21-INFO->> Step 419740 run_train: loss = 5.4549  (0.153 sec)
18-06-05 00:21-INFO->> Step 419750 run_train: loss = 5.4141  (0.153 sec)
18-06-05 00:21-INFO->> Step 419760 run_train: loss = 5.4630  (0.157 sec)
18-06-05 00:21-INFO->> Step 419770 run_train: loss = 5.3800  (0.144 sec)
18-06-05 00:21-INFO->> Step 419780 run_train: loss = 5.4553  (0.155 sec)
18-06-05 00:21-INFO->> Step 419790 run_train: loss = 5.4345  (0.122 sec)
18-06-05 00:21-INFO->> Step 419800 run_train: loss = 5.4344  (0.130 sec)
18-06-05 00:21-INFO->> Step 419810 run_train: loss = 5.4148  (0.202 sec)
18-06-05 00:21-INFO->> Step 419820 run_train: loss = 5.4270  (0.152 sec)
18-06-05 00:21-INFO->> Step 419830 run_train: loss = 5.4412  (0.146 sec)
18-06-05 00:21-INFO->> Step 419840 run_train: loss = 5.4024  (0.158 sec)
18-06-05 00:21-INFO->> Step 419850 run_train: loss = 5.3942  (0.157 sec)
18-06-05 00:21-INFO->> Step 419860 run_train: loss = 5.3624  (0.142 sec)
18-06-05 00:21-INFO->> Step 419870 run_train: loss = 5.4013  (0.159 sec)
18-06-05 00:21-INFO->> Step 419880 run_train: loss = 5.4420  (0.138 sec)
18-06-05 00:21-INFO->> Step 419890 run_train: loss = 5.4819  (0.152 sec)
18-06-05 00:21-INFO->> Step 419900 run_train: loss = 5.4136  (0.175 sec)
18-06-05 00:21-INFO->> Step 419910 run_train: loss = 5.4331  (0.158 sec)
18-06-05 00:21-INFO->> Step 419920 run_train: loss = 5.4088  (0.143 sec)
18-06-05 00:21-INFO->> Step 419930 run_train: loss = 5.4064  (0.160 sec)
18-06-05 00:21-INFO->> Step 419940 run_train: loss = 5.4471  (0.152 sec)
18-06-05 00:21-INFO->> Step 419950 run_train: loss = 5.3604  (0.130 sec)
18-06-05 00:21-INFO->> Step 419960 run_train: loss = 5.3932  (0.159 sec)
18-06-05 00:21-INFO->> Step 419970 run_train: loss = 5.3680  (0.200 sec)
18-06-05 00:21-INFO->> Step 419980 run_train: loss = 5.4083  (0.155 sec)
18-06-05 00:21-INFO->> Step 419990 run_train: loss = 5.4643  (0.172 sec)
18-06-05 00:21-INFO->> Step 420000 run_train: loss = 5.4624  (0.125 sec)
18-06-05 00:21-INFO->> 2018-06-05 00:21:43.645835 Saving in ckpt
18-06-05 00:21-INFO-Test Data Eval:
18-06-05 00:22-INFO-fpr95 = 0.1736184909670563 and auc = 0.96909004083761
18-06-05 00:22-INFO->> Step 420010 run_train: loss = 5.3440  (0.184 sec)
18-06-05 00:22-INFO->> Step 420020 run_train: loss = 5.4445  (0.183 sec)
18-06-05 00:22-INFO->> Step 420030 run_train: loss = 5.4392  (0.153 sec)
18-06-05 00:22-INFO->> Step 420040 run_train: loss = 5.4393  (0.132 sec)
18-06-05 00:22-INFO->> Step 420050 run_train: loss = 5.4569  (0.167 sec)
18-06-05 00:22-INFO->> Step 420060 run_train: loss = 5.4040  (0.162 sec)
18-06-05 00:22-INFO->> Step 420070 run_train: loss = 5.4405  (0.163 sec)
18-06-05 00:22-INFO->> Step 420080 run_train: loss = 5.3946  (0.155 sec)
18-06-05 00:22-INFO->> Step 420090 run_train: loss = 5.3802  (0.161 sec)
18-06-05 00:22-INFO->> Step 420100 run_train: loss = 5.3931  (0.181 sec)
18-06-05 00:22-INFO->> Step 420110 run_train: loss = 5.4133  (0.157 sec)
18-06-05 00:22-INFO->> Step 420120 run_train: loss = 5.3913  (0.112 sec)
18-06-05 00:22-INFO->> Step 420130 run_train: loss = 5.4766  (0.185 sec)
18-06-05 00:22-INFO->> Step 420140 run_train: loss = 5.4573  (0.165 sec)
18-06-05 00:22-INFO->> Step 420150 run_train: loss = 5.3710  (0.187 sec)
18-06-05 00:22-INFO->> Step 420160 run_train: loss = 5.4421  (0.173 sec)
18-06-05 00:22-INFO->> Step 420170 run_train: loss = 5.4232  (0.150 sec)
18-06-05 00:22-INFO->> Step 420180 run_train: loss = 5.4240  (0.138 sec)
18-06-05 00:22-INFO->> Step 420190 run_train: loss = 5.4622  (0.187 sec)
18-06-05 00:22-INFO->> Step 420200 run_train: loss = 5.4318  (0.160 sec)
18-06-05 00:22-INFO->> Step 420210 run_train: loss = 5.3434  (0.149 sec)
18-06-05 00:22-INFO->> Step 420220 run_train: loss = 5.3876  (0.177 sec)
18-06-05 00:23-INFO->> Step 420230 run_train: loss = 5.4452  (0.169 sec)
18-06-05 00:23-INFO->> Step 420240 run_train: loss = 5.4011  (0.154 sec)
18-06-05 00:23-INFO->> Step 420250 run_train: loss = 5.4662  (0.160 sec)
18-06-05 00:23-INFO->> Step 420260 run_train: loss = 5.3289  (0.137 sec)
18-06-05 00:23-INFO->> Step 420270 run_train: loss = 5.4731  (0.184 sec)
18-06-05 00:23-INFO->> Step 420280 run_train: loss = 5.4453  (0.175 sec)
18-06-05 00:23-INFO->> Step 420290 run_train: loss = 5.3715  (0.128 sec)
18-06-05 00:23-INFO->> Step 420300 run_train: loss = 5.4535  (0.189 sec)
18-06-05 00:23-INFO->> Step 420310 run_train: loss = 5.4487  (0.164 sec)
18-06-05 00:23-INFO->> Step 420320 run_train: loss = 5.4969  (0.157 sec)
18-06-05 00:23-INFO->> Step 420330 run_train: loss = 5.4433  (0.172 sec)
18-06-05 00:23-INFO->> Step 420340 run_train: loss = 5.4221  (0.173 sec)
18-06-05 00:23-INFO->> Step 420350 run_train: loss = 5.4130  (0.183 sec)
18-06-05 00:23-INFO->> Step 420360 run_train: loss = 5.4486  (0.169 sec)
18-06-05 00:23-INFO->> Step 420370 run_train: loss = 5.4214  (0.160 sec)
18-06-05 00:23-INFO->> Step 420380 run_train: loss = 5.4742  (0.163 sec)
18-06-05 00:23-INFO->> Step 420390 run_train: loss = 5.4312  (0.159 sec)
18-06-05 00:23-INFO->> Step 420400 run_train: loss = 5.4488  (0.174 sec)
18-06-05 00:23-INFO->> Step 420410 run_train: loss = 5.3890  (0.186 sec)
18-06-05 00:23-INFO->> Step 420420 run_train: loss = 5.4003  (0.145 sec)
18-06-05 00:23-INFO->> Step 420430 run_train: loss = 5.4145  (0.167 sec)
18-06-05 00:23-INFO->> Step 420440 run_train: loss = 5.4528  (0.147 sec)
18-06-05 00:23-INFO->> Step 420450 run_train: loss = 5.4510  (0.157 sec)
18-06-05 00:23-INFO->> Step 420460 run_train: loss = 5.3645  (0.177 sec)
18-06-05 00:23-INFO->> Step 420470 run_train: loss = 5.4581  (0.132 sec)
18-06-05 00:23-INFO->> Step 420480 run_train: loss = 5.4085  (0.143 sec)
18-06-05 00:23-INFO->> Step 420490 run_train: loss = 5.4239  (0.155 sec)
18-06-05 00:23-INFO->> Step 420500 run_train: loss = 5.2919  (0.147 sec)
18-06-05 00:23-INFO->> Step 420510 run_train: loss = 5.3815  (0.169 sec)
18-06-05 00:23-INFO->> Step 420520 run_train: loss = 5.4017  (0.196 sec)
18-06-05 00:23-INFO->> Step 420530 run_train: loss = 5.4547  (0.171 sec)
18-06-05 00:23-INFO->> Step 420540 run_train: loss = 5.4304  (0.128 sec)
18-06-05 00:23-INFO->> Step 420550 run_train: loss = 5.4053  (0.131 sec)
18-06-05 00:23-INFO->> Step 420560 run_train: loss = 5.4242  (0.175 sec)
18-06-05 00:23-INFO->> Step 420570 run_train: loss = 5.4260  (0.166 sec)
18-06-05 00:23-INFO->> Step 420580 run_train: loss = 5.4613  (0.161 sec)
18-06-05 00:23-INFO->> Step 420590 run_train: loss = 5.3780  (0.161 sec)
18-06-05 00:23-INFO->> Step 420600 run_train: loss = 5.4043  (0.148 sec)
18-06-05 00:24-INFO->> Step 420610 run_train: loss = 5.4763  (0.174 sec)
18-06-05 00:24-INFO->> Step 420620 run_train: loss = 5.4469  (0.138 sec)
18-06-05 00:24-INFO->> Step 420630 run_train: loss = 5.3874  (0.159 sec)
18-06-05 00:24-INFO->> Step 420640 run_train: loss = 5.4179  (0.168 sec)
18-06-05 00:24-INFO->> Step 420650 run_train: loss = 5.4121  (0.165 sec)
18-06-05 00:24-INFO->> Step 420660 run_train: loss = 5.3001  (0.158 sec)
18-06-05 00:24-INFO->> Step 420670 run_train: loss = 5.4496  (0.171 sec)
18-06-05 00:24-INFO->> Step 420680 run_train: loss = 5.4106  (0.134 sec)
18-06-05 00:24-INFO->> Step 420690 run_train: loss = 5.3540  (0.181 sec)
18-06-05 00:24-INFO->> Step 420700 run_train: loss = 5.4189  (0.144 sec)
18-06-05 00:24-INFO->> Step 420710 run_train: loss = 5.4373  (0.141 sec)
18-06-05 00:24-INFO->> Step 420720 run_train: loss = 5.4146  (0.134 sec)
18-06-05 00:24-INFO->> Step 420730 run_train: loss = 5.3842  (0.177 sec)
18-06-05 00:24-INFO->> Step 420740 run_train: loss = 5.4301  (0.176 sec)
18-06-05 00:24-INFO->> Step 420750 run_train: loss = 5.4854  (0.169 sec)
18-06-05 00:24-INFO->> Step 420760 run_train: loss = 5.4333  (0.161 sec)
18-06-05 00:24-INFO->> Step 420770 run_train: loss = 5.4171  (0.205 sec)
18-06-05 00:24-INFO->> Step 420780 run_train: loss = 5.4226  (0.145 sec)
18-06-05 00:24-INFO->> Step 420790 run_train: loss = 5.3239  (0.151 sec)
18-06-05 00:24-INFO->> Step 420800 run_train: loss = 5.3856  (0.154 sec)
18-06-05 00:24-INFO->> Step 420810 run_train: loss = 5.4085  (0.149 sec)
18-06-05 00:24-INFO->> Step 420820 run_train: loss = 5.4326  (0.171 sec)
18-06-05 00:24-INFO->> Step 420830 run_train: loss = 5.3682  (0.173 sec)
18-06-05 00:24-INFO->> Step 420840 run_train: loss = 5.4142  (0.153 sec)
18-06-05 00:24-INFO->> Step 420850 run_train: loss = 5.4268  (0.168 sec)
18-06-05 00:24-INFO->> Step 420860 run_train: loss = 5.4481  (0.163 sec)
18-06-05 00:24-INFO->> Step 420870 run_train: loss = 5.4436  (0.182 sec)
18-06-05 00:24-INFO->> Step 420880 run_train: loss = 5.4198  (0.132 sec)
18-06-05 00:24-INFO->> Step 420890 run_train: loss = 5.4194  (0.142 sec)
18-06-05 00:24-INFO->> Step 420900 run_train: loss = 5.4262  (0.184 sec)
18-06-05 00:24-INFO->> Step 420910 run_train: loss = 5.3962  (0.150 sec)
18-06-05 00:24-INFO->> Step 420920 run_train: loss = 5.3942  (0.142 sec)
18-06-05 00:24-INFO->> Step 420930 run_train: loss = 5.3419  (0.154 sec)
18-06-05 00:24-INFO->> Step 420940 run_train: loss = 5.3901  (0.142 sec)
18-06-05 00:24-INFO->> Step 420950 run_train: loss = 5.4110  (0.127 sec)
18-06-05 00:24-INFO->> Step 420960 run_train: loss = 5.3464  (0.153 sec)
18-06-05 00:24-INFO->> Step 420970 run_train: loss = 5.4245  (0.160 sec)
18-06-05 00:24-INFO->> Step 420980 run_train: loss = 5.3406  (0.178 sec)
18-06-05 00:25-INFO->> Step 420990 run_train: loss = 5.3706  (0.176 sec)
18-06-05 00:25-INFO->> Step 421000 run_train: loss = 5.4300  (0.157 sec)
18-06-05 00:25-INFO->> 2018-06-05 00:25:02.988705 Saving in ckpt
18-06-05 00:25-INFO-Test Data Eval:
18-06-05 00:25-INFO-fpr95 = 0.17556123804463336 and auc = 0.9689786007269967
18-06-05 00:25-INFO->> Step 421010 run_train: loss = 5.4430  (0.151 sec)
18-06-05 00:25-INFO->> Step 421020 run_train: loss = 5.4217  (0.177 sec)
18-06-05 00:25-INFO->> Step 421030 run_train: loss = 5.3303  (0.164 sec)
18-06-05 00:25-INFO->> Step 421040 run_train: loss = 5.3853  (0.158 sec)
18-06-05 00:25-INFO->> Step 421050 run_train: loss = 5.4559  (0.137 sec)
18-06-05 00:25-INFO->> Step 421060 run_train: loss = 5.4135  (0.140 sec)
18-06-05 00:25-INFO->> Step 421070 run_train: loss = 5.4441  (0.163 sec)
18-06-05 00:25-INFO->> Step 421080 run_train: loss = 5.4729  (0.158 sec)
18-06-05 00:25-INFO->> Step 421090 run_train: loss = 5.4304  (0.149 sec)
18-06-05 00:25-INFO->> Step 421100 run_train: loss = 5.3913  (0.177 sec)
18-06-05 00:26-INFO->> Step 421110 run_train: loss = 5.4706  (0.167 sec)
18-06-05 00:26-INFO->> Step 421120 run_train: loss = 5.4217  (0.142 sec)
18-06-05 00:26-INFO->> Step 421130 run_train: loss = 5.4366  (0.138 sec)
18-06-05 00:26-INFO->> Step 421140 run_train: loss = 5.4473  (0.139 sec)
18-06-05 00:26-INFO->> Step 421150 run_train: loss = 5.4030  (0.140 sec)
18-06-05 00:26-INFO->> Step 421160 run_train: loss = 5.4452  (0.117 sec)
18-06-05 00:26-INFO->> Step 421170 run_train: loss = 5.4017  (0.162 sec)
18-06-05 00:26-INFO->> Step 421180 run_train: loss = 5.3739  (0.184 sec)
18-06-05 00:26-INFO->> Step 421190 run_train: loss = 5.4427  (0.164 sec)
18-06-05 00:26-INFO->> Step 421200 run_train: loss = 5.4702  (0.148 sec)
18-06-05 00:26-INFO->> Step 421210 run_train: loss = 5.4350  (0.155 sec)
18-06-05 00:26-INFO->> Step 421220 run_train: loss = 5.4535  (0.118 sec)
18-06-05 00:26-INFO->> Step 421230 run_train: loss = 5.4580  (0.217 sec)
18-06-05 00:26-INFO->> Step 421240 run_train: loss = 5.4430  (0.147 sec)
18-06-05 00:26-INFO->> Step 421250 run_train: loss = 5.3990  (0.142 sec)
18-06-05 00:26-INFO->> Step 421260 run_train: loss = 5.4035  (0.159 sec)
18-06-05 00:26-INFO->> Step 421270 run_train: loss = 5.4870  (0.133 sec)
18-06-05 00:26-INFO->> Step 421280 run_train: loss = 5.3913  (0.160 sec)
18-06-05 00:26-INFO->> Step 421290 run_train: loss = 5.3885  (0.158 sec)
18-06-05 00:26-INFO->> Step 421300 run_train: loss = 5.3616  (0.163 sec)
18-06-05 00:26-INFO->> Step 421310 run_train: loss = 5.4244  (0.161 sec)
18-06-05 00:26-INFO->> Step 421320 run_train: loss = 5.3954  (0.134 sec)
18-06-05 00:26-INFO->> Step 421330 run_train: loss = 5.3715  (0.166 sec)
18-06-05 00:26-INFO->> Step 421340 run_train: loss = 5.4030  (0.164 sec)
18-06-05 00:26-INFO->> Step 421350 run_train: loss = 5.4174  (0.148 sec)
18-06-05 00:26-INFO->> Step 421360 run_train: loss = 5.4799  (0.163 sec)
18-06-05 00:26-INFO->> Step 421370 run_train: loss = 5.4452  (0.146 sec)
18-06-05 00:26-INFO->> Step 421380 run_train: loss = 5.4134  (0.158 sec)
18-06-05 00:26-INFO->> Step 421390 run_train: loss = 5.3895  (0.192 sec)
18-06-05 00:26-INFO->> Step 421400 run_train: loss = 5.4313  (0.156 sec)
18-06-05 00:26-INFO->> Step 421410 run_train: loss = 5.3947  (0.116 sec)
18-06-05 00:26-INFO->> Step 421420 run_train: loss = 5.4164  (0.178 sec)
18-06-05 00:26-INFO->> Step 421430 run_train: loss = 5.3927  (0.161 sec)
18-06-05 00:26-INFO->> Step 421440 run_train: loss = 5.4602  (0.160 sec)
18-06-05 00:26-INFO->> Step 421450 run_train: loss = 5.4562  (0.151 sec)
18-06-05 00:26-INFO->> Step 421460 run_train: loss = 5.4320  (0.137 sec)
18-06-05 00:26-INFO->> Step 421470 run_train: loss = 5.3622  (0.161 sec)
18-06-05 00:26-INFO->> Step 421480 run_train: loss = 5.3619  (0.146 sec)
18-06-05 00:27-INFO->> Step 421490 run_train: loss = 5.4195  (0.176 sec)
18-06-05 00:27-INFO->> Step 421500 run_train: loss = 5.3881  (0.127 sec)
18-06-05 00:27-INFO->> Step 421510 run_train: loss = 5.3556  (0.162 sec)
18-06-05 00:27-INFO->> Step 421520 run_train: loss = 5.4017  (0.129 sec)
18-06-05 00:27-INFO->> Step 421530 run_train: loss = 5.3887  (0.142 sec)
18-06-05 00:27-INFO->> Step 421540 run_train: loss = 5.4621  (0.149 sec)
18-06-05 00:27-INFO->> Step 421550 run_train: loss = 5.4485  (0.191 sec)
18-06-05 00:27-INFO->> Step 421560 run_train: loss = 5.4049  (0.165 sec)
18-06-05 00:27-INFO->> Step 421570 run_train: loss = 5.4022  (0.201 sec)
18-06-05 00:27-INFO->> Step 421580 run_train: loss = 5.4605  (0.140 sec)
18-06-05 00:27-INFO->> Step 421590 run_train: loss = 5.4313  (0.186 sec)
18-06-05 00:27-INFO->> Step 421600 run_train: loss = 5.4635  (0.170 sec)
18-06-05 00:27-INFO->> Step 421610 run_train: loss = 5.3735  (0.141 sec)
18-06-05 00:27-INFO->> Step 421620 run_train: loss = 5.4033  (0.150 sec)
18-06-05 00:27-INFO->> Step 421630 run_train: loss = 5.4512  (0.182 sec)
18-06-05 00:27-INFO->> Step 421640 run_train: loss = 5.3764  (0.155 sec)
18-06-05 00:27-INFO->> Step 421650 run_train: loss = 5.4007  (0.154 sec)
18-06-05 00:27-INFO->> Step 421660 run_train: loss = 5.4258  (0.153 sec)
18-06-05 00:27-INFO->> Step 421670 run_train: loss = 5.3840  (0.124 sec)
18-06-05 00:27-INFO->> Step 421680 run_train: loss = 5.3549  (0.182 sec)
18-06-05 00:27-INFO->> Step 421690 run_train: loss = 5.3715  (0.145 sec)
18-06-05 00:27-INFO->> Step 421700 run_train: loss = 5.3715  (0.164 sec)
18-06-05 00:27-INFO->> Step 421710 run_train: loss = 5.4220  (0.219 sec)
18-06-05 00:27-INFO->> Step 421720 run_train: loss = 5.4029  (0.142 sec)
18-06-05 00:27-INFO->> Step 421730 run_train: loss = 5.4266  (0.179 sec)
18-06-05 00:27-INFO->> Step 421740 run_train: loss = 5.3839  (0.163 sec)
18-06-05 00:27-INFO->> Step 421750 run_train: loss = 5.4628  (0.183 sec)
18-06-05 00:27-INFO->> Step 421760 run_train: loss = 5.3707  (0.185 sec)
18-06-05 00:27-INFO->> Step 421770 run_train: loss = 5.4866  (0.164 sec)
18-06-05 00:27-INFO->> Step 421780 run_train: loss = 5.4066  (0.147 sec)
18-06-05 00:27-INFO->> Step 421790 run_train: loss = 5.4087  (0.164 sec)
18-06-05 00:27-INFO->> Step 421800 run_train: loss = 5.4413  (0.153 sec)
18-06-05 00:27-INFO->> Step 421810 run_train: loss = 5.4106  (0.179 sec)
18-06-05 00:27-INFO->> Step 421820 run_train: loss = 5.4258  (0.136 sec)
18-06-05 00:27-INFO->> Step 421830 run_train: loss = 5.3813  (0.167 sec)
18-06-05 00:27-INFO->> Step 421840 run_train: loss = 5.3899  (0.178 sec)
18-06-05 00:27-INFO->> Step 421850 run_train: loss = 5.4071  (0.138 sec)
18-06-05 00:28-INFO->> Step 421860 run_train: loss = 5.4245  (0.160 sec)
18-06-05 00:28-INFO->> Step 421870 run_train: loss = 5.4200  (0.168 sec)
18-06-05 00:28-INFO->> Step 421880 run_train: loss = 5.4749  (0.162 sec)
18-06-05 00:28-INFO->> Step 421890 run_train: loss = 5.4391  (0.147 sec)
18-06-05 00:28-INFO->> Step 421900 run_train: loss = 5.4390  (0.134 sec)
18-06-05 00:28-INFO->> Step 421910 run_train: loss = 5.4099  (0.170 sec)
18-06-05 00:28-INFO->> Step 421920 run_train: loss = 5.4044  (0.158 sec)
18-06-05 00:28-INFO->> Step 421930 run_train: loss = 5.4285  (0.142 sec)
18-06-05 00:28-INFO->> Step 421940 run_train: loss = 5.3932  (0.157 sec)
18-06-05 00:28-INFO->> Step 421950 run_train: loss = 5.3955  (0.128 sec)
18-06-05 00:28-INFO->> Step 421960 run_train: loss = 5.4477  (0.155 sec)
18-06-05 00:28-INFO->> Step 421970 run_train: loss = 5.4022  (0.179 sec)
18-06-05 00:28-INFO->> Step 421980 run_train: loss = 5.4316  (0.169 sec)
18-06-05 00:28-INFO->> Step 421990 run_train: loss = 5.4333  (0.154 sec)
18-06-05 00:28-INFO->> Step 422000 run_train: loss = 5.3889  (0.125 sec)
18-06-05 00:28-INFO->> 2018-06-05 00:28:22.588158 Saving in ckpt
18-06-05 00:28-INFO-Test Data Eval:
18-06-05 00:29-INFO-fpr95 = 0.17468119022316683 and auc = 0.969173650766747
18-06-05 00:29-INFO->> Step 422010 run_train: loss = 5.3845  (0.153 sec)
18-06-05 00:29-INFO->> Step 422020 run_train: loss = 5.3882  (0.151 sec)
18-06-05 00:29-INFO->> Step 422030 run_train: loss = 5.4800  (0.156 sec)
18-06-05 00:29-INFO->> Step 422040 run_train: loss = 5.3963  (0.155 sec)
18-06-05 00:29-INFO->> Step 422050 run_train: loss = 5.4358  (0.139 sec)
18-06-05 00:29-INFO->> Step 422060 run_train: loss = 5.3547  (0.185 sec)
18-06-05 00:29-INFO->> Step 422070 run_train: loss = 5.3715  (0.181 sec)
18-06-05 00:29-INFO->> Step 422080 run_train: loss = 5.3977  (0.197 sec)
18-06-05 00:29-INFO->> Step 422090 run_train: loss = 5.4374  (0.156 sec)
18-06-05 00:29-INFO->> Step 422100 run_train: loss = 5.4350  (0.144 sec)
18-06-05 00:29-INFO->> Step 422110 run_train: loss = 5.4025  (0.124 sec)
18-06-05 00:29-INFO->> Step 422120 run_train: loss = 5.4280  (0.194 sec)
18-06-05 00:29-INFO->> Step 422130 run_train: loss = 5.3958  (0.162 sec)
18-06-05 00:29-INFO->> Step 422140 run_train: loss = 5.3751  (0.158 sec)
18-06-05 00:29-INFO->> Step 422150 run_train: loss = 5.4291  (0.166 sec)
18-06-05 00:29-INFO->> Step 422160 run_train: loss = 5.3435  (0.161 sec)
18-06-05 00:29-INFO->> Step 422170 run_train: loss = 5.4069  (0.155 sec)
18-06-05 00:29-INFO->> Step 422180 run_train: loss = 5.4493  (0.159 sec)
18-06-05 00:29-INFO->> Step 422190 run_train: loss = 5.4194  (0.198 sec)
18-06-05 00:29-INFO->> Step 422200 run_train: loss = 5.3766  (0.163 sec)
18-06-05 00:29-INFO->> Step 422210 run_train: loss = 5.3583  (0.182 sec)
18-06-05 00:29-INFO->> Step 422220 run_train: loss = 5.3821  (0.187 sec)
18-06-05 00:29-INFO->> Step 422230 run_train: loss = 5.4661  (0.153 sec)
18-06-05 00:29-INFO->> Step 422240 run_train: loss = 5.4234  (0.196 sec)
18-06-05 00:29-INFO->> Step 422250 run_train: loss = 5.4628  (0.132 sec)
18-06-05 00:29-INFO->> Step 422260 run_train: loss = 5.4034  (0.124 sec)
18-06-05 00:29-INFO->> Step 422270 run_train: loss = 5.4281  (0.174 sec)
18-06-05 00:29-INFO->> Step 422280 run_train: loss = 5.4917  (0.159 sec)
18-06-05 00:29-INFO->> Step 422290 run_train: loss = 5.4830  (0.152 sec)
18-06-05 00:29-INFO->> Step 422300 run_train: loss = 5.4193  (0.163 sec)
18-06-05 00:29-INFO->> Step 422310 run_train: loss = 5.4360  (0.159 sec)
18-06-05 00:29-INFO->> Step 422320 run_train: loss = 5.4241  (0.157 sec)
18-06-05 00:29-INFO->> Step 422330 run_train: loss = 5.4190  (0.166 sec)
18-06-05 00:29-INFO->> Step 422340 run_train: loss = 5.4678  (0.164 sec)
18-06-05 00:29-INFO->> Step 422350 run_train: loss = 5.4155  (0.161 sec)
18-06-05 00:30-INFO->> Step 422360 run_train: loss = 5.3527  (0.170 sec)
18-06-05 00:30-INFO->> Step 422370 run_train: loss = 5.4131  (0.139 sec)
18-06-05 00:30-INFO->> Step 422380 run_train: loss = 5.4303  (0.161 sec)
18-06-05 00:30-INFO->> Step 422390 run_train: loss = 5.3731  (0.153 sec)
18-06-05 00:30-INFO->> Step 422400 run_train: loss = 5.4293  (0.141 sec)
18-06-05 00:30-INFO->> Step 422410 run_train: loss = 5.4937  (0.181 sec)
18-06-05 00:30-INFO->> Step 422420 run_train: loss = 5.4135  (0.141 sec)
18-06-05 00:30-INFO->> Step 422430 run_train: loss = 5.4322  (0.122 sec)
18-06-05 00:30-INFO->> Step 422440 run_train: loss = 5.3966  (0.153 sec)
18-06-05 00:30-INFO->> Step 422450 run_train: loss = 5.4300  (0.113 sec)
18-06-05 00:30-INFO->> Step 422460 run_train: loss = 5.4148  (0.177 sec)
18-06-05 00:30-INFO->> Step 422470 run_train: loss = 5.3912  (0.169 sec)
18-06-05 00:30-INFO->> Step 422480 run_train: loss = 5.4446  (0.147 sec)
18-06-05 00:30-INFO->> Step 422490 run_train: loss = 5.3823  (0.175 sec)
18-06-05 00:30-INFO->> Step 422500 run_train: loss = 5.4115  (0.167 sec)
18-06-05 00:30-INFO->> Step 422510 run_train: loss = 5.4040  (0.169 sec)
18-06-05 00:30-INFO->> Step 422520 run_train: loss = 5.3997  (0.146 sec)
18-06-05 00:30-INFO->> Step 422530 run_train: loss = 5.4198  (0.188 sec)
18-06-05 00:30-INFO->> Step 422540 run_train: loss = 5.3038  (0.157 sec)
18-06-05 00:30-INFO->> Step 422550 run_train: loss = 5.4130  (0.155 sec)
18-06-05 00:30-INFO->> Step 422560 run_train: loss = 5.3903  (0.216 sec)
18-06-05 00:30-INFO->> Step 422570 run_train: loss = 5.3684  (0.111 sec)
18-06-05 00:30-INFO->> Step 422580 run_train: loss = 5.4037  (0.151 sec)
18-06-05 00:30-INFO->> Step 422590 run_train: loss = 5.4926  (0.148 sec)
18-06-05 00:30-INFO->> Step 422600 run_train: loss = 5.4235  (0.125 sec)
18-06-05 00:30-INFO->> Step 422610 run_train: loss = 5.4481  (0.132 sec)
18-06-05 00:30-INFO->> Step 422620 run_train: loss = 5.3830  (0.176 sec)
18-06-05 00:30-INFO->> Step 422630 run_train: loss = 5.4293  (0.163 sec)
18-06-05 00:30-INFO->> Step 422640 run_train: loss = 5.4469  (0.171 sec)
18-06-05 00:30-INFO->> Step 422650 run_train: loss = 5.4120  (0.188 sec)
18-06-05 00:30-INFO->> Step 422660 run_train: loss = 5.3969  (0.165 sec)
18-06-05 00:30-INFO->> Step 422670 run_train: loss = 5.3834  (0.159 sec)
18-06-05 00:30-INFO->> Step 422680 run_train: loss = 5.4402  (0.152 sec)
18-06-05 00:30-INFO->> Step 422690 run_train: loss = 5.4705  (0.144 sec)
18-06-05 00:30-INFO->> Step 422700 run_train: loss = 5.3386  (0.157 sec)
18-06-05 00:30-INFO->> Step 422710 run_train: loss = 5.4314  (0.154 sec)
18-06-05 00:30-INFO->> Step 422720 run_train: loss = 5.4111  (0.137 sec)
18-06-05 00:30-INFO->> Step 422730 run_train: loss = 5.4980  (0.159 sec)
18-06-05 00:31-INFO->> Step 422740 run_train: loss = 5.4420  (0.183 sec)
18-06-05 00:31-INFO->> Step 422750 run_train: loss = 5.4225  (0.155 sec)
18-06-05 00:31-INFO->> Step 422760 run_train: loss = 5.4087  (0.169 sec)
18-06-05 00:31-INFO->> Step 422770 run_train: loss = 5.3639  (0.156 sec)
18-06-05 00:31-INFO->> Step 422780 run_train: loss = 5.4442  (0.139 sec)
18-06-05 00:31-INFO->> Step 422790 run_train: loss = 5.4060  (0.152 sec)
18-06-05 00:31-INFO->> Step 422800 run_train: loss = 5.3911  (0.157 sec)
18-06-05 00:31-INFO->> Step 422810 run_train: loss = 5.3789  (0.170 sec)
18-06-05 00:31-INFO->> Step 422820 run_train: loss = 5.4039  (0.174 sec)
18-06-05 00:31-INFO->> Step 422830 run_train: loss = 5.5100  (0.156 sec)
18-06-05 00:31-INFO->> Step 422840 run_train: loss = 5.3428  (0.167 sec)
18-06-05 00:31-INFO->> Step 422850 run_train: loss = 5.4083  (0.161 sec)
18-06-05 00:31-INFO->> Step 422860 run_train: loss = 5.4190  (0.163 sec)
18-06-05 00:31-INFO->> Step 422870 run_train: loss = 5.4394  (0.149 sec)
18-06-05 00:31-INFO->> Step 422880 run_train: loss = 5.2852  (0.156 sec)
18-06-05 00:31-INFO->> Step 422890 run_train: loss = 5.4110  (0.179 sec)
18-06-05 00:31-INFO->> Step 422900 run_train: loss = 5.4064  (0.173 sec)
18-06-05 00:31-INFO->> Step 422910 run_train: loss = 5.4533  (0.143 sec)
18-06-05 00:31-INFO->> Step 422920 run_train: loss = 5.3775  (0.156 sec)
18-06-05 00:31-INFO->> Step 422930 run_train: loss = 5.4406  (0.171 sec)
18-06-05 00:31-INFO->> Step 422940 run_train: loss = 5.3509  (0.162 sec)
18-06-05 00:31-INFO->> Step 422950 run_train: loss = 5.3465  (0.155 sec)
18-06-05 00:31-INFO->> Step 422960 run_train: loss = 5.4655  (0.171 sec)
18-06-05 00:31-INFO->> Step 422970 run_train: loss = 5.4547  (0.181 sec)
18-06-05 00:31-INFO->> Step 422980 run_train: loss = 5.3904  (0.173 sec)
18-06-05 00:31-INFO->> Step 422990 run_train: loss = 5.4195  (0.178 sec)
18-06-05 00:31-INFO->> Step 423000 run_train: loss = 5.4497  (0.154 sec)
18-06-05 00:31-INFO->> 2018-06-05 00:31:41.968197 Saving in ckpt
18-06-05 00:31-INFO-Test Data Eval:
18-06-05 00:32-INFO-fpr95 = 0.17164253453772582 and auc = 0.9693196783573813
18-06-05 00:32-INFO->> Step 423010 run_train: loss = 5.4228  (0.195 sec)
18-06-05 00:32-INFO->> Step 423020 run_train: loss = 5.4738  (0.142 sec)
18-06-05 00:32-INFO->> Step 423030 run_train: loss = 5.3721  (0.196 sec)
18-06-05 00:32-INFO->> Step 423040 run_train: loss = 5.4272  (0.193 sec)
18-06-05 00:32-INFO->> Step 423050 run_train: loss = 5.4283  (0.183 sec)
18-06-05 00:32-INFO->> Step 423060 run_train: loss = 5.4179  (0.153 sec)
18-06-05 00:32-INFO->> Step 423070 run_train: loss = 5.4342  (0.142 sec)
18-06-05 00:32-INFO->> Step 423080 run_train: loss = 5.3789  (0.137 sec)
18-06-05 00:32-INFO->> Step 423090 run_train: loss = 5.4055  (0.164 sec)
18-06-05 00:32-INFO->> Step 423100 run_train: loss = 5.4516  (0.166 sec)
18-06-05 00:32-INFO->> Step 423110 run_train: loss = 5.4190  (0.162 sec)
18-06-05 00:32-INFO->> Step 423120 run_train: loss = 5.3972  (0.195 sec)
18-06-05 00:32-INFO->> Step 423130 run_train: loss = 5.4499  (0.156 sec)
18-06-05 00:32-INFO->> Step 423140 run_train: loss = 5.4329  (0.138 sec)
18-06-05 00:32-INFO->> Step 423150 run_train: loss = 5.3421  (0.168 sec)
18-06-05 00:32-INFO->> Step 423160 run_train: loss = 5.4428  (0.151 sec)
18-06-05 00:32-INFO->> Step 423170 run_train: loss = 5.4498  (0.127 sec)
18-06-05 00:32-INFO->> Step 423180 run_train: loss = 5.4270  (0.125 sec)
18-06-05 00:32-INFO->> Step 423190 run_train: loss = 5.4014  (0.174 sec)
18-06-05 00:32-INFO->> Step 423200 run_train: loss = 5.3809  (0.175 sec)
18-06-05 00:32-INFO->> Step 423210 run_train: loss = 5.4124  (0.167 sec)
18-06-05 00:32-INFO->> Step 423220 run_train: loss = 5.3519  (0.160 sec)
18-06-05 00:32-INFO->> Step 423230 run_train: loss = 5.4431  (0.133 sec)
18-06-05 00:32-INFO->> Step 423240 run_train: loss = 5.4039  (0.160 sec)
18-06-05 00:33-INFO->> Step 423250 run_train: loss = 5.3728  (0.160 sec)
18-06-05 00:33-INFO->> Step 423260 run_train: loss = 5.4107  (0.192 sec)
18-06-05 00:33-INFO->> Step 423270 run_train: loss = 5.3944  (0.123 sec)
18-06-05 00:33-INFO->> Step 423280 run_train: loss = 5.4564  (0.183 sec)
18-06-05 00:33-INFO->> Step 423290 run_train: loss = 5.4025  (0.159 sec)
18-06-05 00:33-INFO->> Step 423300 run_train: loss = 5.3839  (0.155 sec)
18-06-05 00:33-INFO->> Step 423310 run_train: loss = 5.4399  (0.138 sec)
18-06-05 00:33-INFO->> Step 423320 run_train: loss = 5.4221  (0.161 sec)
18-06-05 00:33-INFO->> Step 423330 run_train: loss = 5.4166  (0.172 sec)
18-06-05 00:33-INFO->> Step 423340 run_train: loss = 5.3543  (0.116 sec)
18-06-05 00:33-INFO->> Step 423350 run_train: loss = 5.4343  (0.152 sec)
18-06-05 00:33-INFO->> Step 423360 run_train: loss = 5.4064  (0.185 sec)
18-06-05 00:33-INFO->> Step 423370 run_train: loss = 5.4208  (0.181 sec)
18-06-05 00:33-INFO->> Step 423380 run_train: loss = 5.4288  (0.107 sec)
18-06-05 00:33-INFO->> Step 423390 run_train: loss = 5.4320  (0.166 sec)
18-06-05 00:33-INFO->> Step 423400 run_train: loss = 5.3975  (0.175 sec)
18-06-05 00:33-INFO->> Step 423410 run_train: loss = 5.4521  (0.168 sec)
18-06-05 00:33-INFO->> Step 423420 run_train: loss = 5.4914  (0.135 sec)
18-06-05 00:33-INFO->> Step 423430 run_train: loss = 5.4473  (0.152 sec)
18-06-05 00:33-INFO->> Step 423440 run_train: loss = 5.4689  (0.163 sec)
18-06-05 00:33-INFO->> Step 423450 run_train: loss = 5.4086  (0.166 sec)
18-06-05 00:33-INFO->> Step 423460 run_train: loss = 5.4206  (0.149 sec)
18-06-05 00:33-INFO->> Step 423470 run_train: loss = 5.4364  (0.125 sec)
18-06-05 00:33-INFO->> Step 423480 run_train: loss = 5.4621  (0.166 sec)
18-06-05 00:33-INFO->> Step 423490 run_train: loss = 5.4551  (0.161 sec)
18-06-05 00:33-INFO->> Step 423500 run_train: loss = 5.4457  (0.157 sec)
18-06-05 00:33-INFO->> Step 423510 run_train: loss = 5.4593  (0.181 sec)
18-06-05 00:33-INFO->> Step 423520 run_train: loss = 5.5065  (0.144 sec)
18-06-05 00:33-INFO->> Step 423530 run_train: loss = 5.4037  (0.176 sec)
18-06-05 00:33-INFO->> Step 423540 run_train: loss = 5.3666  (0.143 sec)
18-06-05 00:33-INFO->> Step 423550 run_train: loss = 5.3935  (0.168 sec)
18-06-05 00:33-INFO->> Step 423560 run_train: loss = 5.4581  (0.159 sec)
18-06-05 00:33-INFO->> Step 423570 run_train: loss = 5.4152  (0.147 sec)
18-06-05 00:33-INFO->> Step 423580 run_train: loss = 5.3930  (0.149 sec)
18-06-05 00:33-INFO->> Step 423590 run_train: loss = 5.3279  (0.148 sec)
18-06-05 00:33-INFO->> Step 423600 run_train: loss = 5.3666  (0.141 sec)
18-06-05 00:33-INFO->> Step 423610 run_train: loss = 5.3894  (0.179 sec)
18-06-05 00:34-INFO->> Step 423620 run_train: loss = 5.4026  (0.158 sec)
18-06-05 00:34-INFO->> Step 423630 run_train: loss = 5.4092  (0.192 sec)
18-06-05 00:34-INFO->> Step 423640 run_train: loss = 5.4473  (0.143 sec)
18-06-05 00:34-INFO->> Step 423650 run_train: loss = 5.4531  (0.159 sec)
18-06-05 00:34-INFO->> Step 423660 run_train: loss = 5.4990  (0.145 sec)
18-06-05 00:34-INFO->> Step 423670 run_train: loss = 5.4298  (0.180 sec)
18-06-05 00:34-INFO->> Step 423680 run_train: loss = 5.3877  (0.116 sec)
18-06-05 00:34-INFO->> Step 423690 run_train: loss = 5.4219  (0.180 sec)
18-06-05 00:34-INFO->> Step 423700 run_train: loss = 5.4354  (0.136 sec)
18-06-05 00:34-INFO->> Step 423710 run_train: loss = 5.4430  (0.165 sec)
18-06-05 00:34-INFO->> Step 423720 run_train: loss = 5.3868  (0.136 sec)
18-06-05 00:34-INFO->> Step 423730 run_train: loss = 5.4544  (0.135 sec)
18-06-05 00:34-INFO->> Step 423740 run_train: loss = 5.3953  (0.166 sec)
18-06-05 00:34-INFO->> Step 423750 run_train: loss = 5.4248  (0.119 sec)
18-06-05 00:34-INFO->> Step 423760 run_train: loss = 5.4287  (0.154 sec)
18-06-05 00:34-INFO->> Step 423770 run_train: loss = 5.4198  (0.162 sec)
18-06-05 00:34-INFO->> Step 423780 run_train: loss = 5.3679  (0.182 sec)
18-06-05 00:34-INFO->> Step 423790 run_train: loss = 5.4617  (0.129 sec)
18-06-05 00:34-INFO->> Step 423800 run_train: loss = 5.3969  (0.183 sec)
18-06-05 00:34-INFO->> Step 423810 run_train: loss = 5.4700  (0.185 sec)
18-06-05 00:34-INFO->> Step 423820 run_train: loss = 5.4207  (0.156 sec)
18-06-05 00:34-INFO->> Step 423830 run_train: loss = 5.3830  (0.161 sec)
18-06-05 00:34-INFO->> Step 423840 run_train: loss = 5.3676  (0.146 sec)
18-06-05 00:34-INFO->> Step 423850 run_train: loss = 5.4721  (0.147 sec)
18-06-05 00:34-INFO->> Step 423860 run_train: loss = 5.4304  (0.158 sec)
18-06-05 00:34-INFO->> Step 423870 run_train: loss = 5.3865  (0.175 sec)
18-06-05 00:34-INFO->> Step 423880 run_train: loss = 5.4209  (0.143 sec)
18-06-05 00:34-INFO->> Step 423890 run_train: loss = 5.3660  (0.173 sec)
18-06-05 00:34-INFO->> Step 423900 run_train: loss = 5.4201  (0.166 sec)
18-06-05 00:34-INFO->> Step 423910 run_train: loss = 5.3824  (0.158 sec)
18-06-05 00:34-INFO->> Step 423920 run_train: loss = 5.4138  (0.113 sec)
18-06-05 00:34-INFO->> Step 423930 run_train: loss = 5.4268  (0.167 sec)
18-06-05 00:34-INFO->> Step 423940 run_train: loss = 5.3818  (0.140 sec)
18-06-05 00:34-INFO->> Step 423950 run_train: loss = 5.3831  (0.145 sec)
18-06-05 00:34-INFO->> Step 423960 run_train: loss = 5.3683  (0.180 sec)
18-06-05 00:34-INFO->> Step 423970 run_train: loss = 5.4771  (0.171 sec)
18-06-05 00:34-INFO->> Step 423980 run_train: loss = 5.3821  (0.152 sec)
18-06-05 00:34-INFO->> Step 423990 run_train: loss = 5.4434  (0.180 sec)
18-06-05 00:35-INFO->> Step 424000 run_train: loss = 5.4058  (0.178 sec)
18-06-05 00:35-INFO->> 2018-06-05 00:35:01.244139 Saving in ckpt
18-06-05 00:35-INFO-Test Data Eval:
18-06-05 00:35-INFO-fpr95 = 0.17461477151965993 and auc = 0.968956993059852
18-06-05 00:35-INFO->> Step 424010 run_train: loss = 5.4780  (0.145 sec)
18-06-05 00:35-INFO->> Step 424020 run_train: loss = 5.3893  (0.131 sec)
18-06-05 00:35-INFO->> Step 424030 run_train: loss = 5.4418  (0.129 sec)
18-06-05 00:35-INFO->> Step 424040 run_train: loss = 5.4111  (0.144 sec)
18-06-05 00:35-INFO->> Step 424050 run_train: loss = 5.4232  (0.159 sec)
18-06-05 00:35-INFO->> Step 424060 run_train: loss = 5.4342  (0.159 sec)
18-06-05 00:35-INFO->> Step 424070 run_train: loss = 5.4094  (0.160 sec)
18-06-05 00:35-INFO->> Step 424080 run_train: loss = 5.4194  (0.158 sec)
18-06-05 00:35-INFO->> Step 424090 run_train: loss = 5.3867  (0.161 sec)
18-06-05 00:35-INFO->> Step 424100 run_train: loss = 5.4434  (0.153 sec)
18-06-05 00:35-INFO->> Step 424110 run_train: loss = 5.3914  (0.136 sec)
18-06-05 00:36-INFO->> Step 424120 run_train: loss = 5.4535  (0.168 sec)
18-06-05 00:36-INFO->> Step 424130 run_train: loss = 5.4074  (0.141 sec)
18-06-05 00:36-INFO->> Step 424140 run_train: loss = 5.4016  (0.165 sec)
18-06-05 00:36-INFO->> Step 424150 run_train: loss = 5.4500  (0.143 sec)
18-06-05 00:36-INFO->> Step 424160 run_train: loss = 5.4356  (0.124 sec)
18-06-05 00:36-INFO->> Step 424170 run_train: loss = 5.3728  (0.138 sec)
18-06-05 00:36-INFO->> Step 424180 run_train: loss = 5.3547  (0.163 sec)
18-06-05 00:36-INFO->> Step 424190 run_train: loss = 5.3914  (0.157 sec)
18-06-05 00:36-INFO->> Step 424200 run_train: loss = 5.4234  (0.174 sec)
18-06-05 00:36-INFO->> Step 424210 run_train: loss = 5.4029  (0.126 sec)
18-06-05 00:36-INFO->> Step 424220 run_train: loss = 5.4298  (0.151 sec)
18-06-05 00:36-INFO->> Step 424230 run_train: loss = 5.3716  (0.172 sec)
18-06-05 00:36-INFO->> Step 424240 run_train: loss = 5.4374  (0.145 sec)
18-06-05 00:36-INFO->> Step 424250 run_train: loss = 5.4515  (0.175 sec)
18-06-05 00:36-INFO->> Step 424260 run_train: loss = 5.3427  (0.160 sec)
18-06-05 00:36-INFO->> Step 424270 run_train: loss = 5.3545  (0.147 sec)
18-06-05 00:36-INFO->> Step 424280 run_train: loss = 5.3913  (0.140 sec)
18-06-05 00:36-INFO->> Step 424290 run_train: loss = 5.4139  (0.205 sec)
18-06-05 00:36-INFO->> Step 424300 run_train: loss = 5.4058  (0.156 sec)
18-06-05 00:36-INFO->> Step 424310 run_train: loss = 5.3860  (0.163 sec)
18-06-05 00:36-INFO->> Step 424320 run_train: loss = 5.4052  (0.144 sec)
18-06-05 00:36-INFO->> Step 424330 run_train: loss = 5.4137  (0.127 sec)
18-06-05 00:36-INFO->> Step 424340 run_train: loss = 5.4501  (0.127 sec)
18-06-05 00:36-INFO->> Step 424350 run_train: loss = 5.4103  (0.140 sec)
18-06-05 00:36-INFO->> Step 424360 run_train: loss = 5.4491  (0.158 sec)
18-06-05 00:36-INFO->> Step 424370 run_train: loss = 5.4249  (0.144 sec)
18-06-05 00:36-INFO->> Step 424380 run_train: loss = 5.4551  (0.163 sec)
18-06-05 00:36-INFO->> Step 424390 run_train: loss = 5.4020  (0.180 sec)
18-06-05 00:36-INFO->> Step 424400 run_train: loss = 5.4031  (0.138 sec)
18-06-05 00:36-INFO->> Step 424410 run_train: loss = 5.4775  (0.130 sec)
18-06-05 00:36-INFO->> Step 424420 run_train: loss = 5.4011  (0.150 sec)
18-06-05 00:36-INFO->> Step 424430 run_train: loss = 5.3878  (0.153 sec)
18-06-05 00:36-INFO->> Step 424440 run_train: loss = 5.4333  (0.164 sec)
18-06-05 00:36-INFO->> Step 424450 run_train: loss = 5.4641  (0.144 sec)
18-06-05 00:36-INFO->> Step 424460 run_train: loss = 5.4401  (0.158 sec)
18-06-05 00:36-INFO->> Step 424470 run_train: loss = 5.3865  (0.176 sec)
18-06-05 00:36-INFO->> Step 424480 run_train: loss = 5.3902  (0.161 sec)
18-06-05 00:36-INFO->> Step 424490 run_train: loss = 5.4296  (0.182 sec)
18-06-05 00:37-INFO->> Step 424500 run_train: loss = 5.3777  (0.190 sec)
18-06-05 00:37-INFO->> Step 424510 run_train: loss = 5.4126  (0.152 sec)
18-06-05 00:37-INFO->> Step 424520 run_train: loss = 5.3314  (0.146 sec)
18-06-05 00:37-INFO->> Step 424530 run_train: loss = 5.3986  (0.145 sec)
18-06-05 00:37-INFO->> Step 424540 run_train: loss = 5.4340  (0.148 sec)
18-06-05 00:37-INFO->> Step 424550 run_train: loss = 5.3902  (0.133 sec)
18-06-05 00:37-INFO->> Step 424560 run_train: loss = 5.4746  (0.149 sec)
18-06-05 00:37-INFO->> Step 424570 run_train: loss = 5.3645  (0.129 sec)
18-06-05 00:37-INFO->> Step 424580 run_train: loss = 5.4417  (0.159 sec)
18-06-05 00:37-INFO->> Step 424590 run_train: loss = 5.4189  (0.181 sec)
18-06-05 00:37-INFO->> Step 424600 run_train: loss = 5.4641  (0.132 sec)
18-06-05 00:37-INFO->> Step 424610 run_train: loss = 5.4028  (0.164 sec)
18-06-05 00:37-INFO->> Step 424620 run_train: loss = 5.4450  (0.157 sec)
18-06-05 00:37-INFO->> Step 424630 run_train: loss = 5.4334  (0.142 sec)
18-06-05 00:37-INFO->> Step 424640 run_train: loss = 5.4091  (0.129 sec)
18-06-05 00:37-INFO->> Step 424650 run_train: loss = 5.4141  (0.139 sec)
18-06-05 00:37-INFO->> Step 424660 run_train: loss = 5.3770  (0.184 sec)
18-06-05 00:37-INFO->> Step 424670 run_train: loss = 5.4099  (0.137 sec)
18-06-05 00:37-INFO->> Step 424680 run_train: loss = 5.3522  (0.164 sec)
18-06-05 00:37-INFO->> Step 424690 run_train: loss = 5.4970  (0.142 sec)
18-06-05 00:37-INFO->> Step 424700 run_train: loss = 5.4353  (0.171 sec)
18-06-05 00:37-INFO->> Step 424710 run_train: loss = 5.3777  (0.201 sec)
18-06-05 00:37-INFO->> Step 424720 run_train: loss = 5.4626  (0.146 sec)
18-06-05 00:37-INFO->> Step 424730 run_train: loss = 5.3921  (0.183 sec)
18-06-05 00:37-INFO->> Step 424740 run_train: loss = 5.3726  (0.129 sec)
18-06-05 00:37-INFO->> Step 424750 run_train: loss = 5.4211  (0.156 sec)
18-06-05 00:37-INFO->> Step 424760 run_train: loss = 5.3855  (0.135 sec)
18-06-05 00:37-INFO->> Step 424770 run_train: loss = 5.4638  (0.147 sec)
18-06-05 00:37-INFO->> Step 424780 run_train: loss = 5.3998  (0.181 sec)
18-06-05 00:37-INFO->> Step 424790 run_train: loss = 5.4630  (0.162 sec)
18-06-05 00:37-INFO->> Step 424800 run_train: loss = 5.3616  (0.177 sec)
18-06-05 00:37-INFO->> Step 424810 run_train: loss = 5.4172  (0.167 sec)
18-06-05 00:37-INFO->> Step 424820 run_train: loss = 5.4153  (0.154 sec)
18-06-05 00:37-INFO->> Step 424830 run_train: loss = 5.4020  (0.162 sec)
18-06-05 00:37-INFO->> Step 424840 run_train: loss = 5.4902  (0.155 sec)
18-06-05 00:37-INFO->> Step 424850 run_train: loss = 5.3640  (0.151 sec)
18-06-05 00:37-INFO->> Step 424860 run_train: loss = 5.4269  (0.151 sec)
18-06-05 00:37-INFO->> Step 424870 run_train: loss = 5.4187  (0.159 sec)
18-06-05 00:38-INFO->> Step 424880 run_train: loss = 5.4238  (0.139 sec)
18-06-05 00:38-INFO->> Step 424890 run_train: loss = 5.5062  (0.136 sec)
18-06-05 00:38-INFO->> Step 424900 run_train: loss = 5.3843  (0.149 sec)
18-06-05 00:38-INFO->> Step 424910 run_train: loss = 5.3884  (0.138 sec)
18-06-05 00:38-INFO->> Step 424920 run_train: loss = 5.4264  (0.140 sec)
18-06-05 00:38-INFO->> Step 424930 run_train: loss = 5.4544  (0.158 sec)
18-06-05 00:38-INFO->> Step 424940 run_train: loss = 5.4166  (0.159 sec)
18-06-05 00:38-INFO->> Step 424950 run_train: loss = 5.4378  (0.170 sec)
18-06-05 00:38-INFO->> Step 424960 run_train: loss = 5.3985  (0.159 sec)
18-06-05 00:38-INFO->> Step 424970 run_train: loss = 5.3980  (0.154 sec)
18-06-05 00:38-INFO->> Step 424980 run_train: loss = 5.4253  (0.143 sec)
18-06-05 00:38-INFO->> Step 424990 run_train: loss = 5.4041  (0.154 sec)
18-06-05 00:38-INFO->> Step 425000 run_train: loss = 5.4877  (0.205 sec)
18-06-05 00:38-INFO->> 2018-06-05 00:38:19.998359 Saving in ckpt
18-06-05 00:38-INFO-Test Data Eval:
18-06-05 00:39-INFO-fpr95 = 0.17105306854410202 and auc = 0.9693847498471941
18-06-05 00:39-INFO->> Step 425010 run_train: loss = 5.4683  (0.146 sec)
18-06-05 00:39-INFO->> Step 425020 run_train: loss = 5.4335  (0.142 sec)
18-06-05 00:39-INFO->> Step 425030 run_train: loss = 5.3844  (0.160 sec)
18-06-05 00:39-INFO->> Step 425040 run_train: loss = 5.4006  (0.194 sec)
18-06-05 00:39-INFO->> Step 425050 run_train: loss = 5.3789  (0.155 sec)
18-06-05 00:39-INFO->> Step 425060 run_train: loss = 5.3347  (0.147 sec)
18-06-05 00:39-INFO->> Step 425070 run_train: loss = 5.3849  (0.178 sec)
18-06-05 00:39-INFO->> Step 425080 run_train: loss = 5.4416  (0.155 sec)
18-06-05 00:39-INFO->> Step 425090 run_train: loss = 5.4399  (0.166 sec)
18-06-05 00:39-INFO->> Step 425100 run_train: loss = 5.4211  (0.168 sec)
18-06-05 00:39-INFO->> Step 425110 run_train: loss = 5.3647  (0.156 sec)
18-06-05 00:39-INFO->> Step 425120 run_train: loss = 5.4053  (0.168 sec)
18-06-05 00:39-INFO->> Step 425130 run_train: loss = 5.3735  (0.169 sec)
18-06-05 00:39-INFO->> Step 425140 run_train: loss = 5.4099  (0.151 sec)
18-06-05 00:39-INFO->> Step 425150 run_train: loss = 5.3917  (0.132 sec)
18-06-05 00:39-INFO->> Step 425160 run_train: loss = 5.4056  (0.172 sec)
18-06-05 00:39-INFO->> Step 425170 run_train: loss = 5.4384  (0.164 sec)
18-06-05 00:39-INFO->> Step 425180 run_train: loss = 5.4034  (0.142 sec)
18-06-05 00:39-INFO->> Step 425190 run_train: loss = 5.4392  (0.152 sec)
18-06-05 00:39-INFO->> Step 425200 run_train: loss = 5.4156  (0.145 sec)
18-06-05 00:39-INFO->> Step 425210 run_train: loss = 5.4564  (0.206 sec)
18-06-05 00:39-INFO->> Step 425220 run_train: loss = 5.3579  (0.175 sec)
18-06-05 00:39-INFO->> Step 425230 run_train: loss = 5.4009  (0.131 sec)
18-06-05 00:39-INFO->> Step 425240 run_train: loss = 5.3933  (0.149 sec)
18-06-05 00:39-INFO->> Step 425250 run_train: loss = 5.3910  (0.137 sec)
18-06-05 00:39-INFO->> Step 425260 run_train: loss = 5.4415  (0.156 sec)
18-06-05 00:39-INFO->> Step 425270 run_train: loss = 5.3943  (0.154 sec)
18-06-05 00:39-INFO->> Step 425280 run_train: loss = 5.4972  (0.173 sec)
18-06-05 00:39-INFO->> Step 425290 run_train: loss = 5.3887  (0.173 sec)
18-06-05 00:39-INFO->> Step 425300 run_train: loss = 5.3908  (0.151 sec)
18-06-05 00:39-INFO->> Step 425310 run_train: loss = 5.3529  (0.152 sec)
18-06-05 00:39-INFO->> Step 425320 run_train: loss = 5.4126  (0.181 sec)
18-06-05 00:39-INFO->> Step 425330 run_train: loss = 5.4100  (0.141 sec)
18-06-05 00:39-INFO->> Step 425340 run_train: loss = 5.4028  (0.164 sec)
18-06-05 00:39-INFO->> Step 425350 run_train: loss = 5.3816  (0.156 sec)
18-06-05 00:39-INFO->> Step 425360 run_train: loss = 5.3974  (0.206 sec)
18-06-05 00:39-INFO->> Step 425370 run_train: loss = 5.3892  (0.152 sec)
18-06-05 00:40-INFO->> Step 425380 run_train: loss = 5.4097  (0.139 sec)
18-06-05 00:40-INFO->> Step 425390 run_train: loss = 5.4971  (0.176 sec)
18-06-05 00:40-INFO->> Step 425400 run_train: loss = 5.4140  (0.158 sec)
18-06-05 00:40-INFO->> Step 425410 run_train: loss = 5.4113  (0.166 sec)
18-06-05 00:40-INFO->> Step 425420 run_train: loss = 5.3780  (0.125 sec)
18-06-05 00:40-INFO->> Step 425430 run_train: loss = 5.4721  (0.183 sec)
18-06-05 00:40-INFO->> Step 425440 run_train: loss = 5.4797  (0.163 sec)
18-06-05 00:40-INFO->> Step 425450 run_train: loss = 5.3996  (0.130 sec)
18-06-05 00:40-INFO->> Step 425460 run_train: loss = 5.3918  (0.196 sec)
18-06-05 00:40-INFO->> Step 425470 run_train: loss = 5.4106  (0.197 sec)
18-06-05 00:40-INFO->> Step 425480 run_train: loss = 5.4407  (0.162 sec)
18-06-05 00:40-INFO->> Step 425490 run_train: loss = 5.4029  (0.122 sec)
18-06-05 00:40-INFO->> Step 425500 run_train: loss = 5.3952  (0.159 sec)
18-06-05 00:40-INFO->> Step 425510 run_train: loss = 5.3677  (0.186 sec)
18-06-05 00:40-INFO->> Step 425520 run_train: loss = 5.4521  (0.173 sec)
18-06-05 00:40-INFO->> Step 425530 run_train: loss = 5.4008  (0.155 sec)
18-06-05 00:40-INFO->> Step 425540 run_train: loss = 5.4113  (0.187 sec)
18-06-05 00:40-INFO->> Step 425550 run_train: loss = 5.4180  (0.170 sec)
18-06-05 00:40-INFO->> Step 425560 run_train: loss = 5.4876  (0.168 sec)
18-06-05 00:40-INFO->> Step 425570 run_train: loss = 5.4823  (0.152 sec)
18-06-05 00:40-INFO->> Step 425580 run_train: loss = 5.4650  (0.187 sec)
18-06-05 00:40-INFO->> Step 425590 run_train: loss = 5.4063  (0.146 sec)
18-06-05 00:40-INFO->> Step 425600 run_train: loss = 5.4199  (0.172 sec)
18-06-05 00:40-INFO->> Step 425610 run_train: loss = 5.4220  (0.205 sec)
18-06-05 00:40-INFO->> Step 425620 run_train: loss = 5.4437  (0.136 sec)
18-06-05 00:40-INFO->> Step 425630 run_train: loss = 5.3517  (0.148 sec)
18-06-05 00:40-INFO->> Step 425640 run_train: loss = 5.3837  (0.144 sec)
18-06-05 00:40-INFO->> Step 425650 run_train: loss = 5.3886  (0.164 sec)
18-06-05 00:40-INFO->> Step 425660 run_train: loss = 5.3974  (0.147 sec)
18-06-05 00:40-INFO->> Step 425670 run_train: loss = 5.4430  (0.137 sec)
18-06-05 00:40-INFO->> Step 425680 run_train: loss = 5.4449  (0.157 sec)
18-06-05 00:40-INFO->> Step 425690 run_train: loss = 5.3903  (0.155 sec)
18-06-05 00:40-INFO->> Step 425700 run_train: loss = 5.4300  (0.162 sec)
18-06-05 00:40-INFO->> Step 425710 run_train: loss = 5.4345  (0.147 sec)
18-06-05 00:40-INFO->> Step 425720 run_train: loss = 5.3530  (0.172 sec)
18-06-05 00:40-INFO->> Step 425730 run_train: loss = 5.4501  (0.135 sec)
18-06-05 00:40-INFO->> Step 425740 run_train: loss = 5.4049  (0.171 sec)
18-06-05 00:40-INFO->> Step 425750 run_train: loss = 5.4066  (0.145 sec)
18-06-05 00:41-INFO->> Step 425760 run_train: loss = 5.3739  (0.135 sec)
18-06-05 00:41-INFO->> Step 425770 run_train: loss = 5.3619  (0.148 sec)
18-06-05 00:41-INFO->> Step 425780 run_train: loss = 5.4742  (0.152 sec)
18-06-05 00:41-INFO->> Step 425790 run_train: loss = 5.4800  (0.144 sec)
18-06-05 00:41-INFO->> Step 425800 run_train: loss = 5.4288  (0.141 sec)
18-06-05 00:41-INFO->> Step 425810 run_train: loss = 5.3534  (0.142 sec)
18-06-05 00:41-INFO->> Step 425820 run_train: loss = 5.4135  (0.154 sec)
18-06-05 00:41-INFO->> Step 425830 run_train: loss = 5.4548  (0.180 sec)
18-06-05 00:41-INFO->> Step 425840 run_train: loss = 5.4489  (0.139 sec)
18-06-05 00:41-INFO->> Step 425850 run_train: loss = 5.4755  (0.164 sec)
18-06-05 00:41-INFO->> Step 425860 run_train: loss = 5.4619  (0.172 sec)
18-06-05 00:41-INFO->> Step 425870 run_train: loss = 5.3837  (0.138 sec)
18-06-05 00:41-INFO->> Step 425880 run_train: loss = 5.4858  (0.140 sec)
18-06-05 00:41-INFO->> Step 425890 run_train: loss = 5.3276  (0.144 sec)
18-06-05 00:41-INFO->> Step 425900 run_train: loss = 5.4469  (0.177 sec)
18-06-05 00:41-INFO->> Step 425910 run_train: loss = 5.3211  (0.148 sec)
18-06-05 00:41-INFO->> Step 425920 run_train: loss = 5.4138  (0.120 sec)
18-06-05 00:41-INFO->> Step 425930 run_train: loss = 5.5055  (0.147 sec)
18-06-05 00:41-INFO->> Step 425940 run_train: loss = 5.4372  (0.148 sec)
18-06-05 00:41-INFO->> Step 425950 run_train: loss = 5.4240  (0.154 sec)
18-06-05 00:41-INFO->> Step 425960 run_train: loss = 5.4315  (0.171 sec)
18-06-05 00:41-INFO->> Step 425970 run_train: loss = 5.3783  (0.182 sec)
18-06-05 00:41-INFO->> Step 425980 run_train: loss = 5.3596  (0.165 sec)
18-06-05 00:41-INFO->> Step 425990 run_train: loss = 5.3771  (0.130 sec)
18-06-05 00:41-INFO->> Step 426000 run_train: loss = 5.4592  (0.179 sec)
18-06-05 00:41-INFO->> 2018-06-05 00:41:38.899402 Saving in ckpt
18-06-05 00:41-INFO-Test Data Eval:
18-06-05 00:42-INFO-fpr95 = 0.17272183846971306 and auc = 0.9692306507267594
18-06-05 00:42-INFO->> Step 426010 run_train: loss = 5.3518  (0.146 sec)
18-06-05 00:42-INFO->> Step 426020 run_train: loss = 5.3864  (0.174 sec)
18-06-05 00:42-INFO->> Step 426030 run_train: loss = 5.4161  (0.161 sec)
18-06-05 00:42-INFO->> Step 426040 run_train: loss = 5.4472  (0.169 sec)
18-06-05 00:42-INFO->> Step 426050 run_train: loss = 5.4220  (0.133 sec)
18-06-05 00:42-INFO->> Step 426060 run_train: loss = 5.4022  (0.175 sec)
18-06-05 00:42-INFO->> Step 426070 run_train: loss = 5.3871  (0.144 sec)
18-06-05 00:42-INFO->> Step 426080 run_train: loss = 5.3295  (0.150 sec)
18-06-05 00:42-INFO->> Step 426090 run_train: loss = 5.3415  (0.168 sec)
18-06-05 00:42-INFO->> Step 426100 run_train: loss = 5.4183  (0.133 sec)
18-06-05 00:42-INFO->> Step 426110 run_train: loss = 5.4749  (0.137 sec)
18-06-05 00:42-INFO->> Step 426120 run_train: loss = 5.4014  (0.169 sec)
18-06-05 00:42-INFO->> Step 426130 run_train: loss = 5.4294  (0.150 sec)
18-06-05 00:42-INFO->> Step 426140 run_train: loss = 5.4797  (0.168 sec)
18-06-05 00:42-INFO->> Step 426150 run_train: loss = 5.4053  (0.160 sec)
18-06-05 00:42-INFO->> Step 426160 run_train: loss = 5.4323  (0.121 sec)
18-06-05 00:42-INFO->> Step 426170 run_train: loss = 5.3719  (0.154 sec)
18-06-05 00:42-INFO->> Step 426180 run_train: loss = 5.4085  (0.167 sec)
18-06-05 00:42-INFO->> Step 426190 run_train: loss = 5.4331  (0.123 sec)
18-06-05 00:42-INFO->> Step 426200 run_train: loss = 5.4505  (0.155 sec)
18-06-05 00:42-INFO->> Step 426210 run_train: loss = 5.3827  (0.144 sec)
18-06-05 00:42-INFO->> Step 426220 run_train: loss = 5.4590  (0.175 sec)
18-06-05 00:42-INFO->> Step 426230 run_train: loss = 5.3673  (0.144 sec)
18-06-05 00:42-INFO->> Step 426240 run_train: loss = 5.4398  (0.152 sec)
18-06-05 00:42-INFO->> Step 426250 run_train: loss = 5.4779  (0.139 sec)
18-06-05 00:43-INFO->> Step 426260 run_train: loss = 5.4029  (0.153 sec)
18-06-05 00:43-INFO->> Step 426270 run_train: loss = 5.3994  (0.159 sec)
18-06-05 00:43-INFO->> Step 426280 run_train: loss = 5.3888  (0.153 sec)
18-06-05 00:43-INFO->> Step 426290 run_train: loss = 5.3953  (0.175 sec)
18-06-05 00:43-INFO->> Step 426300 run_train: loss = 5.4259  (0.155 sec)
18-06-05 00:43-INFO->> Step 426310 run_train: loss = 5.4128  (0.151 sec)
18-06-05 00:43-INFO->> Step 426320 run_train: loss = 5.3990  (0.156 sec)
18-06-05 00:43-INFO->> Step 426330 run_train: loss = 5.4345  (0.193 sec)
18-06-05 00:43-INFO->> Step 426340 run_train: loss = 5.3857  (0.163 sec)
18-06-05 00:43-INFO->> Step 426350 run_train: loss = 5.3724  (0.153 sec)
18-06-05 00:43-INFO->> Step 426360 run_train: loss = 5.4191  (0.165 sec)
18-06-05 00:43-INFO->> Step 426370 run_train: loss = 5.4319  (0.144 sec)
18-06-05 00:43-INFO->> Step 426380 run_train: loss = 5.4756  (0.157 sec)
18-06-05 00:43-INFO->> Step 426390 run_train: loss = 5.4313  (0.182 sec)
18-06-05 00:43-INFO->> Step 426400 run_train: loss = 5.3804  (0.157 sec)
18-06-05 00:43-INFO->> Step 426410 run_train: loss = 5.4369  (0.159 sec)
18-06-05 00:43-INFO->> Step 426420 run_train: loss = 5.3730  (0.138 sec)
18-06-05 00:43-INFO->> Step 426430 run_train: loss = 5.4261  (0.111 sec)
18-06-05 00:43-INFO->> Step 426440 run_train: loss = 5.4377  (0.129 sec)
18-06-05 00:43-INFO->> Step 426450 run_train: loss = 5.4087  (0.142 sec)
18-06-05 00:43-INFO->> Step 426460 run_train: loss = 5.4214  (0.187 sec)
18-06-05 00:43-INFO->> Step 426470 run_train: loss = 5.4150  (0.145 sec)
18-06-05 00:43-INFO->> Step 426480 run_train: loss = 5.4605  (0.185 sec)
18-06-05 00:43-INFO->> Step 426490 run_train: loss = 5.4051  (0.154 sec)
18-06-05 00:43-INFO->> Step 426500 run_train: loss = 5.4527  (0.200 sec)
18-06-05 00:43-INFO->> Step 426510 run_train: loss = 5.4173  (0.154 sec)
18-06-05 00:43-INFO->> Step 426520 run_train: loss = 5.5137  (0.159 sec)
18-06-05 00:43-INFO->> Step 426530 run_train: loss = 5.3958  (0.123 sec)
18-06-05 00:43-INFO->> Step 426540 run_train: loss = 5.3931  (0.137 sec)
18-06-05 00:43-INFO->> Step 426550 run_train: loss = 5.4402  (0.150 sec)
18-06-05 00:43-INFO->> Step 426560 run_train: loss = 5.4597  (0.151 sec)
18-06-05 00:43-INFO->> Step 426570 run_train: loss = 5.4846  (0.139 sec)
18-06-05 00:43-INFO->> Step 426580 run_train: loss = 5.3624  (0.162 sec)
18-06-05 00:43-INFO->> Step 426590 run_train: loss = 5.4056  (0.153 sec)
18-06-05 00:43-INFO->> Step 426600 run_train: loss = 5.3842  (0.168 sec)
18-06-05 00:43-INFO->> Step 426610 run_train: loss = 5.4202  (0.175 sec)
18-06-05 00:43-INFO->> Step 426620 run_train: loss = 5.4045  (0.138 sec)
18-06-05 00:43-INFO->> Step 426630 run_train: loss = 5.4613  (0.167 sec)
18-06-05 00:44-INFO->> Step 426640 run_train: loss = 5.4164  (0.154 sec)
18-06-05 00:44-INFO->> Step 426650 run_train: loss = 5.4610  (0.182 sec)
18-06-05 00:44-INFO->> Step 426660 run_train: loss = 5.4504  (0.187 sec)
18-06-05 00:44-INFO->> Step 426670 run_train: loss = 5.4107  (0.164 sec)
18-06-05 00:44-INFO->> Step 426680 run_train: loss = 5.3875  (0.156 sec)
18-06-05 00:44-INFO->> Step 426690 run_train: loss = 5.3718  (0.143 sec)
18-06-05 00:44-INFO->> Step 426700 run_train: loss = 5.4541  (0.159 sec)
18-06-05 00:44-INFO->> Step 426710 run_train: loss = 5.4341  (0.183 sec)
18-06-05 00:44-INFO->> Step 426720 run_train: loss = 5.4134  (0.154 sec)
18-06-05 00:44-INFO->> Step 426730 run_train: loss = 5.4362  (0.158 sec)
18-06-05 00:44-INFO->> Step 426740 run_train: loss = 5.4346  (0.121 sec)
18-06-05 00:44-INFO->> Step 426750 run_train: loss = 5.3656  (0.160 sec)
18-06-05 00:44-INFO->> Step 426760 run_train: loss = 5.3916  (0.152 sec)
18-06-05 00:44-INFO->> Step 426770 run_train: loss = 5.4347  (0.174 sec)
18-06-05 00:44-INFO->> Step 426780 run_train: loss = 5.3969  (0.181 sec)
18-06-05 00:44-INFO->> Step 426790 run_train: loss = 5.3961  (0.162 sec)
18-06-05 00:44-INFO->> Step 426800 run_train: loss = 5.4453  (0.146 sec)
18-06-05 00:44-INFO->> Step 426810 run_train: loss = 5.4398  (0.163 sec)
18-06-05 00:44-INFO->> Step 426820 run_train: loss = 5.4286  (0.152 sec)
18-06-05 00:44-INFO->> Step 426830 run_train: loss = 5.4629  (0.189 sec)
18-06-05 00:44-INFO->> Step 426840 run_train: loss = 5.4060  (0.160 sec)
18-06-05 00:44-INFO->> Step 426850 run_train: loss = 5.3881  (0.160 sec)
18-06-05 00:44-INFO->> Step 426860 run_train: loss = 5.3076  (0.149 sec)
18-06-05 00:44-INFO->> Step 426870 run_train: loss = 5.3948  (0.152 sec)
18-06-05 00:44-INFO->> Step 426880 run_train: loss = 5.3990  (0.144 sec)
18-06-05 00:44-INFO->> Step 426890 run_train: loss = 5.4560  (0.150 sec)
18-06-05 00:44-INFO->> Step 426900 run_train: loss = 5.3781  (0.158 sec)
18-06-05 00:44-INFO->> Step 426910 run_train: loss = 5.3944  (0.157 sec)
18-06-05 00:44-INFO->> Step 426920 run_train: loss = 5.4512  (0.166 sec)
18-06-05 00:44-INFO->> Step 426930 run_train: loss = 5.4564  (0.151 sec)
18-06-05 00:44-INFO->> Step 426940 run_train: loss = 5.4040  (0.156 sec)
18-06-05 00:44-INFO->> Step 426950 run_train: loss = 5.3939  (0.150 sec)
18-06-05 00:44-INFO->> Step 426960 run_train: loss = 5.4307  (0.149 sec)
18-06-05 00:44-INFO->> Step 426970 run_train: loss = 5.4643  (0.135 sec)
18-06-05 00:44-INFO->> Step 426980 run_train: loss = 5.4807  (0.183 sec)
18-06-05 00:44-INFO->> Step 426990 run_train: loss = 5.4037  (0.119 sec)
18-06-05 00:44-INFO->> Step 427000 run_train: loss = 5.4501  (0.162 sec)
18-06-05 00:44-INFO->> 2018-06-05 00:44:58.007026 Saving in ckpt
18-06-05 00:44-INFO-Test Data Eval:
18-06-05 00:45-INFO-fpr95 = 0.1737596307120085 and auc = 0.9693419643874611
18-06-05 00:45-INFO->> Step 427010 run_train: loss = 5.4530  (0.154 sec)
18-06-05 00:45-INFO->> Step 427020 run_train: loss = 5.3983  (0.104 sec)
18-06-05 00:45-INFO->> Step 427030 run_train: loss = 5.4360  (0.146 sec)
18-06-05 00:45-INFO->> Step 427040 run_train: loss = 5.4518  (0.122 sec)
18-06-05 00:45-INFO->> Step 427050 run_train: loss = 5.4064  (0.110 sec)
18-06-05 00:45-INFO->> Step 427060 run_train: loss = 5.3924  (0.179 sec)
18-06-05 00:45-INFO->> Step 427070 run_train: loss = 5.4006  (0.192 sec)
18-06-05 00:45-INFO->> Step 427080 run_train: loss = 5.3757  (0.145 sec)
18-06-05 00:45-INFO->> Step 427090 run_train: loss = 5.4158  (0.170 sec)
18-06-05 00:45-INFO->> Step 427100 run_train: loss = 5.4024  (0.138 sec)
18-06-05 00:45-INFO->> Step 427110 run_train: loss = 5.3740  (0.181 sec)
18-06-05 00:45-INFO->> Step 427120 run_train: loss = 5.4599  (0.171 sec)
18-06-05 00:45-INFO->> Step 427130 run_train: loss = 5.3905  (0.163 sec)
18-06-05 00:46-INFO->> Step 427140 run_train: loss = 5.4512  (0.133 sec)
18-06-05 00:46-INFO->> Step 427150 run_train: loss = 5.3570  (0.173 sec)
18-06-05 00:46-INFO->> Step 427160 run_train: loss = 5.4276  (0.175 sec)
18-06-05 00:46-INFO->> Step 427170 run_train: loss = 5.4293  (0.171 sec)
18-06-05 00:46-INFO->> Step 427180 run_train: loss = 5.4052  (0.163 sec)
18-06-05 00:46-INFO->> Step 427190 run_train: loss = 5.4244  (0.154 sec)
18-06-05 00:46-INFO->> Step 427200 run_train: loss = 5.4152  (0.166 sec)
18-06-05 00:46-INFO->> Step 427210 run_train: loss = 5.3279  (0.148 sec)
18-06-05 00:46-INFO->> Step 427220 run_train: loss = 5.3420  (0.176 sec)
18-06-05 00:46-INFO->> Step 427230 run_train: loss = 5.4341  (0.195 sec)
18-06-05 00:46-INFO->> Step 427240 run_train: loss = 5.3764  (0.127 sec)
18-06-05 00:46-INFO->> Step 427250 run_train: loss = 5.4360  (0.139 sec)
18-06-05 00:46-INFO->> Step 427260 run_train: loss = 5.3800  (0.161 sec)
18-06-05 00:46-INFO->> Step 427270 run_train: loss = 5.4575  (0.165 sec)
18-06-05 00:46-INFO->> Step 427280 run_train: loss = 5.4525  (0.187 sec)
18-06-05 00:46-INFO->> Step 427290 run_train: loss = 5.3553  (0.136 sec)
18-06-05 00:46-INFO->> Step 427300 run_train: loss = 5.3671  (0.150 sec)
18-06-05 00:46-INFO->> Step 427310 run_train: loss = 5.4095  (0.141 sec)
18-06-05 00:46-INFO->> Step 427320 run_train: loss = 5.4449  (0.152 sec)
18-06-05 00:46-INFO->> Step 427330 run_train: loss = 5.4341  (0.183 sec)
18-06-05 00:46-INFO->> Step 427340 run_train: loss = 5.4481  (0.144 sec)
18-06-05 00:46-INFO->> Step 427350 run_train: loss = 5.3605  (0.151 sec)
18-06-05 00:46-INFO->> Step 427360 run_train: loss = 5.3593  (0.175 sec)
18-06-05 00:46-INFO->> Step 427370 run_train: loss = 5.3872  (0.174 sec)
18-06-05 00:46-INFO->> Step 427380 run_train: loss = 5.4781  (0.153 sec)
18-06-05 00:46-INFO->> Step 427390 run_train: loss = 5.3219  (0.156 sec)
18-06-05 00:46-INFO->> Step 427400 run_train: loss = 5.3880  (0.160 sec)
18-06-05 00:46-INFO->> Step 427410 run_train: loss = 5.4020  (0.129 sec)
18-06-05 00:46-INFO->> Step 427420 run_train: loss = 5.3980  (0.162 sec)
18-06-05 00:46-INFO->> Step 427430 run_train: loss = 5.4321  (0.174 sec)
18-06-05 00:46-INFO->> Step 427440 run_train: loss = 5.4381  (0.146 sec)
18-06-05 00:46-INFO->> Step 427450 run_train: loss = 5.4359  (0.161 sec)
18-06-05 00:46-INFO->> Step 427460 run_train: loss = 5.4273  (0.184 sec)
18-06-05 00:46-INFO->> Step 427470 run_train: loss = 5.3852  (0.130 sec)
18-06-05 00:46-INFO->> Step 427480 run_train: loss = 5.3825  (0.152 sec)
18-06-05 00:46-INFO->> Step 427490 run_train: loss = 5.4225  (0.177 sec)
18-06-05 00:46-INFO->> Step 427500 run_train: loss = 5.3721  (0.161 sec)
18-06-05 00:46-INFO->> Step 427510 run_train: loss = 5.4002  (0.178 sec)
18-06-05 00:47-INFO->> Step 427520 run_train: loss = 5.4426  (0.157 sec)
18-06-05 00:47-INFO->> Step 427530 run_train: loss = 5.3617  (0.133 sec)
18-06-05 00:47-INFO->> Step 427540 run_train: loss = 5.5153  (0.146 sec)
18-06-05 00:47-INFO->> Step 427550 run_train: loss = 5.3708  (0.161 sec)
18-06-05 00:47-INFO->> Step 427560 run_train: loss = 5.4221  (0.149 sec)
18-06-05 00:47-INFO->> Step 427570 run_train: loss = 5.4069  (0.173 sec)
18-06-05 00:47-INFO->> Step 427580 run_train: loss = 5.4514  (0.201 sec)
18-06-05 00:47-INFO->> Step 427590 run_train: loss = 5.4060  (0.167 sec)
18-06-05 00:47-INFO->> Step 427600 run_train: loss = 5.4825  (0.159 sec)
18-06-05 00:47-INFO->> Step 427610 run_train: loss = 5.4525  (0.164 sec)
18-06-05 00:47-INFO->> Step 427620 run_train: loss = 5.4495  (0.144 sec)
18-06-05 00:47-INFO->> Step 427630 run_train: loss = 5.4102  (0.146 sec)
18-06-05 00:47-INFO->> Step 427640 run_train: loss = 5.4259  (0.136 sec)
18-06-05 00:47-INFO->> Step 427650 run_train: loss = 5.4489  (0.151 sec)
18-06-05 00:47-INFO->> Step 427660 run_train: loss = 5.3807  (0.175 sec)
18-06-05 00:47-INFO->> Step 427670 run_train: loss = 5.4081  (0.147 sec)
18-06-05 00:47-INFO->> Step 427680 run_train: loss = 5.4171  (0.154 sec)
18-06-05 00:47-INFO->> Step 427690 run_train: loss = 5.4385  (0.181 sec)
18-06-05 00:47-INFO->> Step 427700 run_train: loss = 5.3907  (0.118 sec)
18-06-05 00:47-INFO->> Step 427710 run_train: loss = 5.4670  (0.168 sec)
18-06-05 00:47-INFO->> Step 427720 run_train: loss = 5.4903  (0.152 sec)
18-06-05 00:47-INFO->> Step 427730 run_train: loss = 5.3746  (0.203 sec)
18-06-05 00:47-INFO->> Step 427740 run_train: loss = 5.4172  (0.127 sec)
18-06-05 00:47-INFO->> Step 427750 run_train: loss = 5.4456  (0.139 sec)
18-06-05 00:47-INFO->> Step 427760 run_train: loss = 5.4034  (0.155 sec)
18-06-05 00:47-INFO->> Step 427770 run_train: loss = 5.4509  (0.179 sec)
18-06-05 00:47-INFO->> Step 427780 run_train: loss = 5.4238  (0.167 sec)
18-06-05 00:47-INFO->> Step 427790 run_train: loss = 5.4277  (0.154 sec)
18-06-05 00:47-INFO->> Step 427800 run_train: loss = 5.3896  (0.153 sec)
18-06-05 00:47-INFO->> Step 427810 run_train: loss = 5.3910  (0.138 sec)
18-06-05 00:47-INFO->> Step 427820 run_train: loss = 5.4211  (0.141 sec)
18-06-05 00:47-INFO->> Step 427830 run_train: loss = 5.4371  (0.140 sec)
18-06-05 00:47-INFO->> Step 427840 run_train: loss = 5.4289  (0.146 sec)
18-06-05 00:47-INFO->> Step 427850 run_train: loss = 5.4283  (0.130 sec)
18-06-05 00:47-INFO->> Step 427860 run_train: loss = 5.4234  (0.141 sec)
18-06-05 00:47-INFO->> Step 427870 run_train: loss = 5.4487  (0.159 sec)
18-06-05 00:47-INFO->> Step 427880 run_train: loss = 5.4301  (0.137 sec)
18-06-05 00:48-INFO->> Step 427890 run_train: loss = 5.4409  (0.148 sec)
18-06-05 00:48-INFO->> Step 427900 run_train: loss = 5.4448  (0.171 sec)
18-06-05 00:48-INFO->> Step 427910 run_train: loss = 5.4595  (0.144 sec)
18-06-05 00:48-INFO->> Step 427920 run_train: loss = 5.3794  (0.170 sec)
18-06-05 00:48-INFO->> Step 427930 run_train: loss = 5.4588  (0.145 sec)
18-06-05 00:48-INFO->> Step 427940 run_train: loss = 5.3572  (0.156 sec)
18-06-05 00:48-INFO->> Step 427950 run_train: loss = 5.3681  (0.181 sec)
18-06-05 00:48-INFO->> Step 427960 run_train: loss = 5.4355  (0.206 sec)
18-06-05 00:48-INFO->> Step 427970 run_train: loss = 5.4297  (0.150 sec)
18-06-05 00:48-INFO->> Step 427980 run_train: loss = 5.3625  (0.165 sec)
18-06-05 00:48-INFO->> Step 427990 run_train: loss = 5.3560  (0.129 sec)
18-06-05 00:48-INFO->> Step 428000 run_train: loss = 5.3862  (0.184 sec)
18-06-05 00:48-INFO->> 2018-06-05 00:48:17.958772 Saving in ckpt
18-06-05 00:48-INFO-Test Data Eval:
18-06-05 00:48-INFO-fpr95 = 0.17392567747077578 and auc = 0.969019910627582
18-06-05 00:48-INFO->> Step 428010 run_train: loss = 5.4397  (0.169 sec)
18-06-05 00:49-INFO->> Step 428020 run_train: loss = 5.4420  (0.180 sec)
18-06-05 00:49-INFO->> Step 428030 run_train: loss = 5.3601  (0.140 sec)
18-06-05 00:49-INFO->> Step 428040 run_train: loss = 5.3940  (0.134 sec)
18-06-05 00:49-INFO->> Step 428050 run_train: loss = 5.4001  (0.178 sec)
18-06-05 00:49-INFO->> Step 428060 run_train: loss = 5.4434  (0.154 sec)
18-06-05 00:49-INFO->> Step 428070 run_train: loss = 5.4505  (0.166 sec)
18-06-05 00:49-INFO->> Step 428080 run_train: loss = 5.5009  (0.165 sec)
18-06-05 00:49-INFO->> Step 428090 run_train: loss = 5.3837  (0.155 sec)
18-06-05 00:49-INFO->> Step 428100 run_train: loss = 5.3793  (0.136 sec)
18-06-05 00:49-INFO->> Step 428110 run_train: loss = 5.4424  (0.178 sec)
18-06-05 00:49-INFO->> Step 428120 run_train: loss = 5.4229  (0.161 sec)
18-06-05 00:49-INFO->> Step 428130 run_train: loss = 5.4304  (0.171 sec)
18-06-05 00:49-INFO->> Step 428140 run_train: loss = 5.4373  (0.171 sec)
18-06-05 00:49-INFO->> Step 428150 run_train: loss = 5.4683  (0.153 sec)
18-06-05 00:49-INFO->> Step 428160 run_train: loss = 5.4050  (0.156 sec)
18-06-05 00:49-INFO->> Step 428170 run_train: loss = 5.4380  (0.151 sec)
18-06-05 00:49-INFO->> Step 428180 run_train: loss = 5.3856  (0.183 sec)
18-06-05 00:49-INFO->> Step 428190 run_train: loss = 5.4954  (0.183 sec)
18-06-05 00:49-INFO->> Step 428200 run_train: loss = 5.4550  (0.146 sec)
18-06-05 00:49-INFO->> Step 428210 run_train: loss = 5.3765  (0.143 sec)
18-06-05 00:49-INFO->> Step 428220 run_train: loss = 5.4359  (0.153 sec)
18-06-05 00:49-INFO->> Step 428230 run_train: loss = 5.4363  (0.178 sec)
18-06-05 00:49-INFO->> Step 428240 run_train: loss = 5.3992  (0.156 sec)
18-06-05 00:49-INFO->> Step 428250 run_train: loss = 5.3696  (0.142 sec)
18-06-05 00:49-INFO->> Step 428260 run_train: loss = 5.4097  (0.166 sec)
18-06-05 00:49-INFO->> Step 428270 run_train: loss = 5.4068  (0.150 sec)
18-06-05 00:49-INFO->> Step 428280 run_train: loss = 5.4406  (0.164 sec)
18-06-05 00:49-INFO->> Step 428290 run_train: loss = 5.4393  (0.154 sec)
18-06-05 00:49-INFO->> Step 428300 run_train: loss = 5.4930  (0.179 sec)
18-06-05 00:49-INFO->> Step 428310 run_train: loss = 5.3484  (0.157 sec)
18-06-05 00:49-INFO->> Step 428320 run_train: loss = 5.3953  (0.127 sec)
18-06-05 00:49-INFO->> Step 428330 run_train: loss = 5.3934  (0.144 sec)
18-06-05 00:49-INFO->> Step 428340 run_train: loss = 5.3968  (0.144 sec)
18-06-05 00:49-INFO->> Step 428350 run_train: loss = 5.4232  (0.178 sec)
18-06-05 00:49-INFO->> Step 428360 run_train: loss = 5.3872  (0.166 sec)
18-06-05 00:49-INFO->> Step 428370 run_train: loss = 5.4063  (0.157 sec)
18-06-05 00:49-INFO->> Step 428380 run_train: loss = 5.3658  (0.185 sec)
18-06-05 00:50-INFO->> Step 428390 run_train: loss = 5.4279  (0.149 sec)
18-06-05 00:50-INFO->> Step 428400 run_train: loss = 5.4511  (0.155 sec)
18-06-05 00:50-INFO->> Step 428410 run_train: loss = 5.3990  (0.154 sec)
18-06-05 00:50-INFO->> Step 428420 run_train: loss = 5.4525  (0.145 sec)
18-06-05 00:50-INFO->> Step 428430 run_train: loss = 5.4384  (0.144 sec)
18-06-05 00:50-INFO->> Step 428440 run_train: loss = 5.4356  (0.152 sec)
18-06-05 00:50-INFO->> Step 428450 run_train: loss = 5.3497  (0.173 sec)
18-06-05 00:50-INFO->> Step 428460 run_train: loss = 5.4023  (0.155 sec)
18-06-05 00:50-INFO->> Step 428470 run_train: loss = 5.4262  (0.170 sec)
18-06-05 00:50-INFO->> Step 428480 run_train: loss = 5.4700  (0.158 sec)
18-06-05 00:50-INFO->> Step 428490 run_train: loss = 5.4112  (0.159 sec)
18-06-05 00:50-INFO->> Step 428500 run_train: loss = 5.4120  (0.150 sec)
18-06-05 00:50-INFO->> Step 428510 run_train: loss = 5.4571  (0.158 sec)
18-06-05 00:50-INFO->> Step 428520 run_train: loss = 5.3846  (0.166 sec)
18-06-05 00:50-INFO->> Step 428530 run_train: loss = 5.4493  (0.141 sec)
18-06-05 00:50-INFO->> Step 428540 run_train: loss = 5.4495  (0.185 sec)
18-06-05 00:50-INFO->> Step 428550 run_train: loss = 5.4366  (0.172 sec)
18-06-05 00:50-INFO->> Step 428560 run_train: loss = 5.3867  (0.152 sec)
18-06-05 00:50-INFO->> Step 428570 run_train: loss = 5.4728  (0.160 sec)
18-06-05 00:50-INFO->> Step 428580 run_train: loss = 5.3851  (0.203 sec)
18-06-05 00:50-INFO->> Step 428590 run_train: loss = 5.3798  (0.161 sec)
18-06-05 00:50-INFO->> Step 428600 run_train: loss = 5.3036  (0.162 sec)
18-06-05 00:50-INFO->> Step 428610 run_train: loss = 5.4298  (0.159 sec)
18-06-05 00:50-INFO->> Step 428620 run_train: loss = 5.4543  (0.188 sec)
18-06-05 00:50-INFO->> Step 428630 run_train: loss = 5.3965  (0.171 sec)
18-06-05 00:50-INFO->> Step 428640 run_train: loss = 5.4527  (0.137 sec)
18-06-05 00:50-INFO->> Step 428650 run_train: loss = 5.4541  (0.139 sec)
18-06-05 00:50-INFO->> Step 428660 run_train: loss = 5.3656  (0.183 sec)
18-06-05 00:50-INFO->> Step 428670 run_train: loss = 5.4297  (0.155 sec)
18-06-05 00:50-INFO->> Step 428680 run_train: loss = 5.3865  (0.146 sec)
18-06-05 00:50-INFO->> Step 428690 run_train: loss = 5.4477  (0.134 sec)
18-06-05 00:50-INFO->> Step 428700 run_train: loss = 5.4715  (0.153 sec)
18-06-05 00:50-INFO->> Step 428710 run_train: loss = 5.3876  (0.176 sec)
18-06-05 00:50-INFO->> Step 428720 run_train: loss = 5.4727  (0.168 sec)
18-06-05 00:50-INFO->> Step 428730 run_train: loss = 5.5104  (0.190 sec)
18-06-05 00:50-INFO->> Step 428740 run_train: loss = 5.4628  (0.145 sec)
18-06-05 00:50-INFO->> Step 428750 run_train: loss = 5.4496  (0.126 sec)
18-06-05 00:50-INFO->> Step 428760 run_train: loss = 5.4095  (0.155 sec)
18-06-05 00:51-INFO->> Step 428770 run_train: loss = 5.3976  (0.163 sec)
18-06-05 00:51-INFO->> Step 428780 run_train: loss = 5.3900  (0.145 sec)
18-06-05 00:51-INFO->> Step 428790 run_train: loss = 5.4752  (0.162 sec)
18-06-05 00:51-INFO->> Step 428800 run_train: loss = 5.3943  (0.161 sec)
18-06-05 00:51-INFO->> Step 428810 run_train: loss = 5.3997  (0.160 sec)
18-06-05 00:51-INFO->> Step 428820 run_train: loss = 5.2976  (0.176 sec)
18-06-05 00:51-INFO->> Step 428830 run_train: loss = 5.3556  (0.116 sec)
18-06-05 00:51-INFO->> Step 428840 run_train: loss = 5.4273  (0.145 sec)
18-06-05 00:51-INFO->> Step 428850 run_train: loss = 5.4123  (0.153 sec)
18-06-05 00:51-INFO->> Step 428860 run_train: loss = 5.3880  (0.166 sec)
18-06-05 00:51-INFO->> Step 428870 run_train: loss = 5.4789  (0.182 sec)
18-06-05 00:51-INFO->> Step 428880 run_train: loss = 5.3863  (0.163 sec)
18-06-05 00:51-INFO->> Step 428890 run_train: loss = 5.4187  (0.170 sec)
18-06-05 00:51-INFO->> Step 428900 run_train: loss = 5.3892  (0.137 sec)
18-06-05 00:51-INFO->> Step 428910 run_train: loss = 5.4214  (0.157 sec)
18-06-05 00:51-INFO->> Step 428920 run_train: loss = 5.4568  (0.192 sec)
18-06-05 00:51-INFO->> Step 428930 run_train: loss = 5.4666  (0.168 sec)
18-06-05 00:51-INFO->> Step 428940 run_train: loss = 5.4837  (0.169 sec)
18-06-05 00:51-INFO->> Step 428950 run_train: loss = 5.4397  (0.158 sec)
18-06-05 00:51-INFO->> Step 428960 run_train: loss = 5.4253  (0.135 sec)
18-06-05 00:51-INFO->> Step 428970 run_train: loss = 5.4541  (0.151 sec)
18-06-05 00:51-INFO->> Step 428980 run_train: loss = 5.4118  (0.115 sec)
18-06-05 00:51-INFO->> Step 428990 run_train: loss = 5.4295  (0.144 sec)
18-06-05 00:51-INFO->> Step 429000 run_train: loss = 5.4470  (0.167 sec)
18-06-05 00:51-INFO->> 2018-06-05 00:51:37.524383 Saving in ckpt
18-06-05 00:51-INFO-Test Data Eval:
18-06-05 00:52-INFO-fpr95 = 0.1737347236981934 and auc = 0.9691091523511038
18-06-05 00:52-INFO->> Step 429010 run_train: loss = 5.3825  (0.150 sec)
18-06-05 00:52-INFO->> Step 429020 run_train: loss = 5.4853  (0.153 sec)
18-06-05 00:52-INFO->> Step 429030 run_train: loss = 5.4005  (0.180 sec)
18-06-05 00:52-INFO->> Step 429040 run_train: loss = 5.3870  (0.175 sec)
18-06-05 00:52-INFO->> Step 429050 run_train: loss = 5.3668  (0.201 sec)
18-06-05 00:52-INFO->> Step 429060 run_train: loss = 5.4149  (0.178 sec)
18-06-05 00:52-INFO->> Step 429070 run_train: loss = 5.3976  (0.158 sec)
18-06-05 00:52-INFO->> Step 429080 run_train: loss = 5.3893  (0.193 sec)
18-06-05 00:52-INFO->> Step 429090 run_train: loss = 5.4959  (0.187 sec)
18-06-05 00:52-INFO->> Step 429100 run_train: loss = 5.4759  (0.170 sec)
18-06-05 00:52-INFO->> Step 429110 run_train: loss = 5.4833  (0.152 sec)
18-06-05 00:52-INFO->> Step 429120 run_train: loss = 5.4876  (0.146 sec)
18-06-05 00:52-INFO->> Step 429130 run_train: loss = 5.3815  (0.176 sec)
18-06-05 00:52-INFO->> Step 429140 run_train: loss = 5.3745  (0.142 sec)
18-06-05 00:52-INFO->> Step 429150 run_train: loss = 5.3796  (0.141 sec)
18-06-05 00:52-INFO->> Step 429160 run_train: loss = 5.4757  (0.181 sec)
18-06-05 00:52-INFO->> Step 429170 run_train: loss = 5.3793  (0.154 sec)
18-06-05 00:52-INFO->> Step 429180 run_train: loss = 5.3850  (0.153 sec)
18-06-05 00:52-INFO->> Step 429190 run_train: loss = 5.4378  (0.137 sec)
18-06-05 00:52-INFO->> Step 429200 run_train: loss = 5.4529  (0.142 sec)
18-06-05 00:52-INFO->> Step 429210 run_train: loss = 5.4620  (0.185 sec)
18-06-05 00:52-INFO->> Step 429220 run_train: loss = 5.3867  (0.146 sec)
18-06-05 00:52-INFO->> Step 429230 run_train: loss = 5.4012  (0.133 sec)
18-06-05 00:52-INFO->> Step 429240 run_train: loss = 5.4746  (0.165 sec)
18-06-05 00:52-INFO->> Step 429250 run_train: loss = 5.4294  (0.135 sec)
18-06-05 00:52-INFO->> Step 429260 run_train: loss = 5.3989  (0.196 sec)
18-06-05 00:53-INFO->> Step 429270 run_train: loss = 5.4817  (0.158 sec)
18-06-05 00:53-INFO->> Step 429280 run_train: loss = 5.4073  (0.164 sec)
18-06-05 00:53-INFO->> Step 429290 run_train: loss = 5.3491  (0.154 sec)
18-06-05 00:53-INFO->> Step 429300 run_train: loss = 5.4439  (0.161 sec)
18-06-05 00:53-INFO->> Step 429310 run_train: loss = 5.4546  (0.153 sec)
18-06-05 00:53-INFO->> Step 429320 run_train: loss = 5.4065  (0.160 sec)
18-06-05 00:53-INFO->> Step 429330 run_train: loss = 5.4413  (0.143 sec)
18-06-05 00:53-INFO->> Step 429340 run_train: loss = 5.4002  (0.186 sec)
18-06-05 00:53-INFO->> Step 429350 run_train: loss = 5.4502  (0.155 sec)
18-06-05 00:53-INFO->> Step 429360 run_train: loss = 5.4328  (0.204 sec)
18-06-05 00:53-INFO->> Step 429370 run_train: loss = 5.4266  (0.166 sec)
18-06-05 00:53-INFO->> Step 429380 run_train: loss = 5.4116  (0.129 sec)
18-06-05 00:53-INFO->> Step 429390 run_train: loss = 5.3842  (0.132 sec)
18-06-05 00:53-INFO->> Step 429400 run_train: loss = 5.4341  (0.143 sec)
18-06-05 00:53-INFO->> Step 429410 run_train: loss = 5.3817  (0.163 sec)
18-06-05 00:53-INFO->> Step 429420 run_train: loss = 5.4337  (0.162 sec)
18-06-05 00:53-INFO->> Step 429430 run_train: loss = 5.4677  (0.165 sec)
18-06-05 00:53-INFO->> Step 429440 run_train: loss = 5.4186  (0.171 sec)
18-06-05 00:53-INFO->> Step 429450 run_train: loss = 5.3463  (0.186 sec)
18-06-05 00:53-INFO->> Step 429460 run_train: loss = 5.3866  (0.155 sec)
18-06-05 00:53-INFO->> Step 429470 run_train: loss = 5.3984  (0.191 sec)
18-06-05 00:53-INFO->> Step 429480 run_train: loss = 5.4588  (0.130 sec)
18-06-05 00:53-INFO->> Step 429490 run_train: loss = 5.4196  (0.151 sec)
18-06-05 00:53-INFO->> Step 429500 run_train: loss = 5.4740  (0.181 sec)
18-06-05 00:53-INFO->> Step 429510 run_train: loss = 5.3824  (0.154 sec)
18-06-05 00:53-INFO->> Step 429520 run_train: loss = 5.4729  (0.184 sec)
18-06-05 00:53-INFO->> Step 429530 run_train: loss = 5.3763  (0.147 sec)
18-06-05 00:53-INFO->> Step 429540 run_train: loss = 5.3794  (0.120 sec)
18-06-05 00:53-INFO->> Step 429550 run_train: loss = 5.4279  (0.145 sec)
18-06-05 00:53-INFO->> Step 429560 run_train: loss = 5.4281  (0.149 sec)
18-06-05 00:53-INFO->> Step 429570 run_train: loss = 5.3434  (0.191 sec)
18-06-05 00:53-INFO->> Step 429580 run_train: loss = 5.4600  (0.147 sec)
18-06-05 00:53-INFO->> Step 429590 run_train: loss = 5.4103  (0.159 sec)
18-06-05 00:53-INFO->> Step 429600 run_train: loss = 5.3935  (0.132 sec)
18-06-05 00:53-INFO->> Step 429610 run_train: loss = 5.3969  (0.144 sec)
18-06-05 00:53-INFO->> Step 429620 run_train: loss = 5.4033  (0.153 sec)
18-06-05 00:53-INFO->> Step 429630 run_train: loss = 5.3548  (0.187 sec)
18-06-05 00:53-INFO->> Step 429640 run_train: loss = 5.4288  (0.119 sec)
18-06-05 00:54-INFO->> Step 429650 run_train: loss = 5.4683  (0.176 sec)
18-06-05 00:54-INFO->> Step 429660 run_train: loss = 5.4612  (0.175 sec)
18-06-05 00:54-INFO->> Step 429670 run_train: loss = 5.4475  (0.158 sec)
18-06-05 00:54-INFO->> Step 429680 run_train: loss = 5.3665  (0.164 sec)
18-06-05 00:54-INFO->> Step 429690 run_train: loss = 5.4174  (0.137 sec)
18-06-05 00:54-INFO->> Step 429700 run_train: loss = 5.3757  (0.184 sec)
18-06-05 00:54-INFO->> Step 429710 run_train: loss = 5.4102  (0.169 sec)
18-06-05 00:54-INFO->> Step 429720 run_train: loss = 5.4279  (0.139 sec)
18-06-05 00:54-INFO->> Step 429730 run_train: loss = 5.5213  (0.194 sec)
18-06-05 00:54-INFO->> Step 429740 run_train: loss = 5.4221  (0.154 sec)
18-06-05 00:54-INFO->> Step 429750 run_train: loss = 5.3478  (0.170 sec)
18-06-05 00:54-INFO->> Step 429760 run_train: loss = 5.3716  (0.148 sec)
18-06-05 00:54-INFO->> Step 429770 run_train: loss = 5.4467  (0.132 sec)
18-06-05 00:54-INFO->> Step 429780 run_train: loss = 5.4155  (0.146 sec)
18-06-05 00:54-INFO->> Step 429790 run_train: loss = 5.3881  (0.183 sec)
18-06-05 00:54-INFO->> Step 429800 run_train: loss = 5.3322  (0.172 sec)
18-06-05 00:54-INFO->> Step 429810 run_train: loss = 5.4422  (0.151 sec)
18-06-05 00:54-INFO->> Step 429820 run_train: loss = 5.3877  (0.152 sec)
18-06-05 00:54-INFO->> Step 429830 run_train: loss = 5.4244  (0.143 sec)
18-06-05 00:54-INFO->> Step 429840 run_train: loss = 5.4244  (0.174 sec)
18-06-05 00:54-INFO->> Step 429850 run_train: loss = 5.3668  (0.151 sec)
18-06-05 00:54-INFO->> Step 429860 run_train: loss = 5.4158  (0.181 sec)
18-06-05 00:54-INFO->> Step 429870 run_train: loss = 5.4437  (0.177 sec)
18-06-05 00:54-INFO->> Step 429880 run_train: loss = 5.4453  (0.193 sec)
18-06-05 00:54-INFO->> Step 429890 run_train: loss = 5.4141  (0.183 sec)
18-06-05 00:54-INFO->> Step 429900 run_train: loss = 5.4601  (0.162 sec)
18-06-05 00:54-INFO->> Step 429910 run_train: loss = 5.3975  (0.145 sec)
18-06-05 00:54-INFO->> Step 429920 run_train: loss = 5.4093  (0.160 sec)
18-06-05 00:54-INFO->> Step 429930 run_train: loss = 5.4699  (0.172 sec)
18-06-05 00:54-INFO->> Step 429940 run_train: loss = 5.4393  (0.145 sec)
18-06-05 00:54-INFO->> Step 429950 run_train: loss = 5.4315  (0.149 sec)
18-06-05 00:54-INFO->> Step 429960 run_train: loss = 5.4349  (0.180 sec)
18-06-05 00:54-INFO->> Step 429970 run_train: loss = 5.3986  (0.181 sec)
18-06-05 00:54-INFO->> Step 429980 run_train: loss = 5.3645  (0.144 sec)
18-06-05 00:54-INFO->> Step 429990 run_train: loss = 5.4450  (0.150 sec)
18-06-05 00:54-INFO->> Step 430000 run_train: loss = 5.4342  (0.156 sec)
18-06-05 00:54-INFO->> 2018-06-05 00:54:56.368780 Saving in ckpt
18-06-05 00:54-INFO-Test Data Eval:
18-06-05 00:55-INFO-fpr95 = 0.168014412858661 and auc = 0.9695974681689049
18-06-05 00:55-INFO->> Step 430010 run_train: loss = 5.3898  (0.135 sec)
18-06-05 00:55-INFO->> Step 430020 run_train: loss = 5.3940  (0.183 sec)
18-06-05 00:55-INFO->> Step 430030 run_train: loss = 5.3764  (0.131 sec)
18-06-05 00:55-INFO->> Step 430040 run_train: loss = 5.4326  (0.176 sec)
18-06-05 00:55-INFO->> Step 430050 run_train: loss = 5.4052  (0.143 sec)
18-06-05 00:55-INFO->> Step 430060 run_train: loss = 5.4308  (0.185 sec)
18-06-05 00:55-INFO->> Step 430070 run_train: loss = 5.3862  (0.151 sec)
18-06-05 00:55-INFO->> Step 430080 run_train: loss = 5.3471  (0.149 sec)
18-06-05 00:55-INFO->> Step 430090 run_train: loss = 5.4041  (0.156 sec)
18-06-05 00:55-INFO->> Step 430100 run_train: loss = 5.4026  (0.146 sec)
18-06-05 00:55-INFO->> Step 430110 run_train: loss = 5.3230  (0.149 sec)
18-06-05 00:55-INFO->> Step 430120 run_train: loss = 5.4010  (0.188 sec)
18-06-05 00:55-INFO->> Step 430130 run_train: loss = 5.4220  (0.145 sec)
18-06-05 00:55-INFO->> Step 430140 run_train: loss = 5.3573  (0.160 sec)
18-06-05 00:56-INFO->> Step 430150 run_train: loss = 5.3979  (0.150 sec)
18-06-05 00:56-INFO->> Step 430160 run_train: loss = 5.3969  (0.171 sec)
18-06-05 00:56-INFO->> Step 430170 run_train: loss = 5.4810  (0.123 sec)
18-06-05 00:56-INFO->> Step 430180 run_train: loss = 5.3838  (0.141 sec)
18-06-05 00:56-INFO->> Step 430190 run_train: loss = 5.3727  (0.162 sec)
18-06-05 00:56-INFO->> Step 430200 run_train: loss = 5.3618  (0.169 sec)
18-06-05 00:56-INFO->> Step 430210 run_train: loss = 5.4050  (0.155 sec)
18-06-05 00:56-INFO->> Step 430220 run_train: loss = 5.3747  (0.158 sec)
18-06-05 00:56-INFO->> Step 430230 run_train: loss = 5.4375  (0.133 sec)
18-06-05 00:56-INFO->> Step 430240 run_train: loss = 5.4091  (0.184 sec)
18-06-05 00:56-INFO->> Step 430250 run_train: loss = 5.3835  (0.161 sec)
18-06-05 00:56-INFO->> Step 430260 run_train: loss = 5.4063  (0.190 sec)
18-06-05 00:56-INFO->> Step 430270 run_train: loss = 5.3559  (0.166 sec)
18-06-05 00:56-INFO->> Step 430280 run_train: loss = 5.3693  (0.153 sec)
18-06-05 00:56-INFO->> Step 430290 run_train: loss = 5.4358  (0.151 sec)
18-06-05 00:56-INFO->> Step 430300 run_train: loss = 5.4282  (0.156 sec)
18-06-05 00:56-INFO->> Step 430310 run_train: loss = 5.4105  (0.150 sec)
18-06-05 00:56-INFO->> Step 430320 run_train: loss = 5.3648  (0.134 sec)
18-06-05 00:56-INFO->> Step 430330 run_train: loss = 5.4196  (0.150 sec)
18-06-05 00:56-INFO->> Step 430340 run_train: loss = 5.3692  (0.149 sec)
18-06-05 00:56-INFO->> Step 430350 run_train: loss = 5.3420  (0.145 sec)
18-06-05 00:56-INFO->> Step 430360 run_train: loss = 5.4452  (0.168 sec)
18-06-05 00:56-INFO->> Step 430370 run_train: loss = 5.3949  (0.163 sec)
18-06-05 00:56-INFO->> Step 430380 run_train: loss = 5.4526  (0.192 sec)
18-06-05 00:56-INFO->> Step 430390 run_train: loss = 5.4258  (0.165 sec)
18-06-05 00:56-INFO->> Step 430400 run_train: loss = 5.4287  (0.150 sec)
18-06-05 00:56-INFO->> Step 430410 run_train: loss = 5.3390  (0.135 sec)
18-06-05 00:56-INFO->> Step 430420 run_train: loss = 5.4645  (0.168 sec)
18-06-05 00:56-INFO->> Step 430430 run_train: loss = 5.4073  (0.156 sec)
18-06-05 00:56-INFO->> Step 430440 run_train: loss = 5.4773  (0.125 sec)
18-06-05 00:56-INFO->> Step 430450 run_train: loss = 5.4181  (0.191 sec)
18-06-05 00:56-INFO->> Step 430460 run_train: loss = 5.4198  (0.174 sec)
18-06-05 00:56-INFO->> Step 430470 run_train: loss = 5.3489  (0.144 sec)
18-06-05 00:56-INFO->> Step 430480 run_train: loss = 5.4274  (0.157 sec)
18-06-05 00:56-INFO->> Step 430490 run_train: loss = 5.3856  (0.190 sec)
18-06-05 00:56-INFO->> Step 430500 run_train: loss = 5.3681  (0.125 sec)
18-06-05 00:56-INFO->> Step 430510 run_train: loss = 5.3609  (0.164 sec)
18-06-05 00:56-INFO->> Step 430520 run_train: loss = 5.4078  (0.174 sec)
18-06-05 00:57-INFO->> Step 430530 run_train: loss = 5.4074  (0.153 sec)
18-06-05 00:57-INFO->> Step 430540 run_train: loss = 5.4387  (0.155 sec)
18-06-05 00:57-INFO->> Step 430550 run_train: loss = 5.3973  (0.175 sec)
18-06-05 00:57-INFO->> Step 430560 run_train: loss = 5.4192  (0.212 sec)
18-06-05 00:57-INFO->> Step 430570 run_train: loss = 5.4001  (0.192 sec)
18-06-05 00:57-INFO->> Step 430580 run_train: loss = 5.4102  (0.194 sec)
18-06-05 00:57-INFO->> Step 430590 run_train: loss = 5.4229  (0.137 sec)
18-06-05 00:57-INFO->> Step 430600 run_train: loss = 5.4813  (0.139 sec)
18-06-05 00:57-INFO->> Step 430610 run_train: loss = 5.3285  (0.167 sec)
18-06-05 00:57-INFO->> Step 430620 run_train: loss = 5.4126  (0.138 sec)
18-06-05 00:57-INFO->> Step 430630 run_train: loss = 5.4397  (0.142 sec)
18-06-05 00:57-INFO->> Step 430640 run_train: loss = 5.3767  (0.172 sec)
18-06-05 00:57-INFO->> Step 430650 run_train: loss = 5.3981  (0.156 sec)
18-06-05 00:57-INFO->> Step 430660 run_train: loss = 5.4387  (0.148 sec)
18-06-05 00:57-INFO->> Step 430670 run_train: loss = 5.4495  (0.145 sec)
18-06-05 00:57-INFO->> Step 430680 run_train: loss = 5.4122  (0.192 sec)
18-06-05 00:57-INFO->> Step 430690 run_train: loss = 5.3822  (0.114 sec)
18-06-05 00:57-INFO->> Step 430700 run_train: loss = 5.4478  (0.160 sec)
18-06-05 00:57-INFO->> Step 430710 run_train: loss = 5.4049  (0.158 sec)
18-06-05 00:57-INFO->> Step 430720 run_train: loss = 5.4051  (0.155 sec)
18-06-05 00:57-INFO->> Step 430730 run_train: loss = 5.4128  (0.180 sec)
18-06-05 00:57-INFO->> Step 430740 run_train: loss = 5.3850  (0.184 sec)
18-06-05 00:57-INFO->> Step 430750 run_train: loss = 5.4085  (0.142 sec)
18-06-05 00:57-INFO->> Step 430760 run_train: loss = 5.3815  (0.150 sec)
18-06-05 00:57-INFO->> Step 430770 run_train: loss = 5.3474  (0.165 sec)
18-06-05 00:57-INFO->> Step 430780 run_train: loss = 5.4121  (0.151 sec)
18-06-05 00:57-INFO->> Step 430790 run_train: loss = 5.4576  (0.147 sec)
18-06-05 00:57-INFO->> Step 430800 run_train: loss = 5.4352  (0.161 sec)
18-06-05 00:57-INFO->> Step 430810 run_train: loss = 5.4628  (0.156 sec)
18-06-05 00:57-INFO->> Step 430820 run_train: loss = 5.3790  (0.171 sec)
18-06-05 00:57-INFO->> Step 430830 run_train: loss = 5.3928  (0.186 sec)
18-06-05 00:57-INFO->> Step 430840 run_train: loss = 5.3942  (0.197 sec)
18-06-05 00:57-INFO->> Step 430850 run_train: loss = 5.3891  (0.150 sec)
18-06-05 00:57-INFO->> Step 430860 run_train: loss = 5.3963  (0.152 sec)
18-06-05 00:57-INFO->> Step 430870 run_train: loss = 5.4274  (0.181 sec)
18-06-05 00:57-INFO->> Step 430880 run_train: loss = 5.4902  (0.185 sec)
18-06-05 00:57-INFO->> Step 430890 run_train: loss = 5.3893  (0.158 sec)
18-06-05 00:57-INFO->> Step 430900 run_train: loss = 5.4189  (0.160 sec)
18-06-05 00:58-INFO->> Step 430910 run_train: loss = 5.4352  (0.110 sec)
18-06-05 00:58-INFO->> Step 430920 run_train: loss = 5.3784  (0.167 sec)
18-06-05 00:58-INFO->> Step 430930 run_train: loss = 5.3965  (0.154 sec)
18-06-05 00:58-INFO->> Step 430940 run_train: loss = 5.4466  (0.155 sec)
18-06-05 00:58-INFO->> Step 430950 run_train: loss = 5.4376  (0.140 sec)
18-06-05 00:58-INFO->> Step 430960 run_train: loss = 5.4444  (0.140 sec)
18-06-05 00:58-INFO->> Step 430970 run_train: loss = 5.4671  (0.176 sec)
18-06-05 00:58-INFO->> Step 430980 run_train: loss = 5.4098  (0.144 sec)
18-06-05 00:58-INFO->> Step 430990 run_train: loss = 5.4111  (0.169 sec)
18-06-05 00:58-INFO->> Step 431000 run_train: loss = 5.4224  (0.157 sec)
18-06-05 00:58-INFO->> 2018-06-05 00:58:15.759410 Saving in ckpt
18-06-05 00:58-INFO-Test Data Eval:
18-06-05 00:58-INFO-fpr95 = 0.17443212008501593 and auc = 0.9689924871950444
18-06-05 00:58-INFO->> Step 431010 run_train: loss = 5.4308  (0.135 sec)
18-06-05 00:58-INFO->> Step 431020 run_train: loss = 5.3832  (0.181 sec)
18-06-05 00:59-INFO->> Step 431030 run_train: loss = 5.4085  (0.154 sec)
18-06-05 00:59-INFO->> Step 431040 run_train: loss = 5.4536  (0.169 sec)
18-06-05 00:59-INFO->> Step 431050 run_train: loss = 5.4763  (0.160 sec)
18-06-05 00:59-INFO->> Step 431060 run_train: loss = 5.3961  (0.176 sec)
18-06-05 00:59-INFO->> Step 431070 run_train: loss = 5.3779  (0.177 sec)
18-06-05 00:59-INFO->> Step 431080 run_train: loss = 5.4547  (0.165 sec)
18-06-05 00:59-INFO->> Step 431090 run_train: loss = 5.4123  (0.150 sec)
18-06-05 00:59-INFO->> Step 431100 run_train: loss = 5.4543  (0.133 sec)
18-06-05 00:59-INFO->> Step 431110 run_train: loss = 5.3660  (0.174 sec)
18-06-05 00:59-INFO->> Step 431120 run_train: loss = 5.4508  (0.140 sec)
18-06-05 00:59-INFO->> Step 431130 run_train: loss = 5.4223  (0.164 sec)
18-06-05 00:59-INFO->> Step 431140 run_train: loss = 5.3834  (0.161 sec)
18-06-05 00:59-INFO->> Step 431150 run_train: loss = 5.4235  (0.136 sec)
18-06-05 00:59-INFO->> Step 431160 run_train: loss = 5.4070  (0.162 sec)
18-06-05 00:59-INFO->> Step 431170 run_train: loss = 5.4225  (0.185 sec)
18-06-05 00:59-INFO->> Step 431180 run_train: loss = 5.4232  (0.160 sec)
18-06-05 00:59-INFO->> Step 431190 run_train: loss = 5.3725  (0.146 sec)
18-06-05 00:59-INFO->> Step 431200 run_train: loss = 5.4090  (0.136 sec)
18-06-05 00:59-INFO->> Step 431210 run_train: loss = 5.4537  (0.146 sec)
18-06-05 00:59-INFO->> Step 431220 run_train: loss = 5.4492  (0.193 sec)
18-06-05 00:59-INFO->> Step 431230 run_train: loss = 5.4479  (0.191 sec)
18-06-05 00:59-INFO->> Step 431240 run_train: loss = 5.4536  (0.146 sec)
18-06-05 00:59-INFO->> Step 431250 run_train: loss = 5.3863  (0.163 sec)
18-06-05 00:59-INFO->> Step 431260 run_train: loss = 5.4124  (0.161 sec)
18-06-05 00:59-INFO->> Step 431270 run_train: loss = 5.4235  (0.145 sec)
18-06-05 00:59-INFO->> Step 431280 run_train: loss = 5.3959  (0.141 sec)
18-06-05 00:59-INFO->> Step 431290 run_train: loss = 5.4331  (0.210 sec)
18-06-05 00:59-INFO->> Step 431300 run_train: loss = 5.4066  (0.125 sec)
18-06-05 00:59-INFO->> Step 431310 run_train: loss = 5.4248  (0.131 sec)
18-06-05 00:59-INFO->> Step 431320 run_train: loss = 5.4441  (0.149 sec)
18-06-05 00:59-INFO->> Step 431330 run_train: loss = 5.4544  (0.170 sec)
18-06-05 00:59-INFO->> Step 431340 run_train: loss = 5.4097  (0.183 sec)
18-06-05 00:59-INFO->> Step 431350 run_train: loss = 5.3928  (0.169 sec)
18-06-05 00:59-INFO->> Step 431360 run_train: loss = 5.4188  (0.150 sec)
18-06-05 00:59-INFO->> Step 431370 run_train: loss = 5.4574  (0.149 sec)
18-06-05 00:59-INFO->> Step 431380 run_train: loss = 5.4478  (0.180 sec)
18-06-05 00:59-INFO->> Step 431390 run_train: loss = 5.4313  (0.123 sec)
18-06-05 00:59-INFO->> Step 431400 run_train: loss = 5.4244  (0.163 sec)
18-06-05 01:00-INFO->> Step 431410 run_train: loss = 5.3798  (0.132 sec)
18-06-05 01:00-INFO->> Step 431420 run_train: loss = 5.4960  (0.157 sec)
18-06-05 01:00-INFO->> Step 431430 run_train: loss = 5.4361  (0.185 sec)
18-06-05 01:00-INFO->> Step 431440 run_train: loss = 5.4281  (0.176 sec)
18-06-05 01:00-INFO->> Step 431450 run_train: loss = 5.3710  (0.159 sec)
18-06-05 01:00-INFO->> Step 431460 run_train: loss = 5.4100  (0.173 sec)
18-06-05 01:00-INFO->> Step 431470 run_train: loss = 5.3750  (0.154 sec)
18-06-05 01:00-INFO->> Step 431480 run_train: loss = 5.4104  (0.170 sec)
18-06-05 01:00-INFO->> Step 431490 run_train: loss = 5.4785  (0.166 sec)
18-06-05 01:00-INFO->> Step 431500 run_train: loss = 5.3637  (0.169 sec)
18-06-05 01:00-INFO->> Step 431510 run_train: loss = 5.3746  (0.137 sec)
18-06-05 01:00-INFO->> Step 431520 run_train: loss = 5.4222  (0.142 sec)
18-06-05 01:00-INFO->> Step 431530 run_train: loss = 5.3610  (0.128 sec)
18-06-05 01:00-INFO->> Step 431540 run_train: loss = 5.3988  (0.174 sec)
18-06-05 01:00-INFO->> Step 431550 run_train: loss = 5.4184  (0.151 sec)
18-06-05 01:00-INFO->> Step 431560 run_train: loss = 5.4816  (0.141 sec)
18-06-05 01:00-INFO->> Step 431570 run_train: loss = 5.4025  (0.166 sec)
18-06-05 01:00-INFO->> Step 431580 run_train: loss = 5.4165  (0.168 sec)
18-06-05 01:00-INFO->> Step 431590 run_train: loss = 5.3862  (0.171 sec)
18-06-05 01:00-INFO->> Step 431600 run_train: loss = 5.3659  (0.137 sec)
18-06-05 01:00-INFO->> Step 431610 run_train: loss = 5.3793  (0.173 sec)
18-06-05 01:00-INFO->> Step 431620 run_train: loss = 5.4571  (0.184 sec)
18-06-05 01:00-INFO->> Step 431630 run_train: loss = 5.4298  (0.139 sec)
18-06-05 01:00-INFO->> Step 431640 run_train: loss = 5.3598  (0.178 sec)
18-06-05 01:00-INFO->> Step 431650 run_train: loss = 5.4834  (0.183 sec)
18-06-05 01:00-INFO->> Step 431660 run_train: loss = 5.4063  (0.149 sec)
18-06-05 01:00-INFO->> Step 431670 run_train: loss = 5.3990  (0.161 sec)
18-06-05 01:00-INFO->> Step 431680 run_train: loss = 5.4045  (0.155 sec)
18-06-05 01:00-INFO->> Step 431690 run_train: loss = 5.3690  (0.160 sec)
18-06-05 01:00-INFO->> Step 431700 run_train: loss = 5.4291  (0.173 sec)
18-06-05 01:00-INFO->> Step 431710 run_train: loss = 5.4093  (0.178 sec)
18-06-05 01:00-INFO->> Step 431720 run_train: loss = 5.3369  (0.168 sec)
18-06-05 01:00-INFO->> Step 431730 run_train: loss = 5.3364  (0.175 sec)
18-06-05 01:00-INFO->> Step 431740 run_train: loss = 5.3218  (0.148 sec)
18-06-05 01:00-INFO->> Step 431750 run_train: loss = 5.4170  (0.156 sec)
18-06-05 01:00-INFO->> Step 431760 run_train: loss = 5.3951  (0.186 sec)
18-06-05 01:00-INFO->> Step 431770 run_train: loss = 5.4179  (0.142 sec)
18-06-05 01:01-INFO->> Step 431780 run_train: loss = 5.4060  (0.158 sec)
18-06-05 01:01-INFO->> Step 431790 run_train: loss = 5.4359  (0.156 sec)
18-06-05 01:01-INFO->> Step 431800 run_train: loss = 5.3619  (0.169 sec)
18-06-05 01:01-INFO->> Step 431810 run_train: loss = 5.4187  (0.164 sec)
18-06-05 01:01-INFO->> Step 431820 run_train: loss = 5.3989  (0.143 sec)
18-06-05 01:01-INFO->> Step 431830 run_train: loss = 5.3944  (0.146 sec)
18-06-05 01:01-INFO->> Step 431840 run_train: loss = 5.4510  (0.155 sec)
18-06-05 01:01-INFO->> Step 431850 run_train: loss = 5.3883  (0.150 sec)
18-06-05 01:01-INFO->> Step 431860 run_train: loss = 5.4478  (0.161 sec)
18-06-05 01:01-INFO->> Step 431870 run_train: loss = 5.3607  (0.160 sec)
18-06-05 01:01-INFO->> Step 431880 run_train: loss = 5.3535  (0.157 sec)
18-06-05 01:01-INFO->> Step 431890 run_train: loss = 5.4117  (0.164 sec)
18-06-05 01:01-INFO->> Step 431900 run_train: loss = 5.4275  (0.202 sec)
18-06-05 01:01-INFO->> Step 431910 run_train: loss = 5.4064  (0.143 sec)
18-06-05 01:01-INFO->> Step 431920 run_train: loss = 5.4513  (0.192 sec)
18-06-05 01:01-INFO->> Step 431930 run_train: loss = 5.3962  (0.163 sec)
18-06-05 01:01-INFO->> Step 431940 run_train: loss = 5.4100  (0.157 sec)
18-06-05 01:01-INFO->> Step 431950 run_train: loss = 5.4217  (0.143 sec)
18-06-05 01:01-INFO->> Step 431960 run_train: loss = 5.3582  (0.155 sec)
18-06-05 01:01-INFO->> Step 431970 run_train: loss = 5.4530  (0.181 sec)
18-06-05 01:01-INFO->> Step 431980 run_train: loss = 5.4314  (0.188 sec)
18-06-05 01:01-INFO->> Step 431990 run_train: loss = 5.4700  (0.160 sec)
18-06-05 01:01-INFO->> Step 432000 run_train: loss = 5.4677  (0.162 sec)
18-06-05 01:01-INFO->> 2018-06-05 01:01:35.149132 Saving in ckpt
18-06-05 01:01-INFO-Test Data Eval:
18-06-05 01:02-INFO-fpr95 = 0.17473930658873538 and auc = 0.9690343063417166
18-06-05 01:02-INFO->> Step 432010 run_train: loss = 5.4524  (0.177 sec)
18-06-05 01:02-INFO->> Step 432020 run_train: loss = 5.4414  (0.175 sec)
18-06-05 01:02-INFO->> Step 432030 run_train: loss = 5.4663  (0.153 sec)
18-06-05 01:02-INFO->> Step 432040 run_train: loss = 5.4065  (0.155 sec)
18-06-05 01:02-INFO->> Step 432050 run_train: loss = 5.4706  (0.142 sec)
18-06-05 01:02-INFO->> Step 432060 run_train: loss = 5.4960  (0.157 sec)
18-06-05 01:02-INFO->> Step 432070 run_train: loss = 5.3324  (0.127 sec)
18-06-05 01:02-INFO->> Step 432080 run_train: loss = 5.3721  (0.149 sec)
18-06-05 01:02-INFO->> Step 432090 run_train: loss = 5.4167  (0.147 sec)
18-06-05 01:02-INFO->> Step 432100 run_train: loss = 5.4384  (0.166 sec)
18-06-05 01:02-INFO->> Step 432110 run_train: loss = 5.4581  (0.165 sec)
18-06-05 01:02-INFO->> Step 432120 run_train: loss = 5.4533  (0.168 sec)
18-06-05 01:02-INFO->> Step 432130 run_train: loss = 5.3987  (0.199 sec)
18-06-05 01:02-INFO->> Step 432140 run_train: loss = 5.4241  (0.154 sec)
18-06-05 01:02-INFO->> Step 432150 run_train: loss = 5.3992  (0.164 sec)
18-06-05 01:02-INFO->> Step 432160 run_train: loss = 5.3917  (0.161 sec)
18-06-05 01:02-INFO->> Step 432170 run_train: loss = 5.3685  (0.149 sec)
18-06-05 01:02-INFO->> Step 432180 run_train: loss = 5.3866  (0.152 sec)
18-06-05 01:02-INFO->> Step 432190 run_train: loss = 5.3585  (0.144 sec)
18-06-05 01:02-INFO->> Step 432200 run_train: loss = 5.3978  (0.127 sec)
18-06-05 01:02-INFO->> Step 432210 run_train: loss = 5.4134  (0.176 sec)
18-06-05 01:02-INFO->> Step 432220 run_train: loss = 5.4417  (0.174 sec)
18-06-05 01:02-INFO->> Step 432230 run_train: loss = 5.4371  (0.123 sec)
18-06-05 01:02-INFO->> Step 432240 run_train: loss = 5.4040  (0.181 sec)
18-06-05 01:02-INFO->> Step 432250 run_train: loss = 5.3882  (0.169 sec)
18-06-05 01:02-INFO->> Step 432260 run_train: loss = 5.4136  (0.162 sec)
18-06-05 01:02-INFO->> Step 432270 run_train: loss = 5.2992  (0.127 sec)
18-06-05 01:02-INFO->> Step 432280 run_train: loss = 5.4048  (0.147 sec)
18-06-05 01:03-INFO->> Step 432290 run_train: loss = 5.3861  (0.150 sec)
18-06-05 01:03-INFO->> Step 432300 run_train: loss = 5.3931  (0.142 sec)
18-06-05 01:03-INFO->> Step 432310 run_train: loss = 5.3758  (0.147 sec)
18-06-05 01:03-INFO->> Step 432320 run_train: loss = 5.4726  (0.187 sec)
18-06-05 01:03-INFO->> Step 432330 run_train: loss = 5.4627  (0.171 sec)
18-06-05 01:03-INFO->> Step 432340 run_train: loss = 5.4277  (0.143 sec)
18-06-05 01:03-INFO->> Step 432350 run_train: loss = 5.4164  (0.161 sec)
18-06-05 01:03-INFO->> Step 432360 run_train: loss = 5.3878  (0.146 sec)
18-06-05 01:03-INFO->> Step 432370 run_train: loss = 5.3706  (0.146 sec)
18-06-05 01:03-INFO->> Step 432380 run_train: loss = 5.3440  (0.160 sec)
18-06-05 01:03-INFO->> Step 432390 run_train: loss = 5.3869  (0.145 sec)
18-06-05 01:03-INFO->> Step 432400 run_train: loss = 5.4066  (0.131 sec)
18-06-05 01:03-INFO->> Step 432410 run_train: loss = 5.4235  (0.171 sec)
18-06-05 01:03-INFO->> Step 432420 run_train: loss = 5.4282  (0.165 sec)
18-06-05 01:03-INFO->> Step 432430 run_train: loss = 5.4782  (0.152 sec)
18-06-05 01:03-INFO->> Step 432440 run_train: loss = 5.4225  (0.154 sec)
18-06-05 01:03-INFO->> Step 432450 run_train: loss = 5.3894  (0.142 sec)
18-06-05 01:03-INFO->> Step 432460 run_train: loss = 5.3963  (0.179 sec)
18-06-05 01:03-INFO->> Step 432470 run_train: loss = 5.4761  (0.175 sec)
18-06-05 01:03-INFO->> Step 432480 run_train: loss = 5.4134  (0.164 sec)
18-06-05 01:03-INFO->> Step 432490 run_train: loss = 5.3811  (0.178 sec)
18-06-05 01:03-INFO->> Step 432500 run_train: loss = 5.3595  (0.172 sec)
18-06-05 01:03-INFO->> Step 432510 run_train: loss = 5.4097  (0.136 sec)
18-06-05 01:03-INFO->> Step 432520 run_train: loss = 5.4275  (0.157 sec)
18-06-05 01:03-INFO->> Step 432530 run_train: loss = 5.4313  (0.156 sec)
18-06-05 01:03-INFO->> Step 432540 run_train: loss = 5.4095  (0.142 sec)
18-06-05 01:03-INFO->> Step 432550 run_train: loss = 5.4188  (0.180 sec)
18-06-05 01:03-INFO->> Step 432560 run_train: loss = 5.3748  (0.159 sec)
18-06-05 01:03-INFO->> Step 432570 run_train: loss = 5.4219  (0.143 sec)
18-06-05 01:03-INFO->> Step 432580 run_train: loss = 5.3828  (0.144 sec)
18-06-05 01:03-INFO->> Step 432590 run_train: loss = 5.4332  (0.158 sec)
18-06-05 01:03-INFO->> Step 432600 run_train: loss = 5.4100  (0.183 sec)
18-06-05 01:03-INFO->> Step 432610 run_train: loss = 5.3894  (0.183 sec)
18-06-05 01:03-INFO->> Step 432620 run_train: loss = 5.4282  (0.179 sec)
18-06-05 01:03-INFO->> Step 432630 run_train: loss = 5.4260  (0.157 sec)
18-06-05 01:03-INFO->> Step 432640 run_train: loss = 5.4301  (0.177 sec)
18-06-05 01:03-INFO->> Step 432650 run_train: loss = 5.4497  (0.149 sec)
18-06-05 01:04-INFO->> Step 432660 run_train: loss = 5.4612  (0.163 sec)
18-06-05 01:04-INFO->> Step 432670 run_train: loss = 5.3607  (0.158 sec)
18-06-05 01:04-INFO->> Step 432680 run_train: loss = 5.4682  (0.147 sec)
18-06-05 01:04-INFO->> Step 432690 run_train: loss = 5.3634  (0.163 sec)
18-06-05 01:04-INFO->> Step 432700 run_train: loss = 5.4967  (0.158 sec)
18-06-05 01:04-INFO->> Step 432710 run_train: loss = 5.3999  (0.142 sec)
18-06-05 01:04-INFO->> Step 432720 run_train: loss = 5.4576  (0.188 sec)
18-06-05 01:04-INFO->> Step 432730 run_train: loss = 5.4036  (0.150 sec)
18-06-05 01:04-INFO->> Step 432740 run_train: loss = 5.4619  (0.169 sec)
18-06-05 01:04-INFO->> Step 432750 run_train: loss = 5.4137  (0.148 sec)
18-06-05 01:04-INFO->> Step 432760 run_train: loss = 5.4123  (0.114 sec)
18-06-05 01:04-INFO->> Step 432770 run_train: loss = 5.4043  (0.136 sec)
18-06-05 01:04-INFO->> Step 432780 run_train: loss = 5.4611  (0.146 sec)
18-06-05 01:04-INFO->> Step 432790 run_train: loss = 5.3862  (0.157 sec)
18-06-05 01:04-INFO->> Step 432800 run_train: loss = 5.4406  (0.161 sec)
18-06-05 01:04-INFO->> Step 432810 run_train: loss = 5.4586  (0.205 sec)
18-06-05 01:04-INFO->> Step 432820 run_train: loss = 5.4280  (0.183 sec)
18-06-05 01:04-INFO->> Step 432830 run_train: loss = 5.4352  (0.192 sec)
18-06-05 01:04-INFO->> Step 432840 run_train: loss = 5.4495  (0.162 sec)
18-06-05 01:04-INFO->> Step 432850 run_train: loss = 5.3978  (0.169 sec)
18-06-05 01:04-INFO->> Step 432860 run_train: loss = 5.3709  (0.166 sec)
18-06-05 01:04-INFO->> Step 432870 run_train: loss = 5.3853  (0.173 sec)
18-06-05 01:04-INFO->> Step 432880 run_train: loss = 5.4093  (0.167 sec)
18-06-05 01:04-INFO->> Step 432890 run_train: loss = 5.4270  (0.184 sec)
18-06-05 01:04-INFO->> Step 432900 run_train: loss = 5.4518  (0.167 sec)
18-06-05 01:04-INFO->> Step 432910 run_train: loss = 5.4619  (0.132 sec)
18-06-05 01:04-INFO->> Step 432920 run_train: loss = 5.4785  (0.145 sec)
18-06-05 01:04-INFO->> Step 432930 run_train: loss = 5.4089  (0.180 sec)
18-06-05 01:04-INFO->> Step 432940 run_train: loss = 5.4783  (0.147 sec)
18-06-05 01:04-INFO->> Step 432950 run_train: loss = 5.4186  (0.161 sec)
18-06-05 01:04-INFO->> Step 432960 run_train: loss = 5.3852  (0.165 sec)
18-06-05 01:04-INFO->> Step 432970 run_train: loss = 5.4021  (0.182 sec)
18-06-05 01:04-INFO->> Step 432980 run_train: loss = 5.4015  (0.139 sec)
18-06-05 01:04-INFO->> Step 432990 run_train: loss = 5.4501  (0.181 sec)
18-06-05 01:04-INFO->> Step 433000 run_train: loss = 5.3973  (0.177 sec)
18-06-05 01:04-INFO->> 2018-06-05 01:04:54.177460 Saving in ckpt
18-06-05 01:04-INFO-Test Data Eval:
18-06-05 01:05-INFO-fpr95 = 0.17766172954303933 and auc = 0.9690276872109821
18-06-05 01:05-INFO->> Step 433010 run_train: loss = 5.3620  (0.151 sec)
18-06-05 01:05-INFO->> Step 433020 run_train: loss = 5.4042  (0.156 sec)
18-06-05 01:05-INFO->> Step 433030 run_train: loss = 5.4332  (0.152 sec)
18-06-05 01:05-INFO->> Step 433040 run_train: loss = 5.4384  (0.200 sec)
18-06-05 01:05-INFO->> Step 433050 run_train: loss = 5.4084  (0.178 sec)
18-06-05 01:05-INFO->> Step 433060 run_train: loss = 5.3994  (0.113 sec)
18-06-05 01:05-INFO->> Step 433070 run_train: loss = 5.3679  (0.161 sec)
18-06-05 01:05-INFO->> Step 433080 run_train: loss = 5.4226  (0.164 sec)
18-06-05 01:05-INFO->> Step 433090 run_train: loss = 5.3785  (0.143 sec)
18-06-05 01:05-INFO->> Step 433100 run_train: loss = 5.4397  (0.160 sec)
18-06-05 01:05-INFO->> Step 433110 run_train: loss = 5.3671  (0.161 sec)
18-06-05 01:05-INFO->> Step 433120 run_train: loss = 5.3982  (0.155 sec)
18-06-05 01:05-INFO->> Step 433130 run_train: loss = 5.3660  (0.165 sec)
18-06-05 01:05-INFO->> Step 433140 run_train: loss = 5.4014  (0.156 sec)
18-06-05 01:05-INFO->> Step 433150 run_train: loss = 5.3827  (0.133 sec)
18-06-05 01:05-INFO->> Step 433160 run_train: loss = 5.3410  (0.139 sec)
18-06-05 01:06-INFO->> Step 433170 run_train: loss = 5.3938  (0.170 sec)
18-06-05 01:06-INFO->> Step 433180 run_train: loss = 5.4479  (0.149 sec)
18-06-05 01:06-INFO->> Step 433190 run_train: loss = 5.4586  (0.171 sec)
18-06-05 01:06-INFO->> Step 433200 run_train: loss = 5.4209  (0.179 sec)
18-06-05 01:06-INFO->> Step 433210 run_train: loss = 5.4245  (0.141 sec)
18-06-05 01:06-INFO->> Step 433220 run_train: loss = 5.4182  (0.140 sec)
18-06-05 01:06-INFO->> Step 433230 run_train: loss = 5.4506  (0.133 sec)
18-06-05 01:06-INFO->> Step 433240 run_train: loss = 5.3797  (0.164 sec)
18-06-05 01:06-INFO->> Step 433250 run_train: loss = 5.4389  (0.175 sec)
18-06-05 01:06-INFO->> Step 433260 run_train: loss = 5.4592  (0.162 sec)
18-06-05 01:06-INFO->> Step 433270 run_train: loss = 5.4642  (0.147 sec)
18-06-05 01:06-INFO->> Step 433280 run_train: loss = 5.4563  (0.113 sec)
18-06-05 01:06-INFO->> Step 433290 run_train: loss = 5.4281  (0.183 sec)
18-06-05 01:06-INFO->> Step 433300 run_train: loss = 5.3769  (0.172 sec)
18-06-05 01:06-INFO->> Step 433310 run_train: loss = 5.4210  (0.168 sec)
18-06-05 01:06-INFO->> Step 433320 run_train: loss = 5.4053  (0.150 sec)
18-06-05 01:06-INFO->> Step 433330 run_train: loss = 5.4056  (0.167 sec)
18-06-05 01:06-INFO->> Step 433340 run_train: loss = 5.4054  (0.168 sec)
18-06-05 01:06-INFO->> Step 433350 run_train: loss = 5.4227  (0.121 sec)
18-06-05 01:06-INFO->> Step 433360 run_train: loss = 5.3807  (0.155 sec)
18-06-05 01:06-INFO->> Step 433370 run_train: loss = 5.4463  (0.165 sec)
18-06-05 01:06-INFO->> Step 433380 run_train: loss = 5.4486  (0.188 sec)
18-06-05 01:06-INFO->> Step 433390 run_train: loss = 5.3148  (0.165 sec)
18-06-05 01:06-INFO->> Step 433400 run_train: loss = 5.4170  (0.140 sec)
18-06-05 01:06-INFO->> Step 433410 run_train: loss = 5.4752  (0.166 sec)
18-06-05 01:06-INFO->> Step 433420 run_train: loss = 5.4055  (0.132 sec)
18-06-05 01:06-INFO->> Step 433430 run_train: loss = 5.4395  (0.137 sec)
18-06-05 01:06-INFO->> Step 433440 run_train: loss = 5.4049  (0.154 sec)
18-06-05 01:06-INFO->> Step 433450 run_train: loss = 5.3901  (0.156 sec)
18-06-05 01:06-INFO->> Step 433460 run_train: loss = 5.4214  (0.166 sec)
18-06-05 01:06-INFO->> Step 433470 run_train: loss = 5.3875  (0.116 sec)
18-06-05 01:06-INFO->> Step 433480 run_train: loss = 5.4058  (0.177 sec)
18-06-05 01:06-INFO->> Step 433490 run_train: loss = 5.3679  (0.125 sec)
18-06-05 01:06-INFO->> Step 433500 run_train: loss = 5.4061  (0.151 sec)
18-06-05 01:06-INFO->> Step 433510 run_train: loss = 5.3543  (0.153 sec)
18-06-05 01:06-INFO->> Step 433520 run_train: loss = 5.4485  (0.145 sec)
18-06-05 01:06-INFO->> Step 433530 run_train: loss = 5.4539  (0.153 sec)
18-06-05 01:07-INFO->> Step 433540 run_train: loss = 5.4228  (0.153 sec)
18-06-05 01:07-INFO->> Step 433550 run_train: loss = 5.4494  (0.150 sec)
18-06-05 01:07-INFO->> Step 433560 run_train: loss = 5.4613  (0.146 sec)
18-06-05 01:07-INFO->> Step 433570 run_train: loss = 5.3868  (0.183 sec)
18-06-05 01:07-INFO->> Step 433580 run_train: loss = 5.4272  (0.184 sec)
18-06-05 01:07-INFO->> Step 433590 run_train: loss = 5.4400  (0.160 sec)
18-06-05 01:07-INFO->> Step 433600 run_train: loss = 5.4294  (0.150 sec)
18-06-05 01:07-INFO->> Step 433610 run_train: loss = 5.4569  (0.149 sec)
18-06-05 01:07-INFO->> Step 433620 run_train: loss = 5.3839  (0.122 sec)
18-06-05 01:07-INFO->> Step 433630 run_train: loss = 5.3957  (0.157 sec)
18-06-05 01:07-INFO->> Step 433640 run_train: loss = 5.4538  (0.165 sec)
18-06-05 01:07-INFO->> Step 433650 run_train: loss = 5.3826  (0.169 sec)
18-06-05 01:07-INFO->> Step 433660 run_train: loss = 5.3113  (0.167 sec)
18-06-05 01:07-INFO->> Step 433670 run_train: loss = 5.4455  (0.166 sec)
18-06-05 01:07-INFO->> Step 433680 run_train: loss = 5.4571  (0.162 sec)
18-06-05 01:07-INFO->> Step 433690 run_train: loss = 5.4287  (0.190 sec)
18-06-05 01:07-INFO->> Step 433700 run_train: loss = 5.4012  (0.141 sec)
18-06-05 01:07-INFO->> Step 433710 run_train: loss = 5.3918  (0.145 sec)
18-06-05 01:07-INFO->> Step 433720 run_train: loss = 5.4130  (0.128 sec)
18-06-05 01:07-INFO->> Step 433730 run_train: loss = 5.3787  (0.131 sec)
18-06-05 01:07-INFO->> Step 433740 run_train: loss = 5.4459  (0.155 sec)
18-06-05 01:07-INFO->> Step 433750 run_train: loss = 5.4022  (0.162 sec)
18-06-05 01:07-INFO->> Step 433760 run_train: loss = 5.4671  (0.153 sec)
18-06-05 01:07-INFO->> Step 433770 run_train: loss = 5.4126  (0.152 sec)
18-06-05 01:07-INFO->> Step 433780 run_train: loss = 5.3586  (0.163 sec)
18-06-05 01:07-INFO->> Step 433790 run_train: loss = 5.4420  (0.159 sec)
18-06-05 01:07-INFO->> Step 433800 run_train: loss = 5.3682  (0.144 sec)
18-06-05 01:07-INFO->> Step 433810 run_train: loss = 5.4545  (0.184 sec)
18-06-05 01:07-INFO->> Step 433820 run_train: loss = 5.3988  (0.138 sec)
18-06-05 01:07-INFO->> Step 433830 run_train: loss = 5.4251  (0.157 sec)
18-06-05 01:07-INFO->> Step 433840 run_train: loss = 5.4661  (0.180 sec)
18-06-05 01:07-INFO->> Step 433850 run_train: loss = 5.3860  (0.177 sec)
18-06-05 01:07-INFO->> Step 433860 run_train: loss = 5.4103  (0.159 sec)
18-06-05 01:07-INFO->> Step 433870 run_train: loss = 5.3648  (0.156 sec)
18-06-05 01:07-INFO->> Step 433880 run_train: loss = 5.3577  (0.168 sec)
18-06-05 01:07-INFO->> Step 433890 run_train: loss = 5.4629  (0.210 sec)
18-06-05 01:07-INFO->> Step 433900 run_train: loss = 5.4229  (0.164 sec)
18-06-05 01:07-INFO->> Step 433910 run_train: loss = 5.4227  (0.190 sec)
18-06-05 01:08-INFO->> Step 433920 run_train: loss = 5.4264  (0.166 sec)
18-06-05 01:08-INFO->> Step 433930 run_train: loss = 5.3990  (0.164 sec)
18-06-05 01:08-INFO->> Step 433940 run_train: loss = 5.4138  (0.171 sec)
18-06-05 01:08-INFO->> Step 433950 run_train: loss = 5.4590  (0.154 sec)
18-06-05 01:08-INFO->> Step 433960 run_train: loss = 5.4575  (0.176 sec)
18-06-05 01:08-INFO->> Step 433970 run_train: loss = 5.4109  (0.181 sec)
18-06-05 01:08-INFO->> Step 433980 run_train: loss = 5.3610  (0.134 sec)
18-06-05 01:08-INFO->> Step 433990 run_train: loss = 5.4358  (0.167 sec)
18-06-05 01:08-INFO->> Step 434000 run_train: loss = 5.3733  (0.131 sec)
18-06-05 01:08-INFO->> 2018-06-05 01:08:14.039561 Saving in ckpt
18-06-05 01:08-INFO-Test Data Eval:
18-06-05 01:08-INFO-fpr95 = 0.17121081296493093 and auc = 0.9694531783426316
18-06-05 01:08-INFO->> Step 434010 run_train: loss = 5.4090  (0.155 sec)
18-06-05 01:08-INFO->> Step 434020 run_train: loss = 5.3963  (0.143 sec)
18-06-05 01:08-INFO->> Step 434030 run_train: loss = 5.4047  (0.159 sec)
18-06-05 01:09-INFO->> Step 434040 run_train: loss = 5.3423  (0.177 sec)
18-06-05 01:09-INFO->> Step 434050 run_train: loss = 5.4177  (0.188 sec)
18-06-05 01:09-INFO->> Step 434060 run_train: loss = 5.3882  (0.163 sec)
18-06-05 01:09-INFO->> Step 434070 run_train: loss = 5.4507  (0.162 sec)
18-06-05 01:09-INFO->> Step 434080 run_train: loss = 5.3987  (0.157 sec)
18-06-05 01:09-INFO->> Step 434090 run_train: loss = 5.3995  (0.183 sec)
18-06-05 01:09-INFO->> Step 434100 run_train: loss = 5.4359  (0.202 sec)
18-06-05 01:09-INFO->> Step 434110 run_train: loss = 5.4910  (0.149 sec)
18-06-05 01:09-INFO->> Step 434120 run_train: loss = 5.4141  (0.158 sec)
18-06-05 01:09-INFO->> Step 434130 run_train: loss = 5.3772  (0.155 sec)
18-06-05 01:09-INFO->> Step 434140 run_train: loss = 5.3951  (0.176 sec)
18-06-05 01:09-INFO->> Step 434150 run_train: loss = 5.4481  (0.133 sec)
18-06-05 01:09-INFO->> Step 434160 run_train: loss = 5.4695  (0.161 sec)
18-06-05 01:09-INFO->> Step 434170 run_train: loss = 5.4367  (0.155 sec)
18-06-05 01:09-INFO->> Step 434180 run_train: loss = 5.3570  (0.173 sec)
18-06-05 01:09-INFO->> Step 434190 run_train: loss = 5.4377  (0.138 sec)
18-06-05 01:09-INFO->> Step 434200 run_train: loss = 5.4310  (0.209 sec)
18-06-05 01:09-INFO->> Step 434210 run_train: loss = 5.4461  (0.172 sec)
18-06-05 01:09-INFO->> Step 434220 run_train: loss = 5.4459  (0.143 sec)
18-06-05 01:09-INFO->> Step 434230 run_train: loss = 5.4304  (0.149 sec)
18-06-05 01:09-INFO->> Step 434240 run_train: loss = 5.4396  (0.132 sec)
18-06-05 01:09-INFO->> Step 434250 run_train: loss = 5.3937  (0.180 sec)
18-06-05 01:09-INFO->> Step 434260 run_train: loss = 5.4505  (0.159 sec)
18-06-05 01:09-INFO->> Step 434270 run_train: loss = 5.3735  (0.109 sec)
18-06-05 01:09-INFO->> Step 434280 run_train: loss = 5.4431  (0.194 sec)
18-06-05 01:09-INFO->> Step 434290 run_train: loss = 5.4090  (0.152 sec)
18-06-05 01:09-INFO->> Step 434300 run_train: loss = 5.3699  (0.155 sec)
18-06-05 01:09-INFO->> Step 434310 run_train: loss = 5.4330  (0.147 sec)
18-06-05 01:09-INFO->> Step 434320 run_train: loss = 5.3897  (0.178 sec)
18-06-05 01:09-INFO->> Step 434330 run_train: loss = 5.3683  (0.132 sec)
18-06-05 01:09-INFO->> Step 434340 run_train: loss = 5.4934  (0.155 sec)
18-06-05 01:09-INFO->> Step 434350 run_train: loss = 5.4091  (0.172 sec)
18-06-05 01:09-INFO->> Step 434360 run_train: loss = 5.4120  (0.157 sec)
18-06-05 01:09-INFO->> Step 434370 run_train: loss = 5.4696  (0.146 sec)
18-06-05 01:09-INFO->> Step 434380 run_train: loss = 5.4382  (0.168 sec)
18-06-05 01:09-INFO->> Step 434390 run_train: loss = 5.4186  (0.147 sec)
18-06-05 01:09-INFO->> Step 434400 run_train: loss = 5.4376  (0.130 sec)
18-06-05 01:09-INFO->> Step 434410 run_train: loss = 5.3958  (0.142 sec)
18-06-05 01:10-INFO->> Step 434420 run_train: loss = 5.3783  (0.196 sec)
18-06-05 01:10-INFO->> Step 434430 run_train: loss = 5.4083  (0.155 sec)
18-06-05 01:10-INFO->> Step 434440 run_train: loss = 5.4225  (0.153 sec)
18-06-05 01:10-INFO->> Step 434450 run_train: loss = 5.4488  (0.160 sec)
18-06-05 01:10-INFO->> Step 434460 run_train: loss = 5.4136  (0.153 sec)
18-06-05 01:10-INFO->> Step 434470 run_train: loss = 5.3893  (0.178 sec)
18-06-05 01:10-INFO->> Step 434480 run_train: loss = 5.4659  (0.190 sec)
18-06-05 01:10-INFO->> Step 434490 run_train: loss = 5.4698  (0.156 sec)
18-06-05 01:10-INFO->> Step 434500 run_train: loss = 5.3720  (0.142 sec)
18-06-05 01:10-INFO->> Step 434510 run_train: loss = 5.3547  (0.138 sec)
18-06-05 01:10-INFO->> Step 434520 run_train: loss = 5.4266  (0.155 sec)
18-06-05 01:10-INFO->> Step 434530 run_train: loss = 5.4217  (0.195 sec)
18-06-05 01:10-INFO->> Step 434540 run_train: loss = 5.4233  (0.191 sec)
18-06-05 01:10-INFO->> Step 434550 run_train: loss = 5.3996  (0.138 sec)
18-06-05 01:10-INFO->> Step 434560 run_train: loss = 5.4166  (0.196 sec)
18-06-05 01:10-INFO->> Step 434570 run_train: loss = 5.4220  (0.130 sec)
18-06-05 01:10-INFO->> Step 434580 run_train: loss = 5.4635  (0.154 sec)
18-06-05 01:10-INFO->> Step 434590 run_train: loss = 5.4257  (0.157 sec)
18-06-05 01:10-INFO->> Step 434600 run_train: loss = 5.4575  (0.153 sec)
18-06-05 01:10-INFO->> Step 434610 run_train: loss = 5.4346  (0.170 sec)
18-06-05 01:10-INFO->> Step 434620 run_train: loss = 5.3710  (0.159 sec)
18-06-05 01:10-INFO->> Step 434630 run_train: loss = 5.3437  (0.141 sec)
18-06-05 01:10-INFO->> Step 434640 run_train: loss = 5.3844  (0.158 sec)
18-06-05 01:10-INFO->> Step 434650 run_train: loss = 5.4149  (0.148 sec)
18-06-05 01:10-INFO->> Step 434660 run_train: loss = 5.4128  (0.149 sec)
18-06-05 01:10-INFO->> Step 434670 run_train: loss = 5.4128  (0.174 sec)
18-06-05 01:10-INFO->> Step 434680 run_train: loss = 5.4382  (0.139 sec)
18-06-05 01:10-INFO->> Step 434690 run_train: loss = 5.4113  (0.197 sec)
18-06-05 01:10-INFO->> Step 434700 run_train: loss = 5.4300  (0.179 sec)
18-06-05 01:10-INFO->> Step 434710 run_train: loss = 5.4616  (0.135 sec)
18-06-05 01:10-INFO->> Step 434720 run_train: loss = 5.4790  (0.189 sec)
18-06-05 01:10-INFO->> Step 434730 run_train: loss = 5.3800  (0.214 sec)
18-06-05 01:10-INFO->> Step 434740 run_train: loss = 5.4914  (0.141 sec)
18-06-05 01:10-INFO->> Step 434750 run_train: loss = 5.3833  (0.162 sec)
18-06-05 01:10-INFO->> Step 434760 run_train: loss = 5.3943  (0.165 sec)
18-06-05 01:10-INFO->> Step 434770 run_train: loss = 5.4248  (0.197 sec)
18-06-05 01:10-INFO->> Step 434780 run_train: loss = 5.3803  (0.178 sec)
18-06-05 01:11-INFO->> Step 434790 run_train: loss = 5.4290  (0.157 sec)
18-06-05 01:11-INFO->> Step 434800 run_train: loss = 5.3898  (0.151 sec)
18-06-05 01:11-INFO->> Step 434810 run_train: loss = 5.4278  (0.176 sec)
18-06-05 01:11-INFO->> Step 434820 run_train: loss = 5.3369  (0.127 sec)
18-06-05 01:11-INFO->> Step 434830 run_train: loss = 5.4173  (0.151 sec)
18-06-05 01:11-INFO->> Step 434840 run_train: loss = 5.3652  (0.179 sec)
18-06-05 01:11-INFO->> Step 434850 run_train: loss = 5.4110  (0.154 sec)
18-06-05 01:11-INFO->> Step 434860 run_train: loss = 5.4462  (0.159 sec)
18-06-05 01:11-INFO->> Step 434870 run_train: loss = 5.4707  (0.145 sec)
18-06-05 01:11-INFO->> Step 434880 run_train: loss = 5.3790  (0.149 sec)
18-06-05 01:11-INFO->> Step 434890 run_train: loss = 5.3993  (0.183 sec)
18-06-05 01:11-INFO->> Step 434900 run_train: loss = 5.3516  (0.162 sec)
18-06-05 01:11-INFO->> Step 434910 run_train: loss = 5.3739  (0.140 sec)
18-06-05 01:11-INFO->> Step 434920 run_train: loss = 5.4209  (0.130 sec)
18-06-05 01:11-INFO->> Step 434930 run_train: loss = 5.3962  (0.172 sec)
18-06-05 01:11-INFO->> Step 434940 run_train: loss = 5.4729  (0.136 sec)
18-06-05 01:11-INFO->> Step 434950 run_train: loss = 5.4236  (0.190 sec)
18-06-05 01:11-INFO->> Step 434960 run_train: loss = 5.3955  (0.192 sec)
18-06-05 01:11-INFO->> Step 434970 run_train: loss = 5.4061  (0.130 sec)
18-06-05 01:11-INFO->> Step 434980 run_train: loss = 5.4270  (0.149 sec)
18-06-05 01:11-INFO->> Step 434990 run_train: loss = 5.4111  (0.161 sec)
18-06-05 01:11-INFO->> Step 435000 run_train: loss = 5.3791  (0.131 sec)
18-06-05 01:11-INFO->> 2018-06-05 01:11:33.534037 Saving in ckpt
18-06-05 01:11-INFO-Test Data Eval:
18-06-05 01:12-INFO-fpr95 = 0.17380114240170033 and auc = 0.9692422035063737
18-06-05 01:12-INFO->> Step 435010 run_train: loss = 5.4483  (0.118 sec)
18-06-05 01:12-INFO->> Step 435020 run_train: loss = 5.3732  (0.157 sec)
18-06-05 01:12-INFO->> Step 435030 run_train: loss = 5.3789  (0.157 sec)
18-06-05 01:12-INFO->> Step 435040 run_train: loss = 5.4115  (0.131 sec)
18-06-05 01:12-INFO->> Step 435050 run_train: loss = 5.4106  (0.138 sec)
18-06-05 01:12-INFO->> Step 435060 run_train: loss = 5.5029  (0.195 sec)
18-06-05 01:12-INFO->> Step 435070 run_train: loss = 5.4116  (0.186 sec)
18-06-05 01:12-INFO->> Step 435080 run_train: loss = 5.4255  (0.139 sec)
18-06-05 01:12-INFO->> Step 435090 run_train: loss = 5.3809  (0.188 sec)
18-06-05 01:12-INFO->> Step 435100 run_train: loss = 5.4648  (0.162 sec)
18-06-05 01:12-INFO->> Step 435110 run_train: loss = 5.4086  (0.146 sec)
18-06-05 01:12-INFO->> Step 435120 run_train: loss = 5.4679  (0.191 sec)
18-06-05 01:12-INFO->> Step 435130 run_train: loss = 5.4073  (0.156 sec)
18-06-05 01:12-INFO->> Step 435140 run_train: loss = 5.4901  (0.152 sec)
18-06-05 01:12-INFO->> Step 435150 run_train: loss = 5.4244  (0.157 sec)
18-06-05 01:12-INFO->> Step 435160 run_train: loss = 5.3567  (0.145 sec)
18-06-05 01:12-INFO->> Step 435170 run_train: loss = 5.4178  (0.136 sec)
18-06-05 01:12-INFO->> Step 435180 run_train: loss = 5.4030  (0.147 sec)
18-06-05 01:12-INFO->> Step 435190 run_train: loss = 5.4212  (0.176 sec)
18-06-05 01:12-INFO->> Step 435200 run_train: loss = 5.3928  (0.144 sec)
18-06-05 01:12-INFO->> Step 435210 run_train: loss = 5.3666  (0.192 sec)
18-06-05 01:12-INFO->> Step 435220 run_train: loss = 5.3377  (0.195 sec)
18-06-05 01:12-INFO->> Step 435230 run_train: loss = 5.4131  (0.146 sec)
18-06-05 01:12-INFO->> Step 435240 run_train: loss = 5.4266  (0.152 sec)
18-06-05 01:12-INFO->> Step 435250 run_train: loss = 5.3798  (0.145 sec)
18-06-05 01:12-INFO->> Step 435260 run_train: loss = 5.3606  (0.168 sec)
18-06-05 01:12-INFO->> Step 435270 run_train: loss = 5.4487  (0.172 sec)
18-06-05 01:12-INFO->> Step 435280 run_train: loss = 5.4542  (0.180 sec)
18-06-05 01:12-INFO->> Step 435290 run_train: loss = 5.3773  (0.183 sec)
18-06-05 01:13-INFO->> Step 435300 run_train: loss = 5.4076  (0.140 sec)
18-06-05 01:13-INFO->> Step 435310 run_train: loss = 5.4703  (0.215 sec)
18-06-05 01:13-INFO->> Step 435320 run_train: loss = 5.3774  (0.156 sec)
18-06-05 01:13-INFO->> Step 435330 run_train: loss = 5.4193  (0.133 sec)
18-06-05 01:13-INFO->> Step 435340 run_train: loss = 5.4142  (0.168 sec)
18-06-05 01:13-INFO->> Step 435350 run_train: loss = 5.4001  (0.191 sec)
18-06-05 01:13-INFO->> Step 435360 run_train: loss = 5.4949  (0.124 sec)
18-06-05 01:13-INFO->> Step 435370 run_train: loss = 5.3975  (0.155 sec)
18-06-05 01:13-INFO->> Step 435380 run_train: loss = 5.3470  (0.147 sec)
18-06-05 01:13-INFO->> Step 435390 run_train: loss = 5.5273  (0.172 sec)
18-06-05 01:13-INFO->> Step 435400 run_train: loss = 5.3966  (0.181 sec)
18-06-05 01:13-INFO->> Step 435410 run_train: loss = 5.4367  (0.152 sec)
18-06-05 01:13-INFO->> Step 435420 run_train: loss = 5.4116  (0.170 sec)
18-06-05 01:13-INFO->> Step 435430 run_train: loss = 5.4363  (0.184 sec)
18-06-05 01:13-INFO->> Step 435440 run_train: loss = 5.4269  (0.155 sec)
18-06-05 01:13-INFO->> Step 435450 run_train: loss = 5.4765  (0.185 sec)
18-06-05 01:13-INFO->> Step 435460 run_train: loss = 5.4368  (0.156 sec)
18-06-05 01:13-INFO->> Step 435470 run_train: loss = 5.4172  (0.171 sec)
18-06-05 01:13-INFO->> Step 435480 run_train: loss = 5.4771  (0.179 sec)
18-06-05 01:13-INFO->> Step 435490 run_train: loss = 5.3661  (0.134 sec)
18-06-05 01:13-INFO->> Step 435500 run_train: loss = 5.3754  (0.160 sec)
18-06-05 01:13-INFO->> Step 435510 run_train: loss = 5.4027  (0.166 sec)
18-06-05 01:13-INFO->> Step 435520 run_train: loss = 5.4054  (0.155 sec)
18-06-05 01:13-INFO->> Step 435530 run_train: loss = 5.3614  (0.177 sec)
18-06-05 01:13-INFO->> Step 435540 run_train: loss = 5.4425  (0.117 sec)
18-06-05 01:13-INFO->> Step 435550 run_train: loss = 5.4473  (0.153 sec)
18-06-05 01:13-INFO->> Step 435560 run_train: loss = 5.3928  (0.154 sec)
18-06-05 01:13-INFO->> Step 435570 run_train: loss = 5.5357  (0.118 sec)
18-06-05 01:13-INFO->> Step 435580 run_train: loss = 5.4341  (0.154 sec)
18-06-05 01:13-INFO->> Step 435590 run_train: loss = 5.3648  (0.132 sec)
18-06-05 01:13-INFO->> Step 435600 run_train: loss = 5.4588  (0.123 sec)
18-06-05 01:13-INFO->> Step 435610 run_train: loss = 5.3905  (0.127 sec)
18-06-05 01:13-INFO->> Step 435620 run_train: loss = 5.4188  (0.151 sec)
18-06-05 01:13-INFO->> Step 435630 run_train: loss = 5.4018  (0.154 sec)
18-06-05 01:13-INFO->> Step 435640 run_train: loss = 5.3897  (0.120 sec)
18-06-05 01:13-INFO->> Step 435650 run_train: loss = 5.4521  (0.198 sec)
18-06-05 01:13-INFO->> Step 435660 run_train: loss = 5.3904  (0.153 sec)
18-06-05 01:14-INFO->> Step 435670 run_train: loss = 5.4181  (0.125 sec)
18-06-05 01:14-INFO->> Step 435680 run_train: loss = 5.3828  (0.186 sec)
18-06-05 01:14-INFO->> Step 435690 run_train: loss = 5.4436  (0.122 sec)
18-06-05 01:14-INFO->> Step 435700 run_train: loss = 5.3596  (0.145 sec)
18-06-05 01:14-INFO->> Step 435710 run_train: loss = 5.4018  (0.179 sec)
18-06-05 01:14-INFO->> Step 435720 run_train: loss = 5.3754  (0.140 sec)
18-06-05 01:14-INFO->> Step 435730 run_train: loss = 5.4433  (0.125 sec)
18-06-05 01:14-INFO->> Step 435740 run_train: loss = 5.3741  (0.171 sec)
18-06-05 01:14-INFO->> Step 435750 run_train: loss = 5.4384  (0.156 sec)
18-06-05 01:14-INFO->> Step 435760 run_train: loss = 5.3954  (0.138 sec)
18-06-05 01:14-INFO->> Step 435770 run_train: loss = 5.4540  (0.148 sec)
18-06-05 01:14-INFO->> Step 435780 run_train: loss = 5.4537  (0.157 sec)
18-06-05 01:14-INFO->> Step 435790 run_train: loss = 5.4397  (0.181 sec)
18-06-05 01:14-INFO->> Step 435800 run_train: loss = 5.4638  (0.161 sec)
18-06-05 01:14-INFO->> Step 435810 run_train: loss = 5.4408  (0.135 sec)
18-06-05 01:14-INFO->> Step 435820 run_train: loss = 5.3890  (0.176 sec)
18-06-05 01:14-INFO->> Step 435830 run_train: loss = 5.4220  (0.168 sec)
18-06-05 01:14-INFO->> Step 435840 run_train: loss = 5.3919  (0.183 sec)
18-06-05 01:14-INFO->> Step 435850 run_train: loss = 5.4116  (0.173 sec)
18-06-05 01:14-INFO->> Step 435860 run_train: loss = 5.3923  (0.139 sec)
18-06-05 01:14-INFO->> Step 435870 run_train: loss = 5.3882  (0.177 sec)
18-06-05 01:14-INFO->> Step 435880 run_train: loss = 5.3906  (0.157 sec)
18-06-05 01:14-INFO->> Step 435890 run_train: loss = 5.3979  (0.158 sec)
18-06-05 01:14-INFO->> Step 435900 run_train: loss = 5.4475  (0.157 sec)
18-06-05 01:14-INFO->> Step 435910 run_train: loss = 5.4260  (0.139 sec)
18-06-05 01:14-INFO->> Step 435920 run_train: loss = 5.4858  (0.187 sec)
18-06-05 01:14-INFO->> Step 435930 run_train: loss = 5.3999  (0.145 sec)
18-06-05 01:14-INFO->> Step 435940 run_train: loss = 5.3889  (0.138 sec)
18-06-05 01:14-INFO->> Step 435950 run_train: loss = 5.4025  (0.173 sec)
18-06-05 01:14-INFO->> Step 435960 run_train: loss = 5.4961  (0.173 sec)
18-06-05 01:14-INFO->> Step 435970 run_train: loss = 5.4106  (0.156 sec)
18-06-05 01:14-INFO->> Step 435980 run_train: loss = 5.3967  (0.170 sec)
18-06-05 01:14-INFO->> Step 435990 run_train: loss = 5.3445  (0.202 sec)
18-06-05 01:14-INFO->> Step 436000 run_train: loss = 5.4529  (0.146 sec)
18-06-05 01:14-INFO->> 2018-06-05 01:14:52.352878 Saving in ckpt
18-06-05 01:14-INFO-Test Data Eval:
18-06-05 01:15-INFO-fpr95 = 0.16882804197662063 and auc = 0.9694776766430366
18-06-05 01:15-INFO->> Step 436010 run_train: loss = 5.4308  (0.151 sec)
18-06-05 01:15-INFO->> Step 436020 run_train: loss = 5.4320  (0.142 sec)
18-06-05 01:15-INFO->> Step 436030 run_train: loss = 5.3741  (0.130 sec)
18-06-05 01:15-INFO->> Step 436040 run_train: loss = 5.3727  (0.182 sec)
18-06-05 01:15-INFO->> Step 436050 run_train: loss = 5.4117  (0.144 sec)
18-06-05 01:15-INFO->> Step 436060 run_train: loss = 5.4276  (0.136 sec)
18-06-05 01:15-INFO->> Step 436070 run_train: loss = 5.3827  (0.135 sec)
18-06-05 01:15-INFO->> Step 436080 run_train: loss = 5.4236  (0.148 sec)
18-06-05 01:15-INFO->> Step 436090 run_train: loss = 5.4146  (0.150 sec)
18-06-05 01:15-INFO->> Step 436100 run_train: loss = 5.3805  (0.167 sec)
18-06-05 01:15-INFO->> Step 436110 run_train: loss = 5.4123  (0.172 sec)
18-06-05 01:15-INFO->> Step 436120 run_train: loss = 5.4049  (0.203 sec)
18-06-05 01:15-INFO->> Step 436130 run_train: loss = 5.3786  (0.124 sec)
18-06-05 01:15-INFO->> Step 436140 run_train: loss = 5.4437  (0.151 sec)
18-06-05 01:15-INFO->> Step 436150 run_train: loss = 5.3800  (0.152 sec)
18-06-05 01:15-INFO->> Step 436160 run_train: loss = 5.3923  (0.130 sec)
18-06-05 01:15-INFO->> Step 436170 run_train: loss = 5.4186  (0.162 sec)
18-06-05 01:16-INFO->> Step 436180 run_train: loss = 5.4743  (0.182 sec)
18-06-05 01:16-INFO->> Step 436190 run_train: loss = 5.4555  (0.129 sec)
18-06-05 01:16-INFO->> Step 436200 run_train: loss = 5.3765  (0.153 sec)
18-06-05 01:16-INFO->> Step 436210 run_train: loss = 5.3435  (0.149 sec)
18-06-05 01:16-INFO->> Step 436220 run_train: loss = 5.4409  (0.138 sec)
18-06-05 01:16-INFO->> Step 436230 run_train: loss = 5.4355  (0.154 sec)
18-06-05 01:16-INFO->> Step 436240 run_train: loss = 5.3732  (0.115 sec)
18-06-05 01:16-INFO->> Step 436250 run_train: loss = 5.4727  (0.161 sec)
18-06-05 01:16-INFO->> Step 436260 run_train: loss = 5.3896  (0.136 sec)
18-06-05 01:16-INFO->> Step 436270 run_train: loss = 5.4375  (0.160 sec)
18-06-05 01:16-INFO->> Step 436280 run_train: loss = 5.3771  (0.150 sec)
18-06-05 01:16-INFO->> Step 436290 run_train: loss = 5.4273  (0.165 sec)
18-06-05 01:16-INFO->> Step 436300 run_train: loss = 5.4041  (0.144 sec)
18-06-05 01:16-INFO->> Step 436310 run_train: loss = 5.4059  (0.159 sec)
18-06-05 01:16-INFO->> Step 436320 run_train: loss = 5.4269  (0.164 sec)
18-06-05 01:16-INFO->> Step 436330 run_train: loss = 5.4122  (0.192 sec)
18-06-05 01:16-INFO->> Step 436340 run_train: loss = 5.4204  (0.162 sec)
18-06-05 01:16-INFO->> Step 436350 run_train: loss = 5.3910  (0.170 sec)
18-06-05 01:16-INFO->> Step 436360 run_train: loss = 5.3870  (0.157 sec)
18-06-05 01:16-INFO->> Step 436370 run_train: loss = 5.4513  (0.119 sec)
18-06-05 01:16-INFO->> Step 436380 run_train: loss = 5.4139  (0.167 sec)
18-06-05 01:16-INFO->> Step 436390 run_train: loss = 5.4139  (0.149 sec)
18-06-05 01:16-INFO->> Step 436400 run_train: loss = 5.3776  (0.165 sec)
18-06-05 01:16-INFO->> Step 436410 run_train: loss = 5.3782  (0.165 sec)
18-06-05 01:16-INFO->> Step 436420 run_train: loss = 5.4694  (0.196 sec)
18-06-05 01:16-INFO->> Step 436430 run_train: loss = 5.4135  (0.149 sec)
18-06-05 01:16-INFO->> Step 436440 run_train: loss = 5.4652  (0.151 sec)
18-06-05 01:16-INFO->> Step 436450 run_train: loss = 5.4515  (0.186 sec)
18-06-05 01:16-INFO->> Step 436460 run_train: loss = 5.3526  (0.153 sec)
18-06-05 01:16-INFO->> Step 436470 run_train: loss = 5.3834  (0.154 sec)
18-06-05 01:16-INFO->> Step 436480 run_train: loss = 5.3907  (0.165 sec)
18-06-05 01:16-INFO->> Step 436490 run_train: loss = 5.3842  (0.142 sec)
18-06-05 01:16-INFO->> Step 436500 run_train: loss = 5.3787  (0.142 sec)
18-06-05 01:16-INFO->> Step 436510 run_train: loss = 5.4181  (0.168 sec)
18-06-05 01:16-INFO->> Step 436520 run_train: loss = 5.4235  (0.165 sec)
18-06-05 01:16-INFO->> Step 436530 run_train: loss = 5.3792  (0.139 sec)
18-06-05 01:16-INFO->> Step 436540 run_train: loss = 5.3938  (0.152 sec)
18-06-05 01:17-INFO->> Step 436550 run_train: loss = 5.4401  (0.137 sec)
18-06-05 01:17-INFO->> Step 436560 run_train: loss = 5.4002  (0.138 sec)
18-06-05 01:17-INFO->> Step 436570 run_train: loss = 5.4207  (0.173 sec)
18-06-05 01:17-INFO->> Step 436580 run_train: loss = 5.4652  (0.169 sec)
18-06-05 01:17-INFO->> Step 436590 run_train: loss = 5.4276  (0.158 sec)
18-06-05 01:17-INFO->> Step 436600 run_train: loss = 5.4314  (0.183 sec)
18-06-05 01:17-INFO->> Step 436610 run_train: loss = 5.4387  (0.178 sec)
18-06-05 01:17-INFO->> Step 436620 run_train: loss = 5.3910  (0.136 sec)
18-06-05 01:17-INFO->> Step 436630 run_train: loss = 5.4221  (0.148 sec)
18-06-05 01:17-INFO->> Step 436640 run_train: loss = 5.4283  (0.112 sec)
18-06-05 01:17-INFO->> Step 436650 run_train: loss = 5.3799  (0.179 sec)
18-06-05 01:17-INFO->> Step 436660 run_train: loss = 5.3421  (0.160 sec)
18-06-05 01:17-INFO->> Step 436670 run_train: loss = 5.3996  (0.154 sec)
18-06-05 01:17-INFO->> Step 436680 run_train: loss = 5.4026  (0.160 sec)
18-06-05 01:17-INFO->> Step 436690 run_train: loss = 5.3416  (0.175 sec)
18-06-05 01:17-INFO->> Step 436700 run_train: loss = 5.3989  (0.185 sec)
18-06-05 01:17-INFO->> Step 436710 run_train: loss = 5.3689  (0.166 sec)
18-06-05 01:17-INFO->> Step 436720 run_train: loss = 5.3251  (0.228 sec)
18-06-05 01:17-INFO->> Step 436730 run_train: loss = 5.4012  (0.203 sec)
18-06-05 01:17-INFO->> Step 436740 run_train: loss = 5.3602  (0.164 sec)
18-06-05 01:17-INFO->> Step 436750 run_train: loss = 5.4131  (0.175 sec)
18-06-05 01:17-INFO->> Step 436760 run_train: loss = 5.4489  (0.167 sec)
18-06-05 01:17-INFO->> Step 436770 run_train: loss = 5.4066  (0.158 sec)
18-06-05 01:17-INFO->> Step 436780 run_train: loss = 5.4190  (0.170 sec)
18-06-05 01:17-INFO->> Step 436790 run_train: loss = 5.4564  (0.160 sec)
18-06-05 01:17-INFO->> Step 436800 run_train: loss = 5.3817  (0.175 sec)
18-06-05 01:17-INFO->> Step 436810 run_train: loss = 5.3794  (0.153 sec)
18-06-05 01:17-INFO->> Step 436820 run_train: loss = 5.3360  (0.190 sec)
18-06-05 01:17-INFO->> Step 436830 run_train: loss = 5.5056  (0.157 sec)
18-06-05 01:17-INFO->> Step 436840 run_train: loss = 5.3662  (0.151 sec)
18-06-05 01:17-INFO->> Step 436850 run_train: loss = 5.4503  (0.150 sec)
18-06-05 01:17-INFO->> Step 436860 run_train: loss = 5.3879  (0.130 sec)
18-06-05 01:17-INFO->> Step 436870 run_train: loss = 5.4430  (0.164 sec)
18-06-05 01:17-INFO->> Step 436880 run_train: loss = 5.4080  (0.168 sec)
18-06-05 01:17-INFO->> Step 436890 run_train: loss = 5.3971  (0.165 sec)
18-06-05 01:17-INFO->> Step 436900 run_train: loss = 5.3567  (0.143 sec)
18-06-05 01:17-INFO->> Step 436910 run_train: loss = 5.3887  (0.150 sec)
18-06-05 01:17-INFO->> Step 436920 run_train: loss = 5.4190  (0.172 sec)
18-06-05 01:18-INFO->> Step 436930 run_train: loss = 5.4445  (0.127 sec)
18-06-05 01:18-INFO->> Step 436940 run_train: loss = 5.4486  (0.160 sec)
18-06-05 01:18-INFO->> Step 436950 run_train: loss = 5.4493  (0.167 sec)
18-06-05 01:18-INFO->> Step 436960 run_train: loss = 5.3846  (0.164 sec)
18-06-05 01:18-INFO->> Step 436970 run_train: loss = 5.4131  (0.156 sec)
18-06-05 01:18-INFO->> Step 436980 run_train: loss = 5.5092  (0.164 sec)
18-06-05 01:18-INFO->> Step 436990 run_train: loss = 5.4032  (0.122 sec)
18-06-05 01:18-INFO->> Step 437000 run_train: loss = 5.3896  (0.157 sec)
18-06-05 01:18-INFO->> 2018-06-05 01:18:11.946760 Saving in ckpt
18-06-05 01:18-INFO-Test Data Eval:
18-06-05 01:18-INFO-fpr95 = 0.17197462805526037 and auc = 0.9692016692962109
18-06-05 01:18-INFO->> Step 437010 run_train: loss = 5.5116  (0.188 sec)
18-06-05 01:18-INFO->> Step 437020 run_train: loss = 5.4415  (0.169 sec)
18-06-05 01:18-INFO->> Step 437030 run_train: loss = 5.4031  (0.168 sec)
18-06-05 01:18-INFO->> Step 437040 run_train: loss = 5.4338  (0.163 sec)
18-06-05 01:19-INFO->> Step 437050 run_train: loss = 5.5102  (0.158 sec)
18-06-05 01:19-INFO->> Step 437060 run_train: loss = 5.4396  (0.163 sec)
18-06-05 01:19-INFO->> Step 437070 run_train: loss = 5.3590  (0.161 sec)
18-06-05 01:19-INFO->> Step 437080 run_train: loss = 5.4163  (0.179 sec)
18-06-05 01:19-INFO->> Step 437090 run_train: loss = 5.4512  (0.122 sec)
18-06-05 01:19-INFO->> Step 437100 run_train: loss = 5.3832  (0.151 sec)
18-06-05 01:19-INFO->> Step 437110 run_train: loss = 5.4159  (0.179 sec)
18-06-05 01:19-INFO->> Step 437120 run_train: loss = 5.4378  (0.165 sec)
18-06-05 01:19-INFO->> Step 437130 run_train: loss = 5.4409  (0.140 sec)
18-06-05 01:19-INFO->> Step 437140 run_train: loss = 5.4336  (0.195 sec)
18-06-05 01:19-INFO->> Step 437150 run_train: loss = 5.4138  (0.175 sec)
18-06-05 01:19-INFO->> Step 437160 run_train: loss = 5.3918  (0.180 sec)
18-06-05 01:19-INFO->> Step 437170 run_train: loss = 5.4062  (0.162 sec)
18-06-05 01:19-INFO->> Step 437180 run_train: loss = 5.4694  (0.169 sec)
18-06-05 01:19-INFO->> Step 437190 run_train: loss = 5.3703  (0.156 sec)
18-06-05 01:19-INFO->> Step 437200 run_train: loss = 5.3797  (0.151 sec)
18-06-05 01:19-INFO->> Step 437210 run_train: loss = 5.4140  (0.160 sec)
18-06-05 01:19-INFO->> Step 437220 run_train: loss = 5.4018  (0.150 sec)
18-06-05 01:19-INFO->> Step 437230 run_train: loss = 5.4239  (0.139 sec)
18-06-05 01:19-INFO->> Step 437240 run_train: loss = 5.3757  (0.167 sec)
18-06-05 01:19-INFO->> Step 437250 run_train: loss = 5.4188  (0.147 sec)
18-06-05 01:19-INFO->> Step 437260 run_train: loss = 5.4186  (0.188 sec)
18-06-05 01:19-INFO->> Step 437270 run_train: loss = 5.3978  (0.162 sec)
18-06-05 01:19-INFO->> Step 437280 run_train: loss = 5.4424  (0.138 sec)
18-06-05 01:19-INFO->> Step 437290 run_train: loss = 5.4569  (0.148 sec)
18-06-05 01:19-INFO->> Step 437300 run_train: loss = 5.4643  (0.168 sec)
18-06-05 01:19-INFO->> Step 437310 run_train: loss = 5.4075  (0.136 sec)
18-06-05 01:19-INFO->> Step 437320 run_train: loss = 5.4395  (0.170 sec)
18-06-05 01:19-INFO->> Step 437330 run_train: loss = 5.4138  (0.180 sec)
18-06-05 01:19-INFO->> Step 437340 run_train: loss = 5.4532  (0.180 sec)
18-06-05 01:19-INFO->> Step 437350 run_train: loss = 5.3803  (0.146 sec)
18-06-05 01:19-INFO->> Step 437360 run_train: loss = 5.4351  (0.136 sec)
18-06-05 01:19-INFO->> Step 437370 run_train: loss = 5.4365  (0.201 sec)
18-06-05 01:19-INFO->> Step 437380 run_train: loss = 5.3794  (0.182 sec)
18-06-05 01:19-INFO->> Step 437390 run_train: loss = 5.4142  (0.139 sec)
18-06-05 01:19-INFO->> Step 437400 run_train: loss = 5.3923  (0.157 sec)
18-06-05 01:19-INFO->> Step 437410 run_train: loss = 5.3479  (0.164 sec)
18-06-05 01:19-INFO->> Step 437420 run_train: loss = 5.4048  (0.181 sec)
18-06-05 01:20-INFO->> Step 437430 run_train: loss = 5.4166  (0.129 sec)
18-06-05 01:20-INFO->> Step 437440 run_train: loss = 5.4104  (0.150 sec)
18-06-05 01:20-INFO->> Step 437450 run_train: loss = 5.4182  (0.139 sec)
18-06-05 01:20-INFO->> Step 437460 run_train: loss = 5.3472  (0.137 sec)
18-06-05 01:20-INFO->> Step 437470 run_train: loss = 5.3842  (0.157 sec)
18-06-05 01:20-INFO->> Step 437480 run_train: loss = 5.4268  (0.141 sec)
18-06-05 01:20-INFO->> Step 437490 run_train: loss = 5.4306  (0.145 sec)
18-06-05 01:20-INFO->> Step 437500 run_train: loss = 5.3874  (0.149 sec)
18-06-05 01:20-INFO->> Step 437510 run_train: loss = 5.4109  (0.170 sec)
18-06-05 01:20-INFO->> Step 437520 run_train: loss = 5.3128  (0.174 sec)
18-06-05 01:20-INFO->> Step 437530 run_train: loss = 5.4499  (0.166 sec)
18-06-05 01:20-INFO->> Step 437540 run_train: loss = 5.4183  (0.144 sec)
18-06-05 01:20-INFO->> Step 437550 run_train: loss = 5.4799  (0.138 sec)
18-06-05 01:20-INFO->> Step 437560 run_train: loss = 5.4151  (0.145 sec)
18-06-05 01:20-INFO->> Step 437570 run_train: loss = 5.4191  (0.162 sec)
18-06-05 01:20-INFO->> Step 437580 run_train: loss = 5.4831  (0.187 sec)
18-06-05 01:20-INFO->> Step 437590 run_train: loss = 5.4170  (0.138 sec)
18-06-05 01:20-INFO->> Step 437600 run_train: loss = 5.4301  (0.160 sec)
18-06-05 01:20-INFO->> Step 437610 run_train: loss = 5.3988  (0.152 sec)
18-06-05 01:20-INFO->> Step 437620 run_train: loss = 5.3574  (0.161 sec)
18-06-05 01:20-INFO->> Step 437630 run_train: loss = 5.4186  (0.149 sec)
18-06-05 01:20-INFO->> Step 437640 run_train: loss = 5.4526  (0.188 sec)
18-06-05 01:20-INFO->> Step 437650 run_train: loss = 5.4596  (0.180 sec)
18-06-05 01:20-INFO->> Step 437660 run_train: loss = 5.4225  (0.178 sec)
18-06-05 01:20-INFO->> Step 437670 run_train: loss = 5.4179  (0.163 sec)
18-06-05 01:20-INFO->> Step 437680 run_train: loss = 5.3657  (0.176 sec)
18-06-05 01:20-INFO->> Step 437690 run_train: loss = 5.3591  (0.153 sec)
18-06-05 01:20-INFO->> Step 437700 run_train: loss = 5.4025  (0.184 sec)
18-06-05 01:20-INFO->> Step 437710 run_train: loss = 5.4145  (0.172 sec)
18-06-05 01:20-INFO->> Step 437720 run_train: loss = 5.3687  (0.191 sec)
18-06-05 01:20-INFO->> Step 437730 run_train: loss = 5.4508  (0.147 sec)
18-06-05 01:20-INFO->> Step 437740 run_train: loss = 5.3793  (0.173 sec)
18-06-05 01:20-INFO->> Step 437750 run_train: loss = 5.4104  (0.157 sec)
18-06-05 01:20-INFO->> Step 437760 run_train: loss = 5.3666  (0.181 sec)
18-06-05 01:20-INFO->> Step 437770 run_train: loss = 5.3311  (0.188 sec)
18-06-05 01:20-INFO->> Step 437780 run_train: loss = 5.4033  (0.144 sec)
18-06-05 01:20-INFO->> Step 437790 run_train: loss = 5.3200  (0.185 sec)
18-06-05 01:20-INFO->> Step 437800 run_train: loss = 5.4340  (0.164 sec)
18-06-05 01:21-INFO->> Step 437810 run_train: loss = 5.4122  (0.145 sec)
18-06-05 01:21-INFO->> Step 437820 run_train: loss = 5.4383  (0.154 sec)
18-06-05 01:21-INFO->> Step 437830 run_train: loss = 5.4439  (0.156 sec)
18-06-05 01:21-INFO->> Step 437840 run_train: loss = 5.4794  (0.146 sec)
18-06-05 01:21-INFO->> Step 437850 run_train: loss = 5.3728  (0.168 sec)
18-06-05 01:21-INFO->> Step 437860 run_train: loss = 5.4173  (0.154 sec)
18-06-05 01:21-INFO->> Step 437870 run_train: loss = 5.4491  (0.145 sec)
18-06-05 01:21-INFO->> Step 437880 run_train: loss = 5.3962  (0.154 sec)
18-06-05 01:21-INFO->> Step 437890 run_train: loss = 5.4133  (0.154 sec)
18-06-05 01:21-INFO->> Step 437900 run_train: loss = 5.3951  (0.160 sec)
18-06-05 01:21-INFO->> Step 437910 run_train: loss = 5.3739  (0.148 sec)
18-06-05 01:21-INFO->> Step 437920 run_train: loss = 5.3972  (0.133 sec)
18-06-05 01:21-INFO->> Step 437930 run_train: loss = 5.4046  (0.156 sec)
18-06-05 01:21-INFO->> Step 437940 run_train: loss = 5.3682  (0.162 sec)
18-06-05 01:21-INFO->> Step 437950 run_train: loss = 5.4349  (0.130 sec)
18-06-05 01:21-INFO->> Step 437960 run_train: loss = 5.4386  (0.143 sec)
18-06-05 01:21-INFO->> Step 437970 run_train: loss = 5.3861  (0.188 sec)
18-06-05 01:21-INFO->> Step 437980 run_train: loss = 5.4286  (0.163 sec)
18-06-05 01:21-INFO->> Step 437990 run_train: loss = 5.4164  (0.123 sec)
18-06-05 01:21-INFO->> Step 438000 run_train: loss = 5.3139  (0.191 sec)
18-06-05 01:21-INFO->> 2018-06-05 01:21:31.305768 Saving in ckpt
18-06-05 01:21-INFO-Test Data Eval:
18-06-05 01:22-INFO-fpr95 = 0.17809345111583422 and auc = 0.9686972771763666
18-06-05 01:22-INFO->> Step 438010 run_train: loss = 5.3633  (0.128 sec)
18-06-05 01:22-INFO->> Step 438020 run_train: loss = 5.4351  (0.190 sec)
18-06-05 01:22-INFO->> Step 438030 run_train: loss = 5.3712  (0.190 sec)
18-06-05 01:22-INFO->> Step 438040 run_train: loss = 5.3589  (0.174 sec)
18-06-05 01:22-INFO->> Step 438050 run_train: loss = 5.4343  (0.142 sec)
18-06-05 01:22-INFO->> Step 438060 run_train: loss = 5.3601  (0.164 sec)
18-06-05 01:22-INFO->> Step 438070 run_train: loss = 5.4620  (0.172 sec)
18-06-05 01:22-INFO->> Step 438080 run_train: loss = 5.4458  (0.138 sec)
18-06-05 01:22-INFO->> Step 438090 run_train: loss = 5.4512  (0.200 sec)
18-06-05 01:22-INFO->> Step 438100 run_train: loss = 5.4258  (0.159 sec)
18-06-05 01:22-INFO->> Step 438110 run_train: loss = 5.3639  (0.200 sec)
18-06-05 01:22-INFO->> Step 438120 run_train: loss = 5.4584  (0.156 sec)
18-06-05 01:22-INFO->> Step 438130 run_train: loss = 5.4657  (0.138 sec)
18-06-05 01:22-INFO->> Step 438140 run_train: loss = 5.4147  (0.156 sec)
18-06-05 01:22-INFO->> Step 438150 run_train: loss = 5.4003  (0.133 sec)
18-06-05 01:22-INFO->> Step 438160 run_train: loss = 5.4361  (0.145 sec)
18-06-05 01:22-INFO->> Step 438170 run_train: loss = 5.3901  (0.162 sec)
18-06-05 01:22-INFO->> Step 438180 run_train: loss = 5.4035  (0.165 sec)
18-06-05 01:22-INFO->> Step 438190 run_train: loss = 5.4805  (0.145 sec)
18-06-05 01:22-INFO->> Step 438200 run_train: loss = 5.4252  (0.219 sec)
18-06-05 01:22-INFO->> Step 438210 run_train: loss = 5.4158  (0.167 sec)
18-06-05 01:22-INFO->> Step 438220 run_train: loss = 5.4032  (0.166 sec)
18-06-05 01:22-INFO->> Step 438230 run_train: loss = 5.4540  (0.158 sec)
18-06-05 01:22-INFO->> Step 438240 run_train: loss = 5.3898  (0.146 sec)
18-06-05 01:22-INFO->> Step 438250 run_train: loss = 5.4960  (0.195 sec)
18-06-05 01:22-INFO->> Step 438260 run_train: loss = 5.4039  (0.162 sec)
18-06-05 01:22-INFO->> Step 438270 run_train: loss = 5.4240  (0.146 sec)
18-06-05 01:22-INFO->> Step 438280 run_train: loss = 5.3633  (0.150 sec)
18-06-05 01:22-INFO->> Step 438290 run_train: loss = 5.3632  (0.171 sec)
18-06-05 01:22-INFO->> Step 438300 run_train: loss = 5.4046  (0.147 sec)
18-06-05 01:23-INFO->> Step 438310 run_train: loss = 5.3980  (0.189 sec)
18-06-05 01:23-INFO->> Step 438320 run_train: loss = 5.4655  (0.169 sec)
18-06-05 01:23-INFO->> Step 438330 run_train: loss = 5.4250  (0.163 sec)
18-06-05 01:23-INFO->> Step 438340 run_train: loss = 5.3893  (0.177 sec)
18-06-05 01:23-INFO->> Step 438350 run_train: loss = 5.4250  (0.168 sec)
18-06-05 01:23-INFO->> Step 438360 run_train: loss = 5.4377  (0.128 sec)
18-06-05 01:23-INFO->> Step 438370 run_train: loss = 5.3703  (0.164 sec)
18-06-05 01:23-INFO->> Step 438380 run_train: loss = 5.3950  (0.176 sec)
18-06-05 01:23-INFO->> Step 438390 run_train: loss = 5.3682  (0.210 sec)
18-06-05 01:23-INFO->> Step 438400 run_train: loss = 5.4213  (0.122 sec)
18-06-05 01:23-INFO->> Step 438410 run_train: loss = 5.4268  (0.161 sec)
18-06-05 01:23-INFO->> Step 438420 run_train: loss = 5.3599  (0.149 sec)
18-06-05 01:23-INFO->> Step 438430 run_train: loss = 5.4545  (0.146 sec)
18-06-05 01:23-INFO->> Step 438440 run_train: loss = 5.4137  (0.165 sec)
18-06-05 01:23-INFO->> Step 438450 run_train: loss = 5.4202  (0.163 sec)
18-06-05 01:23-INFO->> Step 438460 run_train: loss = 5.4055  (0.131 sec)
18-06-05 01:23-INFO->> Step 438470 run_train: loss = 5.3705  (0.153 sec)
18-06-05 01:23-INFO->> Step 438480 run_train: loss = 5.4308  (0.138 sec)
18-06-05 01:23-INFO->> Step 438490 run_train: loss = 5.3857  (0.155 sec)
18-06-05 01:23-INFO->> Step 438500 run_train: loss = 5.4080  (0.161 sec)
18-06-05 01:23-INFO->> Step 438510 run_train: loss = 5.4063  (0.151 sec)
18-06-05 01:23-INFO->> Step 438520 run_train: loss = 5.4417  (0.168 sec)
18-06-05 01:23-INFO->> Step 438530 run_train: loss = 5.3831  (0.167 sec)
18-06-05 01:23-INFO->> Step 438540 run_train: loss = 5.4839  (0.136 sec)
18-06-05 01:23-INFO->> Step 438550 run_train: loss = 5.4100  (0.182 sec)
18-06-05 01:23-INFO->> Step 438560 run_train: loss = 5.4691  (0.122 sec)
18-06-05 01:23-INFO->> Step 438570 run_train: loss = 5.4205  (0.145 sec)
18-06-05 01:23-INFO->> Step 438580 run_train: loss = 5.4500  (0.170 sec)
18-06-05 01:23-INFO->> Step 438590 run_train: loss = 5.4156  (0.154 sec)
18-06-05 01:23-INFO->> Step 438600 run_train: loss = 5.4231  (0.159 sec)
18-06-05 01:23-INFO->> Step 438610 run_train: loss = 5.4351  (0.161 sec)
18-06-05 01:23-INFO->> Step 438620 run_train: loss = 5.4080  (0.149 sec)
18-06-05 01:23-INFO->> Step 438630 run_train: loss = 5.4485  (0.133 sec)
18-06-05 01:23-INFO->> Step 438640 run_train: loss = 5.2997  (0.168 sec)
18-06-05 01:23-INFO->> Step 438650 run_train: loss = 5.3969  (0.138 sec)
18-06-05 01:23-INFO->> Step 438660 run_train: loss = 5.3809  (0.182 sec)
18-06-05 01:23-INFO->> Step 438670 run_train: loss = 5.3983  (0.171 sec)
18-06-05 01:23-INFO->> Step 438680 run_train: loss = 5.4495  (0.166 sec)
18-06-05 01:24-INFO->> Step 438690 run_train: loss = 5.3927  (0.175 sec)
18-06-05 01:24-INFO->> Step 438700 run_train: loss = 5.4412  (0.186 sec)
18-06-05 01:24-INFO->> Step 438710 run_train: loss = 5.4090  (0.178 sec)
18-06-05 01:24-INFO->> Step 438720 run_train: loss = 5.4155  (0.194 sec)
18-06-05 01:24-INFO->> Step 438730 run_train: loss = 5.3931  (0.144 sec)
18-06-05 01:24-INFO->> Step 438740 run_train: loss = 5.4360  (0.148 sec)
18-06-05 01:24-INFO->> Step 438750 run_train: loss = 5.3781  (0.157 sec)
18-06-05 01:24-INFO->> Step 438760 run_train: loss = 5.3282  (0.165 sec)
18-06-05 01:24-INFO->> Step 438770 run_train: loss = 5.4450  (0.146 sec)
18-06-05 01:24-INFO->> Step 438780 run_train: loss = 5.4120  (0.174 sec)
18-06-05 01:24-INFO->> Step 438790 run_train: loss = 5.4188  (0.159 sec)
18-06-05 01:24-INFO->> Step 438800 run_train: loss = 5.3846  (0.200 sec)
18-06-05 01:24-INFO->> Step 438810 run_train: loss = 5.3316  (0.205 sec)
18-06-05 01:24-INFO->> Step 438820 run_train: loss = 5.4936  (0.145 sec)
18-06-05 01:24-INFO->> Step 438830 run_train: loss = 5.4081  (0.150 sec)
18-06-05 01:24-INFO->> Step 438840 run_train: loss = 5.3715  (0.126 sec)
18-06-05 01:24-INFO->> Step 438850 run_train: loss = 5.3675  (0.110 sec)
18-06-05 01:24-INFO->> Step 438860 run_train: loss = 5.4326  (0.147 sec)
18-06-05 01:24-INFO->> Step 438870 run_train: loss = 5.4606  (0.183 sec)
18-06-05 01:24-INFO->> Step 438880 run_train: loss = 5.3748  (0.155 sec)
18-06-05 01:24-INFO->> Step 438890 run_train: loss = 5.3769  (0.164 sec)
18-06-05 01:24-INFO->> Step 438900 run_train: loss = 5.3836  (0.172 sec)
18-06-05 01:24-INFO->> Step 438910 run_train: loss = 5.3751  (0.168 sec)
18-06-05 01:24-INFO->> Step 438920 run_train: loss = 5.4020  (0.185 sec)
18-06-05 01:24-INFO->> Step 438930 run_train: loss = 5.4024  (0.205 sec)
18-06-05 01:24-INFO->> Step 438940 run_train: loss = 5.3839  (0.132 sec)
18-06-05 01:24-INFO->> Step 438950 run_train: loss = 5.4452  (0.179 sec)
18-06-05 01:24-INFO->> Step 438960 run_train: loss = 5.3718  (0.140 sec)
18-06-05 01:24-INFO->> Step 438970 run_train: loss = 5.4369  (0.160 sec)
18-06-05 01:24-INFO->> Step 438980 run_train: loss = 5.4434  (0.170 sec)
18-06-05 01:24-INFO->> Step 438990 run_train: loss = 5.4193  (0.159 sec)
18-06-05 01:24-INFO->> Step 439000 run_train: loss = 5.4798  (0.170 sec)
18-06-05 01:24-INFO->> 2018-06-05 01:24:50.839415 Saving in ckpt
18-06-05 01:24-INFO-Test Data Eval:
18-06-05 01:25-INFO-fpr95 = 0.17511291179596175 and auc = 0.9688497551255313
18-06-05 01:25-INFO->> Step 439010 run_train: loss = 5.3881  (0.139 sec)
18-06-05 01:25-INFO->> Step 439020 run_train: loss = 5.4368  (0.159 sec)
18-06-05 01:25-INFO->> Step 439030 run_train: loss = 5.3502  (0.152 sec)
18-06-05 01:25-INFO->> Step 439040 run_train: loss = 5.4024  (0.167 sec)
18-06-05 01:25-INFO->> Step 439050 run_train: loss = 5.4494  (0.136 sec)
18-06-05 01:25-INFO->> Step 439060 run_train: loss = 5.4626  (0.174 sec)
18-06-05 01:25-INFO->> Step 439070 run_train: loss = 5.4072  (0.153 sec)
18-06-05 01:25-INFO->> Step 439080 run_train: loss = 5.3600  (0.178 sec)
18-06-05 01:25-INFO->> Step 439090 run_train: loss = 5.3372  (0.154 sec)
18-06-05 01:25-INFO->> Step 439100 run_train: loss = 5.3748  (0.119 sec)
18-06-05 01:25-INFO->> Step 439110 run_train: loss = 5.4031  (0.191 sec)
18-06-05 01:25-INFO->> Step 439120 run_train: loss = 5.3693  (0.174 sec)
18-06-05 01:25-INFO->> Step 439130 run_train: loss = 5.4703  (0.150 sec)
18-06-05 01:25-INFO->> Step 439140 run_train: loss = 5.3285  (0.148 sec)
18-06-05 01:25-INFO->> Step 439150 run_train: loss = 5.3906  (0.186 sec)
18-06-05 01:25-INFO->> Step 439160 run_train: loss = 5.4847  (0.164 sec)
18-06-05 01:25-INFO->> Step 439170 run_train: loss = 5.4438  (0.149 sec)
18-06-05 01:25-INFO->> Step 439180 run_train: loss = 5.4059  (0.182 sec)
18-06-05 01:26-INFO->> Step 439190 run_train: loss = 5.4450  (0.166 sec)
18-06-05 01:26-INFO->> Step 439200 run_train: loss = 5.4593  (0.146 sec)
18-06-05 01:26-INFO->> Step 439210 run_train: loss = 5.4409  (0.169 sec)
18-06-05 01:26-INFO->> Step 439220 run_train: loss = 5.4441  (0.161 sec)
18-06-05 01:26-INFO->> Step 439230 run_train: loss = 5.3746  (0.174 sec)
18-06-05 01:26-INFO->> Step 439240 run_train: loss = 5.4437  (0.177 sec)
18-06-05 01:26-INFO->> Step 439250 run_train: loss = 5.4717  (0.148 sec)
18-06-05 01:26-INFO->> Step 439260 run_train: loss = 5.4564  (0.136 sec)
18-06-05 01:26-INFO->> Step 439270 run_train: loss = 5.4175  (0.172 sec)
18-06-05 01:26-INFO->> Step 439280 run_train: loss = 5.3925  (0.146 sec)
18-06-05 01:26-INFO->> Step 439290 run_train: loss = 5.3946  (0.167 sec)
18-06-05 01:26-INFO->> Step 439300 run_train: loss = 5.3932  (0.158 sec)
18-06-05 01:26-INFO->> Step 439310 run_train: loss = 5.3906  (0.153 sec)
18-06-05 01:26-INFO->> Step 439320 run_train: loss = 5.4363  (0.151 sec)
18-06-05 01:26-INFO->> Step 439330 run_train: loss = 5.4891  (0.186 sec)
18-06-05 01:26-INFO->> Step 439340 run_train: loss = 5.3779  (0.152 sec)
18-06-05 01:26-INFO->> Step 439350 run_train: loss = 5.4729  (0.174 sec)
18-06-05 01:26-INFO->> Step 439360 run_train: loss = 5.4370  (0.170 sec)
18-06-05 01:26-INFO->> Step 439370 run_train: loss = 5.3676  (0.198 sec)
18-06-05 01:26-INFO->> Step 439380 run_train: loss = 5.3964  (0.158 sec)
18-06-05 01:26-INFO->> Step 439390 run_train: loss = 5.4393  (0.145 sec)
18-06-05 01:26-INFO->> Step 439400 run_train: loss = 5.3619  (0.154 sec)
18-06-05 01:26-INFO->> Step 439410 run_train: loss = 5.4587  (0.186 sec)
18-06-05 01:26-INFO->> Step 439420 run_train: loss = 5.4743  (0.143 sec)
18-06-05 01:26-INFO->> Step 439430 run_train: loss = 5.4518  (0.155 sec)
18-06-05 01:26-INFO->> Step 439440 run_train: loss = 5.3914  (0.157 sec)
18-06-05 01:26-INFO->> Step 439450 run_train: loss = 5.4477  (0.160 sec)
18-06-05 01:26-INFO->> Step 439460 run_train: loss = 5.3659  (0.193 sec)
18-06-05 01:26-INFO->> Step 439470 run_train: loss = 5.3393  (0.156 sec)
18-06-05 01:26-INFO->> Step 439480 run_train: loss = 5.3970  (0.127 sec)
18-06-05 01:26-INFO->> Step 439490 run_train: loss = 5.4698  (0.157 sec)
18-06-05 01:26-INFO->> Step 439500 run_train: loss = 5.3937  (0.139 sec)
18-06-05 01:26-INFO->> Step 439510 run_train: loss = 5.4498  (0.188 sec)
18-06-05 01:26-INFO->> Step 439520 run_train: loss = 5.4130  (0.159 sec)
18-06-05 01:26-INFO->> Step 439530 run_train: loss = 5.4350  (0.126 sec)
18-06-05 01:26-INFO->> Step 439540 run_train: loss = 5.4129  (0.188 sec)
18-06-05 01:26-INFO->> Step 439550 run_train: loss = 5.3546  (0.166 sec)
18-06-05 01:27-INFO->> Step 439560 run_train: loss = 5.3757  (0.144 sec)
18-06-05 01:27-INFO->> Step 439570 run_train: loss = 5.4238  (0.197 sec)
18-06-05 01:27-INFO->> Step 439580 run_train: loss = 5.4514  (0.139 sec)
18-06-05 01:27-INFO->> Step 439590 run_train: loss = 5.3744  (0.144 sec)
18-06-05 01:27-INFO->> Step 439600 run_train: loss = 5.3911  (0.188 sec)
18-06-05 01:27-INFO->> Step 439610 run_train: loss = 5.3740  (0.122 sec)
18-06-05 01:27-INFO->> Step 439620 run_train: loss = 5.3904  (0.175 sec)
18-06-05 01:27-INFO->> Step 439630 run_train: loss = 5.3645  (0.165 sec)
18-06-05 01:27-INFO->> Step 439640 run_train: loss = 5.4023  (0.151 sec)
18-06-05 01:27-INFO->> Step 439650 run_train: loss = 5.4017  (0.177 sec)
18-06-05 01:27-INFO->> Step 439660 run_train: loss = 5.4212  (0.143 sec)
18-06-05 01:27-INFO->> Step 439670 run_train: loss = 5.4029  (0.150 sec)
18-06-05 01:27-INFO->> Step 439680 run_train: loss = 5.3491  (0.138 sec)
18-06-05 01:27-INFO->> Step 439690 run_train: loss = 5.4372  (0.154 sec)
18-06-05 01:27-INFO->> Step 439700 run_train: loss = 5.4355  (0.166 sec)
18-06-05 01:27-INFO->> Step 439710 run_train: loss = 5.4050  (0.172 sec)
18-06-05 01:27-INFO->> Step 439720 run_train: loss = 5.4237  (0.121 sec)
18-06-05 01:27-INFO->> Step 439730 run_train: loss = 5.4114  (0.161 sec)
18-06-05 01:27-INFO->> Step 439740 run_train: loss = 5.4360  (0.140 sec)
18-06-05 01:27-INFO->> Step 439750 run_train: loss = 5.4528  (0.116 sec)
18-06-05 01:27-INFO->> Step 439760 run_train: loss = 5.4230  (0.177 sec)
18-06-05 01:27-INFO->> Step 439770 run_train: loss = 5.3462  (0.174 sec)
18-06-05 01:27-INFO->> Step 439780 run_train: loss = 5.3961  (0.177 sec)
18-06-05 01:27-INFO->> Step 439790 run_train: loss = 5.3907  (0.196 sec)
18-06-05 01:27-INFO->> Step 439800 run_train: loss = 5.4453  (0.170 sec)
18-06-05 01:27-INFO->> Step 439810 run_train: loss = 5.3820  (0.161 sec)
18-06-05 01:27-INFO->> Step 439820 run_train: loss = 5.4114  (0.163 sec)
18-06-05 01:27-INFO->> Step 439830 run_train: loss = 5.4442  (0.168 sec)
18-06-05 01:27-INFO->> Step 439840 run_train: loss = 5.4005  (0.141 sec)
18-06-05 01:27-INFO->> Step 439850 run_train: loss = 5.4605  (0.196 sec)
18-06-05 01:27-INFO->> Step 439860 run_train: loss = 5.4374  (0.134 sec)
18-06-05 01:27-INFO->> Step 439870 run_train: loss = 5.4119  (0.147 sec)
18-06-05 01:27-INFO->> Step 439880 run_train: loss = 5.4096  (0.165 sec)
18-06-05 01:27-INFO->> Step 439890 run_train: loss = 5.3957  (0.165 sec)
18-06-05 01:27-INFO->> Step 439900 run_train: loss = 5.4351  (0.134 sec)
18-06-05 01:27-INFO->> Step 439910 run_train: loss = 5.4369  (0.142 sec)
18-06-05 01:27-INFO->> Step 439920 run_train: loss = 5.4501  (0.168 sec)
18-06-05 01:27-INFO->> Step 439930 run_train: loss = 5.3613  (0.188 sec)
18-06-05 01:28-INFO->> Step 439940 run_train: loss = 5.4197  (0.144 sec)
18-06-05 01:28-INFO->> Step 439950 run_train: loss = 5.4024  (0.162 sec)
18-06-05 01:28-INFO->> Step 439960 run_train: loss = 5.4013  (0.192 sec)
18-06-05 01:28-INFO->> Step 439970 run_train: loss = 5.4632  (0.135 sec)
18-06-05 01:28-INFO->> Step 439980 run_train: loss = 5.3998  (0.141 sec)
18-06-05 01:28-INFO->> Step 439990 run_train: loss = 5.4228  (0.204 sec)
18-06-05 01:28-INFO->> Step 440000 run_train: loss = 5.4393  (0.176 sec)
18-06-05 01:28-INFO->> 2018-06-05 01:28:10.324986 Saving in ckpt
18-06-05 01:28-INFO-Test Data Eval:
18-06-05 01:28-INFO-fpr95 = 0.17191651168969183 and auc = 0.9692177303649863
18-06-05 01:28-INFO->> Step 440010 run_train: loss = 5.3986  (0.178 sec)
18-06-05 01:28-INFO->> Step 440020 run_train: loss = 5.3789  (0.146 sec)
18-06-05 01:28-INFO->> Step 440030 run_train: loss = 5.4114  (0.163 sec)
18-06-05 01:28-INFO->> Step 440040 run_train: loss = 5.4617  (0.193 sec)
18-06-05 01:28-INFO->> Step 440050 run_train: loss = 5.4807  (0.140 sec)
18-06-05 01:28-INFO->> Step 440060 run_train: loss = 5.3673  (0.126 sec)
18-06-05 01:29-INFO->> Step 440070 run_train: loss = 5.4498  (0.126 sec)
18-06-05 01:29-INFO->> Step 440080 run_train: loss = 5.4417  (0.158 sec)
18-06-05 01:29-INFO->> Step 440090 run_train: loss = 5.4273  (0.176 sec)
18-06-05 01:29-INFO->> Step 440100 run_train: loss = 5.4328  (0.171 sec)
18-06-05 01:29-INFO->> Step 440110 run_train: loss = 5.4106  (0.163 sec)
18-06-05 01:29-INFO->> Step 440120 run_train: loss = 5.4936  (0.150 sec)
18-06-05 01:29-INFO->> Step 440130 run_train: loss = 5.4018  (0.136 sec)
18-06-05 01:29-INFO->> Step 440140 run_train: loss = 5.4605  (0.176 sec)
18-06-05 01:29-INFO->> Step 440150 run_train: loss = 5.3838  (0.162 sec)
18-06-05 01:29-INFO->> Step 440160 run_train: loss = 5.4500  (0.162 sec)
18-06-05 01:29-INFO->> Step 440170 run_train: loss = 5.3682  (0.141 sec)
18-06-05 01:29-INFO->> Step 440180 run_train: loss = 5.3920  (0.136 sec)
18-06-05 01:29-INFO->> Step 440190 run_train: loss = 5.3596  (0.116 sec)
18-06-05 01:29-INFO->> Step 440200 run_train: loss = 5.4109  (0.154 sec)
18-06-05 01:29-INFO->> Step 440210 run_train: loss = 5.4331  (0.158 sec)
18-06-05 01:29-INFO->> Step 440220 run_train: loss = 5.3812  (0.138 sec)
18-06-05 01:29-INFO->> Step 440230 run_train: loss = 5.4314  (0.169 sec)
18-06-05 01:29-INFO->> Step 440240 run_train: loss = 5.4235  (0.143 sec)
18-06-05 01:29-INFO->> Step 440250 run_train: loss = 5.4337  (0.149 sec)
18-06-05 01:29-INFO->> Step 440260 run_train: loss = 5.4435  (0.155 sec)
18-06-05 01:29-INFO->> Step 440270 run_train: loss = 5.4566  (0.191 sec)
18-06-05 01:29-INFO->> Step 440280 run_train: loss = 5.4458  (0.131 sec)
18-06-05 01:29-INFO->> Step 440290 run_train: loss = 5.4348  (0.178 sec)
18-06-05 01:29-INFO->> Step 440300 run_train: loss = 5.4230  (0.223 sec)
18-06-05 01:29-INFO->> Step 440310 run_train: loss = 5.3409  (0.159 sec)
18-06-05 01:29-INFO->> Step 440320 run_train: loss = 5.4075  (0.158 sec)
18-06-05 01:29-INFO->> Step 440330 run_train: loss = 5.4323  (0.151 sec)
18-06-05 01:29-INFO->> Step 440340 run_train: loss = 5.4286  (0.163 sec)
18-06-05 01:29-INFO->> Step 440350 run_train: loss = 5.3721  (0.167 sec)
18-06-05 01:29-INFO->> Step 440360 run_train: loss = 5.4342  (0.169 sec)
18-06-05 01:29-INFO->> Step 440370 run_train: loss = 5.3628  (0.185 sec)
18-06-05 01:29-INFO->> Step 440380 run_train: loss = 5.4869  (0.167 sec)
18-06-05 01:29-INFO->> Step 440390 run_train: loss = 5.4585  (0.122 sec)
18-06-05 01:29-INFO->> Step 440400 run_train: loss = 5.4182  (0.180 sec)
18-06-05 01:29-INFO->> Step 440410 run_train: loss = 5.3444  (0.145 sec)
18-06-05 01:29-INFO->> Step 440420 run_train: loss = 5.3899  (0.182 sec)
18-06-05 01:29-INFO->> Step 440430 run_train: loss = 5.3149  (0.156 sec)
18-06-05 01:30-INFO->> Step 440440 run_train: loss = 5.4498  (0.141 sec)
18-06-05 01:30-INFO->> Step 440450 run_train: loss = 5.4091  (0.168 sec)
18-06-05 01:30-INFO->> Step 440460 run_train: loss = 5.4483  (0.164 sec)
18-06-05 01:30-INFO->> Step 440470 run_train: loss = 5.4093  (0.137 sec)
18-06-05 01:30-INFO->> Step 440480 run_train: loss = 5.3706  (0.179 sec)
18-06-05 01:30-INFO->> Step 440490 run_train: loss = 5.4000  (0.163 sec)
18-06-05 01:30-INFO->> Step 440500 run_train: loss = 5.4256  (0.176 sec)
18-06-05 01:30-INFO->> Step 440510 run_train: loss = 5.4554  (0.177 sec)
18-06-05 01:30-INFO->> Step 440520 run_train: loss = 5.4148  (0.148 sec)
18-06-05 01:30-INFO->> Step 440530 run_train: loss = 5.4167  (0.202 sec)
18-06-05 01:30-INFO->> Step 440540 run_train: loss = 5.3722  (0.153 sec)
18-06-05 01:30-INFO->> Step 440550 run_train: loss = 5.4349  (0.133 sec)
18-06-05 01:30-INFO->> Step 440560 run_train: loss = 5.4282  (0.150 sec)
18-06-05 01:30-INFO->> Step 440570 run_train: loss = 5.4444  (0.170 sec)
18-06-05 01:30-INFO->> Step 440580 run_train: loss = 5.3886  (0.159 sec)
18-06-05 01:30-INFO->> Step 440590 run_train: loss = 5.3791  (0.183 sec)
18-06-05 01:30-INFO->> Step 440600 run_train: loss = 5.3261  (0.157 sec)
18-06-05 01:30-INFO->> Step 440610 run_train: loss = 5.4296  (0.145 sec)
18-06-05 01:30-INFO->> Step 440620 run_train: loss = 5.4677  (0.161 sec)
18-06-05 01:30-INFO->> Step 440630 run_train: loss = 5.3528  (0.162 sec)
18-06-05 01:30-INFO->> Step 440640 run_train: loss = 5.4412  (0.178 sec)
18-06-05 01:30-INFO->> Step 440650 run_train: loss = 5.3805  (0.165 sec)
18-06-05 01:30-INFO->> Step 440660 run_train: loss = 5.4732  (0.142 sec)
18-06-05 01:30-INFO->> Step 440670 run_train: loss = 5.4329  (0.123 sec)
18-06-05 01:30-INFO->> Step 440680 run_train: loss = 5.4659  (0.201 sec)
18-06-05 01:30-INFO->> Step 440690 run_train: loss = 5.4756  (0.147 sec)
18-06-05 01:30-INFO->> Step 440700 run_train: loss = 5.3963  (0.161 sec)
18-06-05 01:30-INFO->> Step 440710 run_train: loss = 5.4594  (0.142 sec)
18-06-05 01:30-INFO->> Step 440720 run_train: loss = 5.4042  (0.151 sec)
18-06-05 01:30-INFO->> Step 440730 run_train: loss = 5.4132  (0.149 sec)
18-06-05 01:30-INFO->> Step 440740 run_train: loss = 5.3713  (0.115 sec)
18-06-05 01:30-INFO->> Step 440750 run_train: loss = 5.4286  (0.153 sec)
18-06-05 01:30-INFO->> Step 440760 run_train: loss = 5.3898  (0.166 sec)
18-06-05 01:30-INFO->> Step 440770 run_train: loss = 5.4355  (0.144 sec)
18-06-05 01:30-INFO->> Step 440780 run_train: loss = 5.3946  (0.125 sec)
18-06-05 01:30-INFO->> Step 440790 run_train: loss = 5.3562  (0.143 sec)
18-06-05 01:30-INFO->> Step 440800 run_train: loss = 5.3904  (0.163 sec)
18-06-05 01:30-INFO->> Step 440810 run_train: loss = 5.4137  (0.209 sec)
18-06-05 01:31-INFO->> Step 440820 run_train: loss = 5.3745  (0.146 sec)
18-06-05 01:31-INFO->> Step 440830 run_train: loss = 5.4417  (0.182 sec)
18-06-05 01:31-INFO->> Step 440840 run_train: loss = 5.4344  (0.178 sec)
18-06-05 01:31-INFO->> Step 440850 run_train: loss = 5.4302  (0.174 sec)
18-06-05 01:31-INFO->> Step 440860 run_train: loss = 5.4812  (0.181 sec)
18-06-05 01:31-INFO->> Step 440870 run_train: loss = 5.4320  (0.201 sec)
18-06-05 01:31-INFO->> Step 440880 run_train: loss = 5.3725  (0.159 sec)
18-06-05 01:31-INFO->> Step 440890 run_train: loss = 5.4338  (0.160 sec)
18-06-05 01:31-INFO->> Step 440900 run_train: loss = 5.3858  (0.137 sec)
18-06-05 01:31-INFO->> Step 440910 run_train: loss = 5.4451  (0.170 sec)
18-06-05 01:31-INFO->> Step 440920 run_train: loss = 5.3668  (0.212 sec)
18-06-05 01:31-INFO->> Step 440930 run_train: loss = 5.4450  (0.143 sec)
18-06-05 01:31-INFO->> Step 440940 run_train: loss = 5.4497  (0.210 sec)
18-06-05 01:31-INFO->> Step 440950 run_train: loss = 5.3996  (0.158 sec)
18-06-05 01:31-INFO->> Step 440960 run_train: loss = 5.4238  (0.156 sec)
18-06-05 01:31-INFO->> Step 440970 run_train: loss = 5.4409  (0.138 sec)
18-06-05 01:31-INFO->> Step 440980 run_train: loss = 5.3740  (0.151 sec)
18-06-05 01:31-INFO->> Step 440990 run_train: loss = 5.4157  (0.186 sec)
18-06-05 01:31-INFO->> Step 441000 run_train: loss = 5.4691  (0.160 sec)
18-06-05 01:31-INFO->> 2018-06-05 01:31:29.577750 Saving in ckpt
18-06-05 01:31-INFO-Test Data Eval:
18-06-05 01:32-INFO-fpr95 = 0.1709866498405951 and auc = 0.9691906701873049
18-06-05 01:32-INFO->> Step 441010 run_train: loss = 5.4214  (0.128 sec)
18-06-05 01:32-INFO->> Step 441020 run_train: loss = 5.4016  (0.164 sec)
18-06-05 01:32-INFO->> Step 441030 run_train: loss = 5.4504  (0.163 sec)
18-06-05 01:32-INFO->> Step 441040 run_train: loss = 5.4309  (0.162 sec)
18-06-05 01:32-INFO->> Step 441050 run_train: loss = 5.3710  (0.167 sec)
18-06-05 01:32-INFO->> Step 441060 run_train: loss = 5.4267  (0.138 sec)
18-06-05 01:32-INFO->> Step 441070 run_train: loss = 5.3555  (0.182 sec)
18-06-05 01:32-INFO->> Step 441080 run_train: loss = 5.3879  (0.132 sec)
18-06-05 01:32-INFO->> Step 441090 run_train: loss = 5.4584  (0.169 sec)
18-06-05 01:32-INFO->> Step 441100 run_train: loss = 5.4465  (0.155 sec)
18-06-05 01:32-INFO->> Step 441110 run_train: loss = 5.3969  (0.149 sec)
18-06-05 01:32-INFO->> Step 441120 run_train: loss = 5.3682  (0.171 sec)
18-06-05 01:32-INFO->> Step 441130 run_train: loss = 5.4366  (0.198 sec)
18-06-05 01:32-INFO->> Step 441140 run_train: loss = 5.3572  (0.148 sec)
18-06-05 01:32-INFO->> Step 441150 run_train: loss = 5.4426  (0.165 sec)
18-06-05 01:32-INFO->> Step 441160 run_train: loss = 5.4047  (0.169 sec)
18-06-05 01:32-INFO->> Step 441170 run_train: loss = 5.3299  (0.126 sec)
18-06-05 01:32-INFO->> Step 441180 run_train: loss = 5.3838  (0.198 sec)
18-06-05 01:32-INFO->> Step 441190 run_train: loss = 5.4286  (0.172 sec)
18-06-05 01:32-INFO->> Step 441200 run_train: loss = 5.3289  (0.160 sec)
18-06-05 01:32-INFO->> Step 441210 run_train: loss = 5.4090  (0.178 sec)
18-06-05 01:32-INFO->> Step 441220 run_train: loss = 5.4055  (0.160 sec)
18-06-05 01:32-INFO->> Step 441230 run_train: loss = 5.4768  (0.156 sec)
18-06-05 01:32-INFO->> Step 441240 run_train: loss = 5.3704  (0.165 sec)
18-06-05 01:32-INFO->> Step 441250 run_train: loss = 5.4119  (0.160 sec)
18-06-05 01:32-INFO->> Step 441260 run_train: loss = 5.4405  (0.184 sec)
18-06-05 01:32-INFO->> Step 441270 run_train: loss = 5.4018  (0.133 sec)
18-06-05 01:32-INFO->> Step 441280 run_train: loss = 5.4370  (0.179 sec)
18-06-05 01:32-INFO->> Step 441290 run_train: loss = 5.4591  (0.166 sec)
18-06-05 01:32-INFO->> Step 441300 run_train: loss = 5.4066  (0.159 sec)
18-06-05 01:32-INFO->> Step 441310 run_train: loss = 5.3659  (0.163 sec)
18-06-05 01:33-INFO->> Step 441320 run_train: loss = 5.4185  (0.195 sec)
18-06-05 01:33-INFO->> Step 441330 run_train: loss = 5.4547  (0.176 sec)
18-06-05 01:33-INFO->> Step 441340 run_train: loss = 5.3639  (0.174 sec)
18-06-05 01:33-INFO->> Step 441350 run_train: loss = 5.4927  (0.153 sec)
18-06-05 01:33-INFO->> Step 441360 run_train: loss = 5.4040  (0.193 sec)
18-06-05 01:33-INFO->> Step 441370 run_train: loss = 5.4265  (0.169 sec)
18-06-05 01:33-INFO->> Step 441380 run_train: loss = 5.3972  (0.157 sec)
18-06-05 01:33-INFO->> Step 441390 run_train: loss = 5.4794  (0.163 sec)
18-06-05 01:33-INFO->> Step 441400 run_train: loss = 5.3849  (0.154 sec)
18-06-05 01:33-INFO->> Step 441410 run_train: loss = 5.4120  (0.184 sec)
18-06-05 01:33-INFO->> Step 441420 run_train: loss = 5.3642  (0.192 sec)
18-06-05 01:33-INFO->> Step 441430 run_train: loss = 5.4065  (0.159 sec)
18-06-05 01:33-INFO->> Step 441440 run_train: loss = 5.3699  (0.154 sec)
18-06-05 01:33-INFO->> Step 441450 run_train: loss = 5.3790  (0.161 sec)
18-06-05 01:33-INFO->> Step 441460 run_train: loss = 5.4396  (0.171 sec)
18-06-05 01:33-INFO->> Step 441470 run_train: loss = 5.4278  (0.150 sec)
18-06-05 01:33-INFO->> Step 441480 run_train: loss = 5.4090  (0.131 sec)
18-06-05 01:33-INFO->> Step 441490 run_train: loss = 5.5021  (0.157 sec)
18-06-05 01:33-INFO->> Step 441500 run_train: loss = 5.4175  (0.156 sec)
18-06-05 01:33-INFO->> Step 441510 run_train: loss = 5.4072  (0.127 sec)
18-06-05 01:33-INFO->> Step 441520 run_train: loss = 5.4146  (0.199 sec)
18-06-05 01:33-INFO->> Step 441530 run_train: loss = 5.3838  (0.189 sec)
18-06-05 01:33-INFO->> Step 441540 run_train: loss = 5.4253  (0.152 sec)
18-06-05 01:33-INFO->> Step 441550 run_train: loss = 5.4615  (0.114 sec)
18-06-05 01:33-INFO->> Step 441560 run_train: loss = 5.4728  (0.145 sec)
18-06-05 01:33-INFO->> Step 441570 run_train: loss = 5.3650  (0.176 sec)
18-06-05 01:33-INFO->> Step 441580 run_train: loss = 5.3820  (0.159 sec)
18-06-05 01:33-INFO->> Step 441590 run_train: loss = 5.4312  (0.154 sec)
18-06-05 01:33-INFO->> Step 441600 run_train: loss = 5.3938  (0.156 sec)
18-06-05 01:33-INFO->> Step 441610 run_train: loss = 5.4130  (0.155 sec)
18-06-05 01:33-INFO->> Step 441620 run_train: loss = 5.3931  (0.162 sec)
18-06-05 01:33-INFO->> Step 441630 run_train: loss = 5.3826  (0.151 sec)
18-06-05 01:33-INFO->> Step 441640 run_train: loss = 5.4678  (0.133 sec)
18-06-05 01:33-INFO->> Step 441650 run_train: loss = 5.4368  (0.124 sec)
18-06-05 01:33-INFO->> Step 441660 run_train: loss = 5.4497  (0.175 sec)
18-06-05 01:33-INFO->> Step 441670 run_train: loss = 5.4388  (0.120 sec)
18-06-05 01:33-INFO->> Step 441680 run_train: loss = 5.3919  (0.149 sec)
18-06-05 01:33-INFO->> Step 441690 run_train: loss = 5.4692  (0.170 sec)
18-06-05 01:34-INFO->> Step 441700 run_train: loss = 5.4853  (0.148 sec)
18-06-05 01:34-INFO->> Step 441710 run_train: loss = 5.4153  (0.155 sec)
18-06-05 01:34-INFO->> Step 441720 run_train: loss = 5.4251  (0.166 sec)
18-06-05 01:34-INFO->> Step 441730 run_train: loss = 5.4110  (0.184 sec)
18-06-05 01:34-INFO->> Step 441740 run_train: loss = 5.4007  (0.150 sec)
18-06-05 01:34-INFO->> Step 441750 run_train: loss = 5.3714  (0.135 sec)
18-06-05 01:34-INFO->> Step 441760 run_train: loss = 5.4280  (0.182 sec)
18-06-05 01:34-INFO->> Step 441770 run_train: loss = 5.4127  (0.143 sec)
18-06-05 01:34-INFO->> Step 441780 run_train: loss = 5.3913  (0.155 sec)
18-06-05 01:34-INFO->> Step 441790 run_train: loss = 5.3520  (0.162 sec)
18-06-05 01:34-INFO->> Step 441800 run_train: loss = 5.4157  (0.164 sec)
18-06-05 01:34-INFO->> Step 441810 run_train: loss = 5.4278  (0.126 sec)
18-06-05 01:34-INFO->> Step 441820 run_train: loss = 5.4073  (0.149 sec)
18-06-05 01:34-INFO->> Step 441830 run_train: loss = 5.3351  (0.175 sec)
18-06-05 01:34-INFO->> Step 441840 run_train: loss = 5.4292  (0.156 sec)
18-06-05 01:34-INFO->> Step 441850 run_train: loss = 5.4150  (0.173 sec)
18-06-05 01:34-INFO->> Step 441860 run_train: loss = 5.3667  (0.155 sec)
18-06-05 01:34-INFO->> Step 441870 run_train: loss = 5.4225  (0.137 sec)
18-06-05 01:34-INFO->> Step 441880 run_train: loss = 5.3751  (0.151 sec)
18-06-05 01:34-INFO->> Step 441890 run_train: loss = 5.3881  (0.151 sec)
18-06-05 01:34-INFO->> Step 441900 run_train: loss = 5.4710  (0.149 sec)
18-06-05 01:34-INFO->> Step 441910 run_train: loss = 5.4101  (0.172 sec)
18-06-05 01:34-INFO->> Step 441920 run_train: loss = 5.4620  (0.167 sec)
18-06-05 01:34-INFO->> Step 441930 run_train: loss = 5.3942  (0.148 sec)
18-06-05 01:34-INFO->> Step 441940 run_train: loss = 5.4097  (0.182 sec)
18-06-05 01:34-INFO->> Step 441950 run_train: loss = 5.4049  (0.148 sec)
18-06-05 01:34-INFO->> Step 441960 run_train: loss = 5.2981  (0.117 sec)
18-06-05 01:34-INFO->> Step 441970 run_train: loss = 5.3821  (0.144 sec)
18-06-05 01:34-INFO->> Step 441980 run_train: loss = 5.4177  (0.165 sec)
18-06-05 01:34-INFO->> Step 441990 run_train: loss = 5.4054  (0.149 sec)
18-06-05 01:34-INFO->> Step 442000 run_train: loss = 5.3855  (0.165 sec)
18-06-05 01:34-INFO->> 2018-06-05 01:34:49.397459 Saving in ckpt
18-06-05 01:34-INFO-Test Data Eval:
18-06-05 01:35-INFO-fpr95 = 0.17595144792773645 and auc = 0.9690448961168532
18-06-05 01:35-INFO->> Step 442010 run_train: loss = 5.3981  (0.145 sec)
18-06-05 01:35-INFO->> Step 442020 run_train: loss = 5.3467  (0.171 sec)
18-06-05 01:35-INFO->> Step 442030 run_train: loss = 5.3657  (0.167 sec)
18-06-05 01:35-INFO->> Step 442040 run_train: loss = 5.3943  (0.162 sec)
18-06-05 01:35-INFO->> Step 442050 run_train: loss = 5.3812  (0.165 sec)
18-06-05 01:35-INFO->> Step 442060 run_train: loss = 5.3849  (0.178 sec)
18-06-05 01:35-INFO->> Step 442070 run_train: loss = 5.3968  (0.152 sec)
18-06-05 01:35-INFO->> Step 442080 run_train: loss = 5.3725  (0.119 sec)
18-06-05 01:35-INFO->> Step 442090 run_train: loss = 5.3467  (0.156 sec)
18-06-05 01:35-INFO->> Step 442100 run_train: loss = 5.4112  (0.159 sec)
18-06-05 01:35-INFO->> Step 442110 run_train: loss = 5.4342  (0.150 sec)
18-06-05 01:35-INFO->> Step 442120 run_train: loss = 5.4202  (0.128 sec)
18-06-05 01:35-INFO->> Step 442130 run_train: loss = 5.4158  (0.178 sec)
18-06-05 01:35-INFO->> Step 442140 run_train: loss = 5.3703  (0.166 sec)
18-06-05 01:35-INFO->> Step 442150 run_train: loss = 5.3769  (0.154 sec)
18-06-05 01:35-INFO->> Step 442160 run_train: loss = 5.4652  (0.121 sec)
18-06-05 01:35-INFO->> Step 442170 run_train: loss = 5.4204  (0.142 sec)
18-06-05 01:35-INFO->> Step 442180 run_train: loss = 5.3968  (0.141 sec)
18-06-05 01:35-INFO->> Step 442190 run_train: loss = 5.4516  (0.187 sec)
18-06-05 01:36-INFO->> Step 442200 run_train: loss = 5.3990  (0.140 sec)
18-06-05 01:36-INFO->> Step 442210 run_train: loss = 5.4047  (0.154 sec)
18-06-05 01:36-INFO->> Step 442220 run_train: loss = 5.3549  (0.178 sec)
18-06-05 01:36-INFO->> Step 442230 run_train: loss = 5.4318  (0.171 sec)
18-06-05 01:36-INFO->> Step 442240 run_train: loss = 5.4325  (0.164 sec)
18-06-05 01:36-INFO->> Step 442250 run_train: loss = 5.4333  (0.173 sec)
18-06-05 01:36-INFO->> Step 442260 run_train: loss = 5.4258  (0.143 sec)
18-06-05 01:36-INFO->> Step 442270 run_train: loss = 5.4066  (0.228 sec)
18-06-05 01:36-INFO->> Step 442280 run_train: loss = 5.3949  (0.156 sec)
18-06-05 01:36-INFO->> Step 442290 run_train: loss = 5.4246  (0.162 sec)
18-06-05 01:36-INFO->> Step 442300 run_train: loss = 5.4822  (0.140 sec)
18-06-05 01:36-INFO->> Step 442310 run_train: loss = 5.4433  (0.182 sec)
18-06-05 01:36-INFO->> Step 442320 run_train: loss = 5.4056  (0.151 sec)
18-06-05 01:36-INFO->> Step 442330 run_train: loss = 5.4036  (0.158 sec)
18-06-05 01:36-INFO->> Step 442340 run_train: loss = 5.3810  (0.160 sec)
18-06-05 01:36-INFO->> Step 442350 run_train: loss = 5.3913  (0.122 sec)
18-06-05 01:36-INFO->> Step 442360 run_train: loss = 5.4284  (0.139 sec)
18-06-05 01:36-INFO->> Step 442370 run_train: loss = 5.4933  (0.164 sec)
18-06-05 01:36-INFO->> Step 442380 run_train: loss = 5.4356  (0.162 sec)
18-06-05 01:36-INFO->> Step 442390 run_train: loss = 5.4003  (0.163 sec)
18-06-05 01:36-INFO->> Step 442400 run_train: loss = 5.4482  (0.162 sec)
18-06-05 01:36-INFO->> Step 442410 run_train: loss = 5.4244  (0.193 sec)
18-06-05 01:36-INFO->> Step 442420 run_train: loss = 5.4445  (0.166 sec)
18-06-05 01:36-INFO->> Step 442430 run_train: loss = 5.4464  (0.149 sec)
18-06-05 01:36-INFO->> Step 442440 run_train: loss = 5.3721  (0.166 sec)
18-06-05 01:36-INFO->> Step 442450 run_train: loss = 5.3664  (0.161 sec)
18-06-05 01:36-INFO->> Step 442460 run_train: loss = 5.3591  (0.182 sec)
18-06-05 01:36-INFO->> Step 442470 run_train: loss = 5.3693  (0.159 sec)
18-06-05 01:36-INFO->> Step 442480 run_train: loss = 5.4254  (0.165 sec)
18-06-05 01:36-INFO->> Step 442490 run_train: loss = 5.4471  (0.171 sec)
18-06-05 01:36-INFO->> Step 442500 run_train: loss = 5.3477  (0.185 sec)
18-06-05 01:36-INFO->> Step 442510 run_train: loss = 5.3426  (0.143 sec)
18-06-05 01:36-INFO->> Step 442520 run_train: loss = 5.4073  (0.163 sec)
18-06-05 01:36-INFO->> Step 442530 run_train: loss = 5.4381  (0.131 sec)
18-06-05 01:36-INFO->> Step 442540 run_train: loss = 5.4291  (0.131 sec)
18-06-05 01:36-INFO->> Step 442550 run_train: loss = 5.4513  (0.176 sec)
18-06-05 01:36-INFO->> Step 442560 run_train: loss = 5.4375  (0.174 sec)
18-06-05 01:37-INFO->> Step 442570 run_train: loss = 5.4213  (0.192 sec)
18-06-05 01:37-INFO->> Step 442580 run_train: loss = 5.3500  (0.150 sec)
18-06-05 01:37-INFO->> Step 442590 run_train: loss = 5.4399  (0.173 sec)
18-06-05 01:37-INFO->> Step 442600 run_train: loss = 5.3888  (0.160 sec)
18-06-05 01:37-INFO->> Step 442610 run_train: loss = 5.4494  (0.173 sec)
18-06-05 01:37-INFO->> Step 442620 run_train: loss = 5.4645  (0.173 sec)
18-06-05 01:37-INFO->> Step 442630 run_train: loss = 5.4459  (0.189 sec)
18-06-05 01:37-INFO->> Step 442640 run_train: loss = 5.4013  (0.182 sec)
18-06-05 01:37-INFO->> Step 442650 run_train: loss = 5.3805  (0.170 sec)
18-06-05 01:37-INFO->> Step 442660 run_train: loss = 5.4070  (0.145 sec)
18-06-05 01:37-INFO->> Step 442670 run_train: loss = 5.4176  (0.154 sec)
18-06-05 01:37-INFO->> Step 442680 run_train: loss = 5.3983  (0.167 sec)
18-06-05 01:37-INFO->> Step 442690 run_train: loss = 5.3621  (0.166 sec)
18-06-05 01:37-INFO->> Step 442700 run_train: loss = 5.4723  (0.176 sec)
18-06-05 01:37-INFO->> Step 442710 run_train: loss = 5.4084  (0.157 sec)
18-06-05 01:37-INFO->> Step 442720 run_train: loss = 5.4144  (0.157 sec)
18-06-05 01:37-INFO->> Step 442730 run_train: loss = 5.4037  (0.151 sec)
18-06-05 01:37-INFO->> Step 442740 run_train: loss = 5.3909  (0.150 sec)
18-06-05 01:37-INFO->> Step 442750 run_train: loss = 5.3969  (0.168 sec)
18-06-05 01:37-INFO->> Step 442760 run_train: loss = 5.4437  (0.168 sec)
18-06-05 01:37-INFO->> Step 442770 run_train: loss = 5.3991  (0.158 sec)
18-06-05 01:37-INFO->> Step 442780 run_train: loss = 5.4062  (0.160 sec)
18-06-05 01:37-INFO->> Step 442790 run_train: loss = 5.4501  (0.173 sec)
18-06-05 01:37-INFO->> Step 442800 run_train: loss = 5.4255  (0.170 sec)
18-06-05 01:37-INFO->> Step 442810 run_train: loss = 5.3446  (0.195 sec)
18-06-05 01:37-INFO->> Step 442820 run_train: loss = 5.3911  (0.163 sec)
18-06-05 01:37-INFO->> Step 442830 run_train: loss = 5.4369  (0.152 sec)
18-06-05 01:37-INFO->> Step 442840 run_train: loss = 5.4513  (0.148 sec)
18-06-05 01:37-INFO->> Step 442850 run_train: loss = 5.3839  (0.149 sec)
18-06-05 01:37-INFO->> Step 442860 run_train: loss = 5.3822  (0.169 sec)
18-06-05 01:37-INFO->> Step 442870 run_train: loss = 5.3864  (0.125 sec)
18-06-05 01:37-INFO->> Step 442880 run_train: loss = 5.4487  (0.160 sec)
18-06-05 01:37-INFO->> Step 442890 run_train: loss = 5.4097  (0.113 sec)
18-06-05 01:37-INFO->> Step 442900 run_train: loss = 5.3899  (0.155 sec)
18-06-05 01:37-INFO->> Step 442910 run_train: loss = 5.5046  (0.166 sec)
18-06-05 01:37-INFO->> Step 442920 run_train: loss = 5.4207  (0.145 sec)
18-06-05 01:37-INFO->> Step 442930 run_train: loss = 5.4398  (0.193 sec)
18-06-05 01:37-INFO->> Step 442940 run_train: loss = 5.3339  (0.197 sec)
18-06-05 01:38-INFO->> Step 442950 run_train: loss = 5.3881  (0.126 sec)
18-06-05 01:38-INFO->> Step 442960 run_train: loss = 5.4311  (0.168 sec)
18-06-05 01:38-INFO->> Step 442970 run_train: loss = 5.4114  (0.158 sec)
18-06-05 01:38-INFO->> Step 442980 run_train: loss = 5.4019  (0.158 sec)
18-06-05 01:38-INFO->> Step 442990 run_train: loss = 5.4029  (0.147 sec)
18-06-05 01:38-INFO->> Step 443000 run_train: loss = 5.3812  (0.180 sec)
18-06-05 01:38-INFO->> 2018-06-05 01:38:08.763865 Saving in ckpt
18-06-05 01:38-INFO-Test Data Eval:
18-06-05 01:38-INFO-fpr95 = 0.16888615834218917 and auc = 0.9693043095406975
18-06-05 01:38-INFO->> Step 443010 run_train: loss = 5.4114  (0.162 sec)
18-06-05 01:38-INFO->> Step 443020 run_train: loss = 5.3939  (0.150 sec)
18-06-05 01:38-INFO->> Step 443030 run_train: loss = 5.4799  (0.171 sec)
18-06-05 01:38-INFO->> Step 443040 run_train: loss = 5.4066  (0.154 sec)
18-06-05 01:38-INFO->> Step 443050 run_train: loss = 5.4225  (0.163 sec)
18-06-05 01:38-INFO->> Step 443060 run_train: loss = 5.4192  (0.183 sec)
18-06-05 01:39-INFO->> Step 443070 run_train: loss = 5.4497  (0.174 sec)
18-06-05 01:39-INFO->> Step 443080 run_train: loss = 5.4644  (0.159 sec)
18-06-05 01:39-INFO->> Step 443090 run_train: loss = 5.4174  (0.169 sec)
18-06-05 01:39-INFO->> Step 443100 run_train: loss = 5.3896  (0.168 sec)
18-06-05 01:39-INFO->> Step 443110 run_train: loss = 5.4159  (0.201 sec)
18-06-05 01:39-INFO->> Step 443120 run_train: loss = 5.3726  (0.185 sec)
18-06-05 01:39-INFO->> Step 443130 run_train: loss = 5.3755  (0.165 sec)
18-06-05 01:39-INFO->> Step 443140 run_train: loss = 5.3861  (0.165 sec)
18-06-05 01:39-INFO->> Step 443150 run_train: loss = 5.3592  (0.148 sec)
18-06-05 01:39-INFO->> Step 443160 run_train: loss = 5.4283  (0.119 sec)
18-06-05 01:39-INFO->> Step 443170 run_train: loss = 5.3683  (0.146 sec)
18-06-05 01:39-INFO->> Step 443180 run_train: loss = 5.4008  (0.152 sec)
18-06-05 01:39-INFO->> Step 443190 run_train: loss = 5.4536  (0.155 sec)
18-06-05 01:39-INFO->> Step 443200 run_train: loss = 5.4462  (0.210 sec)
18-06-05 01:39-INFO->> Step 443210 run_train: loss = 5.4194  (0.147 sec)
18-06-05 01:39-INFO->> Step 443220 run_train: loss = 5.3704  (0.136 sec)
18-06-05 01:39-INFO->> Step 443230 run_train: loss = 5.4153  (0.157 sec)
18-06-05 01:39-INFO->> Step 443240 run_train: loss = 5.4157  (0.162 sec)
18-06-05 01:39-INFO->> Step 443250 run_train: loss = 5.3829  (0.178 sec)
18-06-05 01:39-INFO->> Step 443260 run_train: loss = 5.4860  (0.200 sec)
18-06-05 01:39-INFO->> Step 443270 run_train: loss = 5.4022  (0.162 sec)
18-06-05 01:39-INFO->> Step 443280 run_train: loss = 5.4098  (0.143 sec)
18-06-05 01:39-INFO->> Step 443290 run_train: loss = 5.3862  (0.181 sec)
18-06-05 01:39-INFO->> Step 443300 run_train: loss = 5.4514  (0.139 sec)
18-06-05 01:39-INFO->> Step 443310 run_train: loss = 5.3984  (0.164 sec)
18-06-05 01:39-INFO->> Step 443320 run_train: loss = 5.3807  (0.173 sec)
18-06-05 01:39-INFO->> Step 443330 run_train: loss = 5.4163  (0.156 sec)
18-06-05 01:39-INFO->> Step 443340 run_train: loss = 5.3843  (0.173 sec)
18-06-05 01:39-INFO->> Step 443350 run_train: loss = 5.4728  (0.178 sec)
18-06-05 01:39-INFO->> Step 443360 run_train: loss = 5.4053  (0.174 sec)
18-06-05 01:39-INFO->> Step 443370 run_train: loss = 5.4749  (0.180 sec)
18-06-05 01:39-INFO->> Step 443380 run_train: loss = 5.4399  (0.155 sec)
18-06-05 01:39-INFO->> Step 443390 run_train: loss = 5.4400  (0.104 sec)
18-06-05 01:39-INFO->> Step 443400 run_train: loss = 5.4118  (0.158 sec)
18-06-05 01:39-INFO->> Step 443410 run_train: loss = 5.4163  (0.149 sec)
18-06-05 01:39-INFO->> Step 443420 run_train: loss = 5.3933  (0.184 sec)
18-06-05 01:39-INFO->> Step 443430 run_train: loss = 5.3962  (0.172 sec)
18-06-05 01:39-INFO->> Step 443440 run_train: loss = 5.4335  (0.211 sec)
18-06-05 01:40-INFO->> Step 443450 run_train: loss = 5.4425  (0.158 sec)
18-06-05 01:40-INFO->> Step 443460 run_train: loss = 5.4779  (0.170 sec)
18-06-05 01:40-INFO->> Step 443470 run_train: loss = 5.4485  (0.136 sec)
18-06-05 01:40-INFO->> Step 443480 run_train: loss = 5.4124  (0.139 sec)
18-06-05 01:40-INFO->> Step 443490 run_train: loss = 5.4276  (0.183 sec)
18-06-05 01:40-INFO->> Step 443500 run_train: loss = 5.4669  (0.157 sec)
18-06-05 01:40-INFO->> Step 443510 run_train: loss = 5.4183  (0.169 sec)
18-06-05 01:40-INFO->> Step 443520 run_train: loss = 5.3883  (0.168 sec)
18-06-05 01:40-INFO->> Step 443530 run_train: loss = 5.4576  (0.186 sec)
18-06-05 01:40-INFO->> Step 443540 run_train: loss = 5.4411  (0.165 sec)
18-06-05 01:40-INFO->> Step 443550 run_train: loss = 5.4512  (0.169 sec)
18-06-05 01:40-INFO->> Step 443560 run_train: loss = 5.4798  (0.151 sec)
18-06-05 01:40-INFO->> Step 443570 run_train: loss = 5.3172  (0.173 sec)
18-06-05 01:40-INFO->> Step 443580 run_train: loss = 5.3722  (0.160 sec)
18-06-05 01:40-INFO->> Step 443590 run_train: loss = 5.3783  (0.157 sec)
18-06-05 01:40-INFO->> Step 443600 run_train: loss = 5.4061  (0.146 sec)
18-06-05 01:40-INFO->> Step 443610 run_train: loss = 5.4170  (0.157 sec)
18-06-05 01:40-INFO->> Step 443620 run_train: loss = 5.3999  (0.165 sec)
18-06-05 01:40-INFO->> Step 443630 run_train: loss = 5.4017  (0.151 sec)
18-06-05 01:40-INFO->> Step 443640 run_train: loss = 5.3740  (0.153 sec)
18-06-05 01:40-INFO->> Step 443650 run_train: loss = 5.3823  (0.172 sec)
18-06-05 01:40-INFO->> Step 443660 run_train: loss = 5.3417  (0.202 sec)
18-06-05 01:40-INFO->> Step 443670 run_train: loss = 5.4545  (0.157 sec)
18-06-05 01:40-INFO->> Step 443680 run_train: loss = 5.4688  (0.160 sec)
18-06-05 01:40-INFO->> Step 443690 run_train: loss = 5.3937  (0.159 sec)
18-06-05 01:40-INFO->> Step 443700 run_train: loss = 5.3537  (0.199 sec)
18-06-05 01:40-INFO->> Step 443710 run_train: loss = 5.5015  (0.145 sec)
18-06-05 01:40-INFO->> Step 443720 run_train: loss = 5.3623  (0.151 sec)
18-06-05 01:40-INFO->> Step 443730 run_train: loss = 5.4368  (0.154 sec)
18-06-05 01:40-INFO->> Step 443740 run_train: loss = 5.4727  (0.136 sec)
18-06-05 01:40-INFO->> Step 443750 run_train: loss = 5.3614  (0.158 sec)
18-06-05 01:40-INFO->> Step 443760 run_train: loss = 5.4045  (0.177 sec)
18-06-05 01:40-INFO->> Step 443770 run_train: loss = 5.4880  (0.137 sec)
18-06-05 01:40-INFO->> Step 443780 run_train: loss = 5.4968  (0.149 sec)
18-06-05 01:40-INFO->> Step 443790 run_train: loss = 5.4109  (0.163 sec)
18-06-05 01:40-INFO->> Step 443800 run_train: loss = 5.3920  (0.158 sec)
18-06-05 01:40-INFO->> Step 443810 run_train: loss = 5.4276  (0.171 sec)
18-06-05 01:40-INFO->> Step 443820 run_train: loss = 5.3644  (0.198 sec)
18-06-05 01:41-INFO->> Step 443830 run_train: loss = 5.3864  (0.170 sec)
18-06-05 01:41-INFO->> Step 443840 run_train: loss = 5.4258  (0.158 sec)
18-06-05 01:41-INFO->> Step 443850 run_train: loss = 5.3980  (0.176 sec)
18-06-05 01:41-INFO->> Step 443860 run_train: loss = 5.3657  (0.142 sec)
18-06-05 01:41-INFO->> Step 443870 run_train: loss = 5.4032  (0.150 sec)
18-06-05 01:41-INFO->> Step 443880 run_train: loss = 5.4647  (0.147 sec)
18-06-05 01:41-INFO->> Step 443890 run_train: loss = 5.4125  (0.170 sec)
18-06-05 01:41-INFO->> Step 443900 run_train: loss = 5.3709  (0.128 sec)
18-06-05 01:41-INFO->> Step 443910 run_train: loss = 5.4321  (0.152 sec)
18-06-05 01:41-INFO->> Step 443920 run_train: loss = 5.3778  (0.186 sec)
18-06-05 01:41-INFO->> Step 443930 run_train: loss = 5.3831  (0.155 sec)
18-06-05 01:41-INFO->> Step 443940 run_train: loss = 5.4149  (0.139 sec)
18-06-05 01:41-INFO->> Step 443950 run_train: loss = 5.4046  (0.195 sec)
18-06-05 01:41-INFO->> Step 443960 run_train: loss = 5.4289  (0.151 sec)
18-06-05 01:41-INFO->> Step 443970 run_train: loss = 5.4300  (0.145 sec)
18-06-05 01:41-INFO->> Step 443980 run_train: loss = 5.3865  (0.195 sec)
18-06-05 01:41-INFO->> Step 443990 run_train: loss = 5.4534  (0.188 sec)
18-06-05 01:41-INFO->> Step 444000 run_train: loss = 5.4377  (0.154 sec)
18-06-05 01:41-INFO->> 2018-06-05 01:41:28.192805 Saving in ckpt
18-06-05 01:41-INFO-Test Data Eval:
18-06-05 01:42-INFO-fpr95 = 0.17544500531349627 and auc = 0.969011763896444
18-06-05 01:42-INFO->> Step 444010 run_train: loss = 5.4022  (0.160 sec)
18-06-05 01:42-INFO->> Step 444020 run_train: loss = 5.4067  (0.151 sec)
18-06-05 01:42-INFO->> Step 444030 run_train: loss = 5.4547  (0.173 sec)
18-06-05 01:42-INFO->> Step 444040 run_train: loss = 5.3857  (0.164 sec)
18-06-05 01:42-INFO->> Step 444050 run_train: loss = 5.3463  (0.146 sec)
18-06-05 01:42-INFO->> Step 444060 run_train: loss = 5.4370  (0.154 sec)
18-06-05 01:42-INFO->> Step 444070 run_train: loss = 5.4090  (0.157 sec)
18-06-05 01:42-INFO->> Step 444080 run_train: loss = 5.4680  (0.160 sec)
18-06-05 01:42-INFO->> Step 444090 run_train: loss = 5.4592  (0.156 sec)
18-06-05 01:42-INFO->> Step 444100 run_train: loss = 5.3716  (0.152 sec)
18-06-05 01:42-INFO->> Step 444110 run_train: loss = 5.3988  (0.143 sec)
18-06-05 01:42-INFO->> Step 444120 run_train: loss = 5.4187  (0.138 sec)
18-06-05 01:42-INFO->> Step 444130 run_train: loss = 5.4280  (0.184 sec)
18-06-05 01:42-INFO->> Step 444140 run_train: loss = 5.4043  (0.146 sec)
18-06-05 01:42-INFO->> Step 444150 run_train: loss = 5.4012  (0.164 sec)
18-06-05 01:42-INFO->> Step 444160 run_train: loss = 5.4142  (0.180 sec)
18-06-05 01:42-INFO->> Step 444170 run_train: loss = 5.4548  (0.144 sec)
18-06-05 01:42-INFO->> Step 444180 run_train: loss = 5.3935  (0.187 sec)
18-06-05 01:42-INFO->> Step 444190 run_train: loss = 5.4827  (0.150 sec)
18-06-05 01:42-INFO->> Step 444200 run_train: loss = 5.3869  (0.152 sec)
18-06-05 01:42-INFO->> Step 444210 run_train: loss = 5.4234  (0.125 sec)
18-06-05 01:42-INFO->> Step 444220 run_train: loss = 5.4035  (0.162 sec)
18-06-05 01:42-INFO->> Step 444230 run_train: loss = 5.3911  (0.183 sec)
18-06-05 01:42-INFO->> Step 444240 run_train: loss = 5.4098  (0.161 sec)
18-06-05 01:42-INFO->> Step 444250 run_train: loss = 5.4016  (0.162 sec)
18-06-05 01:42-INFO->> Step 444260 run_train: loss = 5.3673  (0.179 sec)
18-06-05 01:42-INFO->> Step 444270 run_train: loss = 5.4258  (0.157 sec)
18-06-05 01:42-INFO->> Step 444280 run_train: loss = 5.4240  (0.150 sec)
18-06-05 01:42-INFO->> Step 444290 run_train: loss = 5.4482  (0.176 sec)
18-06-05 01:42-INFO->> Step 444300 run_train: loss = 5.3632  (0.167 sec)
18-06-05 01:42-INFO->> Step 444310 run_train: loss = 5.4440  (0.154 sec)
18-06-05 01:42-INFO->> Step 444320 run_train: loss = 5.4382  (0.165 sec)
18-06-05 01:43-INFO->> Step 444330 run_train: loss = 5.4285  (0.182 sec)
18-06-05 01:43-INFO->> Step 444340 run_train: loss = 5.4398  (0.172 sec)
18-06-05 01:43-INFO->> Step 444350 run_train: loss = 5.3831  (0.183 sec)
18-06-05 01:43-INFO->> Step 444360 run_train: loss = 5.4578  (0.167 sec)
18-06-05 01:43-INFO->> Step 444370 run_train: loss = 5.4577  (0.141 sec)
18-06-05 01:43-INFO->> Step 444380 run_train: loss = 5.3959  (0.145 sec)
18-06-05 01:43-INFO->> Step 444390 run_train: loss = 5.4605  (0.162 sec)
18-06-05 01:43-INFO->> Step 444400 run_train: loss = 5.4480  (0.146 sec)
18-06-05 01:43-INFO->> Step 444410 run_train: loss = 5.3508  (0.153 sec)
18-06-05 01:43-INFO->> Step 444420 run_train: loss = 5.3825  (0.109 sec)
18-06-05 01:43-INFO->> Step 444430 run_train: loss = 5.3998  (0.189 sec)
18-06-05 01:43-INFO->> Step 444440 run_train: loss = 5.4177  (0.152 sec)
18-06-05 01:43-INFO->> Step 444450 run_train: loss = 5.4112  (0.173 sec)
18-06-05 01:43-INFO->> Step 444460 run_train: loss = 5.4191  (0.154 sec)
18-06-05 01:43-INFO->> Step 444470 run_train: loss = 5.3846  (0.161 sec)
18-06-05 01:43-INFO->> Step 444480 run_train: loss = 5.3693  (0.159 sec)
18-06-05 01:43-INFO->> Step 444490 run_train: loss = 5.3593  (0.153 sec)
18-06-05 01:43-INFO->> Step 444500 run_train: loss = 5.3899  (0.159 sec)
18-06-05 01:43-INFO->> Step 444510 run_train: loss = 5.3995  (0.159 sec)
18-06-05 01:43-INFO->> Step 444520 run_train: loss = 5.4458  (0.174 sec)
18-06-05 01:43-INFO->> Step 444530 run_train: loss = 5.4138  (0.172 sec)
18-06-05 01:43-INFO->> Step 444540 run_train: loss = 5.3838  (0.125 sec)
18-06-05 01:43-INFO->> Step 444550 run_train: loss = 5.4072  (0.165 sec)
18-06-05 01:43-INFO->> Step 444560 run_train: loss = 5.4726  (0.132 sec)
18-06-05 01:43-INFO->> Step 444570 run_train: loss = 5.4173  (0.184 sec)
18-06-05 01:43-INFO->> Step 444580 run_train: loss = 5.4684  (0.149 sec)
18-06-05 01:43-INFO->> Step 444590 run_train: loss = 5.3507  (0.152 sec)
18-06-05 01:43-INFO->> Step 444600 run_train: loss = 5.3925  (0.169 sec)
18-06-05 01:43-INFO->> Step 444610 run_train: loss = 5.3632  (0.134 sec)
18-06-05 01:43-INFO->> Step 444620 run_train: loss = 5.4550  (0.168 sec)
18-06-05 01:43-INFO->> Step 444630 run_train: loss = 5.3266  (0.142 sec)
18-06-05 01:43-INFO->> Step 444640 run_train: loss = 5.4513  (0.135 sec)
18-06-05 01:43-INFO->> Step 444650 run_train: loss = 5.3433  (0.176 sec)
18-06-05 01:43-INFO->> Step 444660 run_train: loss = 5.4284  (0.171 sec)
18-06-05 01:43-INFO->> Step 444670 run_train: loss = 5.4144  (0.163 sec)
18-06-05 01:43-INFO->> Step 444680 run_train: loss = 5.4452  (0.204 sec)
18-06-05 01:43-INFO->> Step 444690 run_train: loss = 5.4025  (0.153 sec)
18-06-05 01:43-INFO->> Step 444700 run_train: loss = 5.3833  (0.173 sec)
18-06-05 01:44-INFO->> Step 444710 run_train: loss = 5.3599  (0.170 sec)
18-06-05 01:44-INFO->> Step 444720 run_train: loss = 5.5135  (0.139 sec)
18-06-05 01:44-INFO->> Step 444730 run_train: loss = 5.4073  (0.189 sec)
18-06-05 01:44-INFO->> Step 444740 run_train: loss = 5.4836  (0.156 sec)
18-06-05 01:44-INFO->> Step 444750 run_train: loss = 5.3870  (0.175 sec)
18-06-05 01:44-INFO->> Step 444760 run_train: loss = 5.4094  (0.169 sec)
18-06-05 01:44-INFO->> Step 444770 run_train: loss = 5.4186  (0.176 sec)
18-06-05 01:44-INFO->> Step 444780 run_train: loss = 5.3571  (0.154 sec)
18-06-05 01:44-INFO->> Step 444790 run_train: loss = 5.3807  (0.122 sec)
18-06-05 01:44-INFO->> Step 444800 run_train: loss = 5.4047  (0.149 sec)
18-06-05 01:44-INFO->> Step 444810 run_train: loss = 5.4103  (0.129 sec)
18-06-05 01:44-INFO->> Step 444820 run_train: loss = 5.3533  (0.163 sec)
18-06-05 01:44-INFO->> Step 444830 run_train: loss = 5.3980  (0.160 sec)
18-06-05 01:44-INFO->> Step 444840 run_train: loss = 5.3589  (0.145 sec)
18-06-05 01:44-INFO->> Step 444850 run_train: loss = 5.3924  (0.160 sec)
18-06-05 01:44-INFO->> Step 444860 run_train: loss = 5.4205  (0.157 sec)
18-06-05 01:44-INFO->> Step 444870 run_train: loss = 5.3971  (0.155 sec)
18-06-05 01:44-INFO->> Step 444880 run_train: loss = 5.3959  (0.179 sec)
18-06-05 01:44-INFO->> Step 444890 run_train: loss = 5.4581  (0.113 sec)
18-06-05 01:44-INFO->> Step 444900 run_train: loss = 5.4295  (0.169 sec)
18-06-05 01:44-INFO->> Step 444910 run_train: loss = 5.4160  (0.178 sec)
18-06-05 01:44-INFO->> Step 444920 run_train: loss = 5.3922  (0.134 sec)
18-06-05 01:44-INFO->> Step 444930 run_train: loss = 5.4110  (0.198 sec)
18-06-05 01:44-INFO->> Step 444940 run_train: loss = 5.4517  (0.159 sec)
18-06-05 01:44-INFO->> Step 444950 run_train: loss = 5.3864  (0.175 sec)
18-06-05 01:44-INFO->> Step 444960 run_train: loss = 5.4457  (0.169 sec)
18-06-05 01:44-INFO->> Step 444970 run_train: loss = 5.4344  (0.169 sec)
18-06-05 01:44-INFO->> Step 444980 run_train: loss = 5.3954  (0.171 sec)
18-06-05 01:44-INFO->> Step 444990 run_train: loss = 5.4187  (0.158 sec)
18-06-05 01:44-INFO->> Step 445000 run_train: loss = 5.4280  (0.152 sec)
18-06-05 01:44-INFO->> 2018-06-05 01:44:47.335028 Saving in ckpt
18-06-05 01:44-INFO-Test Data Eval:
18-06-05 01:45-INFO-fpr95 = 0.1741415382571732 and auc = 0.9690223631148283
18-06-05 01:45-INFO->> Step 445010 run_train: loss = 5.4407  (0.131 sec)
18-06-05 01:45-INFO->> Step 445020 run_train: loss = 5.4816  (0.170 sec)
18-06-05 01:45-INFO->> Step 445030 run_train: loss = 5.4426  (0.129 sec)
18-06-05 01:45-INFO->> Step 445040 run_train: loss = 5.3603  (0.165 sec)
18-06-05 01:45-INFO->> Step 445050 run_train: loss = 5.4787  (0.198 sec)
18-06-05 01:45-INFO->> Step 445060 run_train: loss = 5.4473  (0.151 sec)
18-06-05 01:45-INFO->> Step 445070 run_train: loss = 5.3908  (0.158 sec)
18-06-05 01:45-INFO->> Step 445080 run_train: loss = 5.4497  (0.156 sec)
18-06-05 01:45-INFO->> Step 445090 run_train: loss = 5.4313  (0.150 sec)
18-06-05 01:45-INFO->> Step 445100 run_train: loss = 5.3844  (0.167 sec)
18-06-05 01:45-INFO->> Step 445110 run_train: loss = 5.3664  (0.176 sec)
18-06-05 01:45-INFO->> Step 445120 run_train: loss = 5.3700  (0.158 sec)
18-06-05 01:45-INFO->> Step 445130 run_train: loss = 5.3787  (0.149 sec)
18-06-05 01:45-INFO->> Step 445140 run_train: loss = 5.4408  (0.173 sec)
18-06-05 01:45-INFO->> Step 445150 run_train: loss = 5.3969  (0.174 sec)
18-06-05 01:45-INFO->> Step 445160 run_train: loss = 5.3840  (0.159 sec)
18-06-05 01:45-INFO->> Step 445170 run_train: loss = 5.4008  (0.177 sec)
18-06-05 01:45-INFO->> Step 445180 run_train: loss = 5.4597  (0.146 sec)
18-06-05 01:45-INFO->> Step 445190 run_train: loss = 5.4406  (0.180 sec)
18-06-05 01:45-INFO->> Step 445200 run_train: loss = 5.4518  (0.136 sec)
18-06-05 01:46-INFO->> Step 445210 run_train: loss = 5.4257  (0.150 sec)
18-06-05 01:46-INFO->> Step 445220 run_train: loss = 5.3187  (0.159 sec)
18-06-05 01:46-INFO->> Step 445230 run_train: loss = 5.4054  (0.168 sec)
18-06-05 01:46-INFO->> Step 445240 run_train: loss = 5.3301  (0.134 sec)
18-06-05 01:46-INFO->> Step 445250 run_train: loss = 5.4505  (0.186 sec)
18-06-05 01:46-INFO->> Step 445260 run_train: loss = 5.4342  (0.140 sec)
18-06-05 01:46-INFO->> Step 445270 run_train: loss = 5.3839  (0.182 sec)
18-06-05 01:46-INFO->> Step 445280 run_train: loss = 5.3795  (0.184 sec)
18-06-05 01:46-INFO->> Step 445290 run_train: loss = 5.4271  (0.117 sec)
18-06-05 01:46-INFO->> Step 445300 run_train: loss = 5.4101  (0.171 sec)
18-06-05 01:46-INFO->> Step 445310 run_train: loss = 5.4531  (0.148 sec)
18-06-05 01:46-INFO->> Step 445320 run_train: loss = 5.4077  (0.170 sec)
18-06-05 01:46-INFO->> Step 445330 run_train: loss = 5.4099  (0.148 sec)
18-06-05 01:46-INFO->> Step 445340 run_train: loss = 5.3522  (0.175 sec)
18-06-05 01:46-INFO->> Step 445350 run_train: loss = 5.4025  (0.187 sec)
18-06-05 01:46-INFO->> Step 445360 run_train: loss = 5.3776  (0.141 sec)
18-06-05 01:46-INFO->> Step 445370 run_train: loss = 5.3877  (0.210 sec)
18-06-05 01:46-INFO->> Step 445380 run_train: loss = 5.4608  (0.127 sec)
18-06-05 01:46-INFO->> Step 445390 run_train: loss = 5.4900  (0.131 sec)
18-06-05 01:46-INFO->> Step 445400 run_train: loss = 5.4056  (0.156 sec)
18-06-05 01:46-INFO->> Step 445410 run_train: loss = 5.4055  (0.162 sec)
18-06-05 01:46-INFO->> Step 445420 run_train: loss = 5.3811  (0.171 sec)
18-06-05 01:46-INFO->> Step 445430 run_train: loss = 5.4582  (0.148 sec)
18-06-05 01:46-INFO->> Step 445440 run_train: loss = 5.4593  (0.152 sec)
18-06-05 01:46-INFO->> Step 445450 run_train: loss = 5.3923  (0.131 sec)
18-06-05 01:46-INFO->> Step 445460 run_train: loss = 5.3716  (0.156 sec)
18-06-05 01:46-INFO->> Step 445470 run_train: loss = 5.4680  (0.154 sec)
18-06-05 01:46-INFO->> Step 445480 run_train: loss = 5.4332  (0.142 sec)
18-06-05 01:46-INFO->> Step 445490 run_train: loss = 5.3685  (0.168 sec)
18-06-05 01:46-INFO->> Step 445500 run_train: loss = 5.4689  (0.164 sec)
18-06-05 01:46-INFO->> Step 445510 run_train: loss = 5.3907  (0.150 sec)
18-06-05 01:46-INFO->> Step 445520 run_train: loss = 5.3989  (0.155 sec)
18-06-05 01:46-INFO->> Step 445530 run_train: loss = 5.3333  (0.151 sec)
18-06-05 01:46-INFO->> Step 445540 run_train: loss = 5.4350  (0.141 sec)
18-06-05 01:46-INFO->> Step 445550 run_train: loss = 5.3751  (0.170 sec)
18-06-05 01:46-INFO->> Step 445560 run_train: loss = 5.4206  (0.181 sec)
18-06-05 01:46-INFO->> Step 445570 run_train: loss = 5.4418  (0.150 sec)
18-06-05 01:46-INFO->> Step 445580 run_train: loss = 5.3693  (0.149 sec)
18-06-05 01:47-INFO->> Step 445590 run_train: loss = 5.4418  (0.181 sec)
18-06-05 01:47-INFO->> Step 445600 run_train: loss = 5.4067  (0.133 sec)
18-06-05 01:47-INFO->> Step 445610 run_train: loss = 5.4825  (0.165 sec)
18-06-05 01:47-INFO->> Step 445620 run_train: loss = 5.3763  (0.158 sec)
18-06-05 01:47-INFO->> Step 445630 run_train: loss = 5.3529  (0.143 sec)
18-06-05 01:47-INFO->> Step 445640 run_train: loss = 5.4670  (0.176 sec)
18-06-05 01:47-INFO->> Step 445650 run_train: loss = 5.3434  (0.164 sec)
18-06-05 01:47-INFO->> Step 445660 run_train: loss = 5.4261  (0.197 sec)
18-06-05 01:47-INFO->> Step 445670 run_train: loss = 5.4121  (0.181 sec)
18-06-05 01:47-INFO->> Step 445680 run_train: loss = 5.3629  (0.140 sec)
18-06-05 01:47-INFO->> Step 445690 run_train: loss = 5.4629  (0.159 sec)
18-06-05 01:47-INFO->> Step 445700 run_train: loss = 5.4797  (0.182 sec)
18-06-05 01:47-INFO->> Step 445710 run_train: loss = 5.3337  (0.142 sec)
18-06-05 01:47-INFO->> Step 445720 run_train: loss = 5.4738  (0.150 sec)
18-06-05 01:47-INFO->> Step 445730 run_train: loss = 5.4354  (0.155 sec)
18-06-05 01:47-INFO->> Step 445740 run_train: loss = 5.3859  (0.154 sec)
18-06-05 01:47-INFO->> Step 445750 run_train: loss = 5.4532  (0.178 sec)
18-06-05 01:47-INFO->> Step 445760 run_train: loss = 5.3776  (0.126 sec)
18-06-05 01:47-INFO->> Step 445770 run_train: loss = 5.3842  (0.157 sec)
18-06-05 01:47-INFO->> Step 445780 run_train: loss = 5.4287  (0.158 sec)
18-06-05 01:47-INFO->> Step 445790 run_train: loss = 5.4115  (0.157 sec)
18-06-05 01:47-INFO->> Step 445800 run_train: loss = 5.4315  (0.188 sec)
18-06-05 01:47-INFO->> Step 445810 run_train: loss = 5.3708  (0.151 sec)
18-06-05 01:47-INFO->> Step 445820 run_train: loss = 5.4551  (0.167 sec)
18-06-05 01:47-INFO->> Step 445830 run_train: loss = 5.4002  (0.162 sec)
18-06-05 01:47-INFO->> Step 445840 run_train: loss = 5.4506  (0.146 sec)
18-06-05 01:47-INFO->> Step 445850 run_train: loss = 5.3550  (0.197 sec)
18-06-05 01:47-INFO->> Step 445860 run_train: loss = 5.4508  (0.163 sec)
18-06-05 01:47-INFO->> Step 445870 run_train: loss = 5.4087  (0.162 sec)
18-06-05 01:47-INFO->> Step 445880 run_train: loss = 5.4261  (0.147 sec)
18-06-05 01:47-INFO->> Step 445890 run_train: loss = 5.4067  (0.126 sec)
18-06-05 01:47-INFO->> Step 445900 run_train: loss = 5.4737  (0.154 sec)
18-06-05 01:47-INFO->> Step 445910 run_train: loss = 5.4281  (0.138 sec)
18-06-05 01:47-INFO->> Step 445920 run_train: loss = 5.4635  (0.136 sec)
18-06-05 01:47-INFO->> Step 445930 run_train: loss = 5.4003  (0.137 sec)
18-06-05 01:47-INFO->> Step 445940 run_train: loss = 5.4646  (0.157 sec)
18-06-05 01:47-INFO->> Step 445950 run_train: loss = 5.4168  (0.140 sec)
18-06-05 01:47-INFO->> Step 445960 run_train: loss = 5.5351  (0.167 sec)
18-06-05 01:48-INFO->> Step 445970 run_train: loss = 5.4366  (0.129 sec)
18-06-05 01:48-INFO->> Step 445980 run_train: loss = 5.3891  (0.155 sec)
18-06-05 01:48-INFO->> Step 445990 run_train: loss = 5.4695  (0.138 sec)
18-06-05 01:48-INFO->> Step 446000 run_train: loss = 5.4258  (0.182 sec)
18-06-05 01:48-INFO->> 2018-06-05 01:48:06.084412 Saving in ckpt
18-06-05 01:48-INFO-Test Data Eval:
18-06-05 01:48-INFO-fpr95 = 0.17301242029755579 and auc = 0.9689059592679075
18-06-05 01:48-INFO->> Step 446010 run_train: loss = 5.4202  (0.155 sec)
18-06-05 01:48-INFO->> Step 446020 run_train: loss = 5.4665  (0.155 sec)
18-06-05 01:48-INFO->> Step 446030 run_train: loss = 5.3657  (0.162 sec)
18-06-05 01:48-INFO->> Step 446040 run_train: loss = 5.4594  (0.137 sec)
18-06-05 01:48-INFO->> Step 446050 run_train: loss = 5.3635  (0.156 sec)
18-06-05 01:48-INFO->> Step 446060 run_train: loss = 5.4944  (0.166 sec)
18-06-05 01:48-INFO->> Step 446070 run_train: loss = 5.3738  (0.109 sec)
18-06-05 01:48-INFO->> Step 446080 run_train: loss = 5.4104  (0.141 sec)
18-06-05 01:49-INFO->> Step 446090 run_train: loss = 5.4645  (0.190 sec)
18-06-05 01:49-INFO->> Step 446100 run_train: loss = 5.3682  (0.132 sec)
18-06-05 01:49-INFO->> Step 446110 run_train: loss = 5.3853  (0.182 sec)
18-06-05 01:49-INFO->> Step 446120 run_train: loss = 5.3907  (0.122 sec)
18-06-05 01:49-INFO->> Step 446130 run_train: loss = 5.3182  (0.159 sec)
18-06-05 01:49-INFO->> Step 446140 run_train: loss = 5.4012  (0.154 sec)
18-06-05 01:49-INFO->> Step 446150 run_train: loss = 5.4317  (0.128 sec)
18-06-05 01:49-INFO->> Step 446160 run_train: loss = 5.4255  (0.148 sec)
18-06-05 01:49-INFO->> Step 446170 run_train: loss = 5.4468  (0.149 sec)
18-06-05 01:49-INFO->> Step 446180 run_train: loss = 5.4331  (0.125 sec)
18-06-05 01:49-INFO->> Step 446190 run_train: loss = 5.3663  (0.166 sec)
18-06-05 01:49-INFO->> Step 446200 run_train: loss = 5.4288  (0.171 sec)
18-06-05 01:49-INFO->> Step 446210 run_train: loss = 5.2885  (0.157 sec)
18-06-05 01:49-INFO->> Step 446220 run_train: loss = 5.3589  (0.201 sec)
18-06-05 01:49-INFO->> Step 446230 run_train: loss = 5.4525  (0.148 sec)
18-06-05 01:49-INFO->> Step 446240 run_train: loss = 5.4103  (0.175 sec)
18-06-05 01:49-INFO->> Step 446250 run_train: loss = 5.4137  (0.153 sec)
18-06-05 01:49-INFO->> Step 446260 run_train: loss = 5.4453  (0.196 sec)
18-06-05 01:49-INFO->> Step 446270 run_train: loss = 5.4061  (0.136 sec)
18-06-05 01:49-INFO->> Step 446280 run_train: loss = 5.3900  (0.193 sec)
18-06-05 01:49-INFO->> Step 446290 run_train: loss = 5.3917  (0.140 sec)
18-06-05 01:49-INFO->> Step 446300 run_train: loss = 5.4569  (0.121 sec)
18-06-05 01:49-INFO->> Step 446310 run_train: loss = 5.4431  (0.156 sec)
18-06-05 01:49-INFO->> Step 446320 run_train: loss = 5.4157  (0.162 sec)
18-06-05 01:49-INFO->> Step 446330 run_train: loss = 5.3689  (0.175 sec)
18-06-05 01:49-INFO->> Step 446340 run_train: loss = 5.3976  (0.175 sec)
18-06-05 01:49-INFO->> Step 446350 run_train: loss = 5.4321  (0.118 sec)
18-06-05 01:49-INFO->> Step 446360 run_train: loss = 5.4036  (0.182 sec)
18-06-05 01:49-INFO->> Step 446370 run_train: loss = 5.3914  (0.183 sec)
18-06-05 01:49-INFO->> Step 446380 run_train: loss = 5.4018  (0.186 sec)
18-06-05 01:49-INFO->> Step 446390 run_train: loss = 5.4142  (0.146 sec)
18-06-05 01:49-INFO->> Step 446400 run_train: loss = 5.4516  (0.149 sec)
18-06-05 01:49-INFO->> Step 446410 run_train: loss = 5.4323  (0.161 sec)
18-06-05 01:49-INFO->> Step 446420 run_train: loss = 5.3692  (0.142 sec)
18-06-05 01:49-INFO->> Step 446430 run_train: loss = 5.3882  (0.176 sec)
18-06-05 01:49-INFO->> Step 446440 run_train: loss = 5.4473  (0.216 sec)
18-06-05 01:49-INFO->> Step 446450 run_train: loss = 5.4190  (0.171 sec)
18-06-05 01:49-INFO->> Step 446460 run_train: loss = 5.4067  (0.151 sec)
18-06-05 01:50-INFO->> Step 446470 run_train: loss = 5.4214  (0.138 sec)
18-06-05 01:50-INFO->> Step 446480 run_train: loss = 5.4642  (0.179 sec)
18-06-05 01:50-INFO->> Step 446490 run_train: loss = 5.4420  (0.179 sec)
18-06-05 01:50-INFO->> Step 446500 run_train: loss = 5.4242  (0.151 sec)
18-06-05 01:50-INFO->> Step 446510 run_train: loss = 5.4275  (0.185 sec)
18-06-05 01:50-INFO->> Step 446520 run_train: loss = 5.4760  (0.165 sec)
18-06-05 01:50-INFO->> Step 446530 run_train: loss = 5.4370  (0.171 sec)
18-06-05 01:50-INFO->> Step 446540 run_train: loss = 5.4239  (0.171 sec)
18-06-05 01:50-INFO->> Step 446550 run_train: loss = 5.4169  (0.157 sec)
18-06-05 01:50-INFO->> Step 446560 run_train: loss = 5.4611  (0.124 sec)
18-06-05 01:50-INFO->> Step 446570 run_train: loss = 5.4158  (0.154 sec)
18-06-05 01:50-INFO->> Step 446580 run_train: loss = 5.3933  (0.141 sec)
18-06-05 01:50-INFO->> Step 446590 run_train: loss = 5.3521  (0.177 sec)
18-06-05 01:50-INFO->> Step 446600 run_train: loss = 5.4174  (0.174 sec)
18-06-05 01:50-INFO->> Step 446610 run_train: loss = 5.4530  (0.158 sec)
18-06-05 01:50-INFO->> Step 446620 run_train: loss = 5.4508  (0.172 sec)
18-06-05 01:50-INFO->> Step 446630 run_train: loss = 5.4197  (0.139 sec)
18-06-05 01:50-INFO->> Step 446640 run_train: loss = 5.3924  (0.136 sec)
18-06-05 01:50-INFO->> Step 446650 run_train: loss = 5.4474  (0.150 sec)
18-06-05 01:50-INFO->> Step 446660 run_train: loss = 5.4494  (0.144 sec)
18-06-05 01:50-INFO->> Step 446670 run_train: loss = 5.3690  (0.164 sec)
18-06-05 01:50-INFO->> Step 446680 run_train: loss = 5.4058  (0.184 sec)
18-06-05 01:50-INFO->> Step 446690 run_train: loss = 5.3933  (0.170 sec)
18-06-05 01:50-INFO->> Step 446700 run_train: loss = 5.3605  (0.214 sec)
18-06-05 01:50-INFO->> Step 446710 run_train: loss = 5.3850  (0.160 sec)
18-06-05 01:50-INFO->> Step 446720 run_train: loss = 5.4383  (0.150 sec)
18-06-05 01:50-INFO->> Step 446730 run_train: loss = 5.3673  (0.183 sec)
18-06-05 01:50-INFO->> Step 446740 run_train: loss = 5.3739  (0.166 sec)
18-06-05 01:50-INFO->> Step 446750 run_train: loss = 5.4835  (0.164 sec)
18-06-05 01:50-INFO->> Step 446760 run_train: loss = 5.4517  (0.172 sec)
18-06-05 01:50-INFO->> Step 446770 run_train: loss = 5.4389  (0.163 sec)
18-06-05 01:50-INFO->> Step 446780 run_train: loss = 5.3478  (0.157 sec)
18-06-05 01:50-INFO->> Step 446790 run_train: loss = 5.5135  (0.144 sec)
18-06-05 01:50-INFO->> Step 446800 run_train: loss = 5.3883  (0.204 sec)
18-06-05 01:50-INFO->> Step 446810 run_train: loss = 5.4037  (0.154 sec)
18-06-05 01:50-INFO->> Step 446820 run_train: loss = 5.4636  (0.157 sec)
18-06-05 01:50-INFO->> Step 446830 run_train: loss = 5.4148  (0.203 sec)
18-06-05 01:50-INFO->> Step 446840 run_train: loss = 5.4417  (0.171 sec)
18-06-05 01:51-INFO->> Step 446850 run_train: loss = 5.4560  (0.143 sec)
18-06-05 01:51-INFO->> Step 446860 run_train: loss = 5.5030  (0.127 sec)
18-06-05 01:51-INFO->> Step 446870 run_train: loss = 5.4637  (0.172 sec)
18-06-05 01:51-INFO->> Step 446880 run_train: loss = 5.3967  (0.189 sec)
18-06-05 01:51-INFO->> Step 446890 run_train: loss = 5.4468  (0.171 sec)
18-06-05 01:51-INFO->> Step 446900 run_train: loss = 5.4260  (0.144 sec)
18-06-05 01:51-INFO->> Step 446910 run_train: loss = 5.4677  (0.172 sec)
18-06-05 01:51-INFO->> Step 446920 run_train: loss = 5.4096  (0.175 sec)
18-06-05 01:51-INFO->> Step 446930 run_train: loss = 5.4316  (0.167 sec)
18-06-05 01:51-INFO->> Step 446940 run_train: loss = 5.3757  (0.144 sec)
18-06-05 01:51-INFO->> Step 446950 run_train: loss = 5.4852  (0.193 sec)
18-06-05 01:51-INFO->> Step 446960 run_train: loss = 5.4700  (0.182 sec)
18-06-05 01:51-INFO->> Step 446970 run_train: loss = 5.3057  (0.159 sec)
18-06-05 01:51-INFO->> Step 446980 run_train: loss = 5.3402  (0.140 sec)
18-06-05 01:51-INFO->> Step 446990 run_train: loss = 5.4235  (0.148 sec)
18-06-05 01:51-INFO->> Step 447000 run_train: loss = 5.4298  (0.149 sec)
18-06-05 01:51-INFO->> 2018-06-05 01:51:25.388987 Saving in ckpt
18-06-05 01:51-INFO-Test Data Eval:
18-06-05 01:52-INFO-fpr95 = 0.1746230738575983 and auc = 0.9690210326853009
18-06-05 01:52-INFO->> Step 447010 run_train: loss = 5.4277  (0.124 sec)
18-06-05 01:52-INFO->> Step 447020 run_train: loss = 5.4021  (0.220 sec)
18-06-05 01:52-INFO->> Step 447030 run_train: loss = 5.3943  (0.192 sec)
18-06-05 01:52-INFO->> Step 447040 run_train: loss = 5.3977  (0.147 sec)
18-06-05 01:52-INFO->> Step 447050 run_train: loss = 5.3819  (0.156 sec)
18-06-05 01:52-INFO->> Step 447060 run_train: loss = 5.4396  (0.153 sec)
18-06-05 01:52-INFO->> Step 447070 run_train: loss = 5.4073  (0.174 sec)
18-06-05 01:52-INFO->> Step 447080 run_train: loss = 5.3901  (0.160 sec)
18-06-05 01:52-INFO->> Step 447090 run_train: loss = 5.4749  (0.161 sec)
18-06-05 01:52-INFO->> Step 447100 run_train: loss = 5.4114  (0.193 sec)
18-06-05 01:52-INFO->> Step 447110 run_train: loss = 5.4412  (0.115 sec)
18-06-05 01:52-INFO->> Step 447120 run_train: loss = 5.4196  (0.163 sec)
18-06-05 01:52-INFO->> Step 447130 run_train: loss = 5.4362  (0.153 sec)
18-06-05 01:52-INFO->> Step 447140 run_train: loss = 5.3968  (0.156 sec)
18-06-05 01:52-INFO->> Step 447150 run_train: loss = 5.4253  (0.161 sec)
18-06-05 01:52-INFO->> Step 447160 run_train: loss = 5.4068  (0.161 sec)
18-06-05 01:52-INFO->> Step 447170 run_train: loss = 5.3701  (0.140 sec)
18-06-05 01:52-INFO->> Step 447180 run_train: loss = 5.4064  (0.131 sec)
18-06-05 01:52-INFO->> Step 447190 run_train: loss = 5.3805  (0.161 sec)
18-06-05 01:52-INFO->> Step 447200 run_train: loss = 5.4395  (0.145 sec)
18-06-05 01:52-INFO->> Step 447210 run_train: loss = 5.3685  (0.151 sec)
18-06-05 01:52-INFO->> Step 447220 run_train: loss = 5.4382  (0.157 sec)
18-06-05 01:52-INFO->> Step 447230 run_train: loss = 5.4903  (0.195 sec)
18-06-05 01:52-INFO->> Step 447240 run_train: loss = 5.3946  (0.166 sec)
18-06-05 01:52-INFO->> Step 447250 run_train: loss = 5.3711  (0.188 sec)
18-06-05 01:52-INFO->> Step 447260 run_train: loss = 5.4237  (0.177 sec)
18-06-05 01:52-INFO->> Step 447270 run_train: loss = 5.4647  (0.165 sec)
18-06-05 01:52-INFO->> Step 447280 run_train: loss = 5.4207  (0.147 sec)
18-06-05 01:52-INFO->> Step 447290 run_train: loss = 5.4527  (0.143 sec)
18-06-05 01:52-INFO->> Step 447300 run_train: loss = 5.4349  (0.169 sec)
18-06-05 01:52-INFO->> Step 447310 run_train: loss = 5.4642  (0.147 sec)
18-06-05 01:52-INFO->> Step 447320 run_train: loss = 5.5044  (0.172 sec)
18-06-05 01:52-INFO->> Step 447330 run_train: loss = 5.4210  (0.194 sec)
18-06-05 01:52-INFO->> Step 447340 run_train: loss = 5.4333  (0.198 sec)
18-06-05 01:53-INFO->> Step 447350 run_train: loss = 5.3787  (0.144 sec)
18-06-05 01:53-INFO->> Step 447360 run_train: loss = 5.3983  (0.184 sec)
18-06-05 01:53-INFO->> Step 447370 run_train: loss = 5.4413  (0.157 sec)
18-06-05 01:53-INFO->> Step 447380 run_train: loss = 5.4684  (0.154 sec)
18-06-05 01:53-INFO->> Step 447390 run_train: loss = 5.3910  (0.173 sec)
18-06-05 01:53-INFO->> Step 447400 run_train: loss = 5.3918  (0.140 sec)
18-06-05 01:53-INFO->> Step 447410 run_train: loss = 5.4867  (0.156 sec)
18-06-05 01:53-INFO->> Step 447420 run_train: loss = 5.4446  (0.199 sec)
18-06-05 01:53-INFO->> Step 447430 run_train: loss = 5.4094  (0.122 sec)
18-06-05 01:53-INFO->> Step 447440 run_train: loss = 5.4060  (0.131 sec)
18-06-05 01:53-INFO->> Step 447450 run_train: loss = 5.4029  (0.169 sec)
18-06-05 01:53-INFO->> Step 447460 run_train: loss = 5.4167  (0.169 sec)
18-06-05 01:53-INFO->> Step 447470 run_train: loss = 5.3938  (0.157 sec)
18-06-05 01:53-INFO->> Step 447480 run_train: loss = 5.4203  (0.164 sec)
18-06-05 01:53-INFO->> Step 447490 run_train: loss = 5.4005  (0.151 sec)
18-06-05 01:53-INFO->> Step 447500 run_train: loss = 5.3856  (0.143 sec)
18-06-05 01:53-INFO->> Step 447510 run_train: loss = 5.3717  (0.158 sec)
18-06-05 01:53-INFO->> Step 447520 run_train: loss = 5.3754  (0.137 sec)
18-06-05 01:53-INFO->> Step 447530 run_train: loss = 5.5157  (0.164 sec)
18-06-05 01:53-INFO->> Step 447540 run_train: loss = 5.4407  (0.186 sec)
18-06-05 01:53-INFO->> Step 447550 run_train: loss = 5.4256  (0.147 sec)
18-06-05 01:53-INFO->> Step 447560 run_train: loss = 5.4190  (0.158 sec)
18-06-05 01:53-INFO->> Step 447570 run_train: loss = 5.3929  (0.166 sec)
18-06-05 01:53-INFO->> Step 447580 run_train: loss = 5.4519  (0.158 sec)
18-06-05 01:53-INFO->> Step 447590 run_train: loss = 5.4313  (0.202 sec)
18-06-05 01:53-INFO->> Step 447600 run_train: loss = 5.4404  (0.133 sec)
18-06-05 01:53-INFO->> Step 447610 run_train: loss = 5.4429  (0.144 sec)
18-06-05 01:53-INFO->> Step 447620 run_train: loss = 5.3848  (0.195 sec)
18-06-05 01:53-INFO->> Step 447630 run_train: loss = 5.4147  (0.164 sec)
18-06-05 01:53-INFO->> Step 447640 run_train: loss = 5.3813  (0.151 sec)
18-06-05 01:53-INFO->> Step 447650 run_train: loss = 5.4491  (0.166 sec)
18-06-05 01:53-INFO->> Step 447660 run_train: loss = 5.3687  (0.150 sec)
18-06-05 01:53-INFO->> Step 447670 run_train: loss = 5.4061  (0.179 sec)
18-06-05 01:53-INFO->> Step 447680 run_train: loss = 5.4160  (0.163 sec)
18-06-05 01:53-INFO->> Step 447690 run_train: loss = 5.3712  (0.157 sec)
18-06-05 01:53-INFO->> Step 447700 run_train: loss = 5.4642  (0.176 sec)
18-06-05 01:53-INFO->> Step 447710 run_train: loss = 5.3799  (0.178 sec)
18-06-05 01:54-INFO->> Step 447720 run_train: loss = 5.4280  (0.176 sec)
18-06-05 01:54-INFO->> Step 447730 run_train: loss = 5.4106  (0.136 sec)
18-06-05 01:54-INFO->> Step 447740 run_train: loss = 5.3805  (0.186 sec)
18-06-05 01:54-INFO->> Step 447750 run_train: loss = 5.3885  (0.152 sec)
18-06-05 01:54-INFO->> Step 447760 run_train: loss = 5.4127  (0.166 sec)
18-06-05 01:54-INFO->> Step 447770 run_train: loss = 5.4223  (0.151 sec)
18-06-05 01:54-INFO->> Step 447780 run_train: loss = 5.3802  (0.165 sec)
18-06-05 01:54-INFO->> Step 447790 run_train: loss = 5.3459  (0.145 sec)
18-06-05 01:54-INFO->> Step 447800 run_train: loss = 5.3762  (0.151 sec)
18-06-05 01:54-INFO->> Step 447810 run_train: loss = 5.4081  (0.170 sec)
18-06-05 01:54-INFO->> Step 447820 run_train: loss = 5.4201  (0.135 sec)
18-06-05 01:54-INFO->> Step 447830 run_train: loss = 5.3988  (0.166 sec)
18-06-05 01:54-INFO->> Step 447840 run_train: loss = 5.4349  (0.139 sec)
18-06-05 01:54-INFO->> Step 447850 run_train: loss = 5.4271  (0.170 sec)
18-06-05 01:54-INFO->> Step 447860 run_train: loss = 5.4169  (0.176 sec)
18-06-05 01:54-INFO->> Step 447870 run_train: loss = 5.4038  (0.152 sec)
18-06-05 01:54-INFO->> Step 447880 run_train: loss = 5.4359  (0.165 sec)
18-06-05 01:54-INFO->> Step 447890 run_train: loss = 5.4334  (0.161 sec)
18-06-05 01:54-INFO->> Step 447900 run_train: loss = 5.3897  (0.183 sec)
18-06-05 01:54-INFO->> Step 447910 run_train: loss = 5.3947  (0.117 sec)
18-06-05 01:54-INFO->> Step 447920 run_train: loss = 5.4063  (0.147 sec)
18-06-05 01:54-INFO->> Step 447930 run_train: loss = 5.4066  (0.137 sec)
18-06-05 01:54-INFO->> Step 447940 run_train: loss = 5.4152  (0.192 sec)
18-06-05 01:54-INFO->> Step 447950 run_train: loss = 5.3178  (0.202 sec)
18-06-05 01:54-INFO->> Step 447960 run_train: loss = 5.3606  (0.165 sec)
18-06-05 01:54-INFO->> Step 447970 run_train: loss = 5.3502  (0.174 sec)
18-06-05 01:54-INFO->> Step 447980 run_train: loss = 5.4313  (0.179 sec)
18-06-05 01:54-INFO->> Step 447990 run_train: loss = 5.4450  (0.149 sec)
18-06-05 01:54-INFO->> Step 448000 run_train: loss = 5.3911  (0.187 sec)
18-06-05 01:54-INFO->> 2018-06-05 01:54:45.112509 Saving in ckpt
18-06-05 01:54-INFO-Test Data Eval:
18-06-05 01:55-INFO-fpr95 = 0.17272183846971306 and auc = 0.9693559096173237
18-06-05 01:55-INFO->> Step 448010 run_train: loss = 5.4212  (0.166 sec)
18-06-05 01:55-INFO->> Step 448020 run_train: loss = 5.3841  (0.136 sec)
18-06-05 01:55-INFO->> Step 448030 run_train: loss = 5.4505  (0.142 sec)
18-06-05 01:55-INFO->> Step 448040 run_train: loss = 5.3735  (0.135 sec)
18-06-05 01:55-INFO->> Step 448050 run_train: loss = 5.3703  (0.182 sec)
18-06-05 01:55-INFO->> Step 448060 run_train: loss = 5.3626  (0.159 sec)
18-06-05 01:55-INFO->> Step 448070 run_train: loss = 5.3976  (0.157 sec)
18-06-05 01:55-INFO->> Step 448080 run_train: loss = 5.4029  (0.153 sec)
18-06-05 01:55-INFO->> Step 448090 run_train: loss = 5.4115  (0.135 sec)
18-06-05 01:55-INFO->> Step 448100 run_train: loss = 5.4604  (0.162 sec)
18-06-05 01:55-INFO->> Step 448110 run_train: loss = 5.4497  (0.145 sec)
18-06-05 01:55-INFO->> Step 448120 run_train: loss = 5.3585  (0.178 sec)
18-06-05 01:55-INFO->> Step 448130 run_train: loss = 5.4044  (0.149 sec)
18-06-05 01:55-INFO->> Step 448140 run_train: loss = 5.4550  (0.163 sec)
18-06-05 01:55-INFO->> Step 448150 run_train: loss = 5.4206  (0.152 sec)
18-06-05 01:55-INFO->> Step 448160 run_train: loss = 5.4308  (0.190 sec)
18-06-05 01:55-INFO->> Step 448170 run_train: loss = 5.3720  (0.224 sec)
18-06-05 01:55-INFO->> Step 448180 run_train: loss = 5.4458  (0.143 sec)
18-06-05 01:55-INFO->> Step 448190 run_train: loss = 5.4363  (0.130 sec)
18-06-05 01:55-INFO->> Step 448200 run_train: loss = 5.4438  (0.137 sec)
18-06-05 01:55-INFO->> Step 448210 run_train: loss = 5.4418  (0.166 sec)
18-06-05 01:55-INFO->> Step 448220 run_train: loss = 5.3736  (0.159 sec)
18-06-05 01:56-INFO->> Step 448230 run_train: loss = 5.4030  (0.162 sec)
18-06-05 01:56-INFO->> Step 448240 run_train: loss = 5.4179  (0.178 sec)
18-06-05 01:56-INFO->> Step 448250 run_train: loss = 5.4044  (0.243 sec)
18-06-05 01:56-INFO->> Step 448260 run_train: loss = 5.3922  (0.155 sec)
18-06-05 01:56-INFO->> Step 448270 run_train: loss = 5.3353  (0.166 sec)
18-06-05 01:56-INFO->> Step 448280 run_train: loss = 5.3914  (0.149 sec)
18-06-05 01:56-INFO->> Step 448290 run_train: loss = 5.3851  (0.173 sec)
18-06-05 01:56-INFO->> Step 448300 run_train: loss = 5.4067  (0.138 sec)
18-06-05 01:56-INFO->> Step 448310 run_train: loss = 5.3826  (0.180 sec)
18-06-05 01:56-INFO->> Step 448320 run_train: loss = 5.3657  (0.157 sec)
18-06-05 01:56-INFO->> Step 448330 run_train: loss = 5.3328  (0.183 sec)
18-06-05 01:56-INFO->> Step 448340 run_train: loss = 5.3601  (0.177 sec)
18-06-05 01:56-INFO->> Step 448350 run_train: loss = 5.3824  (0.195 sec)
18-06-05 01:56-INFO->> Step 448360 run_train: loss = 5.4023  (0.176 sec)
18-06-05 01:56-INFO->> Step 448370 run_train: loss = 5.3648  (0.175 sec)
18-06-05 01:56-INFO->> Step 448380 run_train: loss = 5.4195  (0.147 sec)
18-06-05 01:56-INFO->> Step 448390 run_train: loss = 5.4279  (0.124 sec)
18-06-05 01:56-INFO->> Step 448400 run_train: loss = 5.4287  (0.145 sec)
18-06-05 01:56-INFO->> Step 448410 run_train: loss = 5.4417  (0.184 sec)
18-06-05 01:56-INFO->> Step 448420 run_train: loss = 5.4125  (0.160 sec)
18-06-05 01:56-INFO->> Step 448430 run_train: loss = 5.4102  (0.165 sec)
18-06-05 01:56-INFO->> Step 448440 run_train: loss = 5.3963  (0.144 sec)
18-06-05 01:56-INFO->> Step 448450 run_train: loss = 5.4469  (0.177 sec)
18-06-05 01:56-INFO->> Step 448460 run_train: loss = 5.4530  (0.128 sec)
18-06-05 01:56-INFO->> Step 448470 run_train: loss = 5.4378  (0.163 sec)
18-06-05 01:56-INFO->> Step 448480 run_train: loss = 5.2858  (0.158 sec)
18-06-05 01:56-INFO->> Step 448490 run_train: loss = 5.4376  (0.159 sec)
18-06-05 01:56-INFO->> Step 448500 run_train: loss = 5.3704  (0.169 sec)
18-06-05 01:56-INFO->> Step 448510 run_train: loss = 5.4144  (0.135 sec)
18-06-05 01:56-INFO->> Step 448520 run_train: loss = 5.4634  (0.133 sec)
18-06-05 01:56-INFO->> Step 448530 run_train: loss = 5.4341  (0.176 sec)
18-06-05 01:56-INFO->> Step 448540 run_train: loss = 5.3996  (0.165 sec)
18-06-05 01:56-INFO->> Step 448550 run_train: loss = 5.3966  (0.144 sec)
18-06-05 01:56-INFO->> Step 448560 run_train: loss = 5.3687  (0.170 sec)
18-06-05 01:56-INFO->> Step 448570 run_train: loss = 5.4062  (0.137 sec)
18-06-05 01:56-INFO->> Step 448580 run_train: loss = 5.4221  (0.156 sec)
18-06-05 01:56-INFO->> Step 448590 run_train: loss = 5.3623  (0.197 sec)
18-06-05 01:57-INFO->> Step 448600 run_train: loss = 5.4133  (0.140 sec)
18-06-05 01:57-INFO->> Step 448610 run_train: loss = 5.3931  (0.157 sec)
18-06-05 01:57-INFO->> Step 448620 run_train: loss = 5.4284  (0.183 sec)
18-06-05 01:57-INFO->> Step 448630 run_train: loss = 5.4025  (0.181 sec)
18-06-05 01:57-INFO->> Step 448640 run_train: loss = 5.4406  (0.130 sec)
18-06-05 01:57-INFO->> Step 448650 run_train: loss = 5.3819  (0.173 sec)
18-06-05 01:57-INFO->> Step 448660 run_train: loss = 5.4076  (0.134 sec)
18-06-05 01:57-INFO->> Step 448670 run_train: loss = 5.4161  (0.176 sec)
18-06-05 01:57-INFO->> Step 448680 run_train: loss = 5.4198  (0.149 sec)
18-06-05 01:57-INFO->> Step 448690 run_train: loss = 5.4332  (0.151 sec)
18-06-05 01:57-INFO->> Step 448700 run_train: loss = 5.4171  (0.202 sec)
18-06-05 01:57-INFO->> Step 448710 run_train: loss = 5.4258  (0.159 sec)
18-06-05 01:57-INFO->> Step 448720 run_train: loss = 5.4321  (0.158 sec)
18-06-05 01:57-INFO->> Step 448730 run_train: loss = 5.3643  (0.181 sec)
18-06-05 01:57-INFO->> Step 448740 run_train: loss = 5.3423  (0.115 sec)
18-06-05 01:57-INFO->> Step 448750 run_train: loss = 5.3534  (0.157 sec)
18-06-05 01:57-INFO->> Step 448760 run_train: loss = 5.4189  (0.140 sec)
18-06-05 01:57-INFO->> Step 448770 run_train: loss = 5.3595  (0.170 sec)
18-06-05 01:57-INFO->> Step 448780 run_train: loss = 5.4086  (0.168 sec)
18-06-05 01:57-INFO->> Step 448790 run_train: loss = 5.4409  (0.178 sec)
18-06-05 01:57-INFO->> Step 448800 run_train: loss = 5.4199  (0.118 sec)
18-06-05 01:57-INFO->> Step 448810 run_train: loss = 5.3570  (0.122 sec)
18-06-05 01:57-INFO->> Step 448820 run_train: loss = 5.4116  (0.177 sec)
18-06-05 01:57-INFO->> Step 448830 run_train: loss = 5.4465  (0.135 sec)
18-06-05 01:57-INFO->> Step 448840 run_train: loss = 5.3415  (0.175 sec)
18-06-05 01:57-INFO->> Step 448850 run_train: loss = 5.4622  (0.148 sec)
18-06-05 01:57-INFO->> Step 448860 run_train: loss = 5.4344  (0.131 sec)
18-06-05 01:57-INFO->> Step 448870 run_train: loss = 5.3923  (0.108 sec)
18-06-05 01:57-INFO->> Step 448880 run_train: loss = 5.4672  (0.161 sec)
18-06-05 01:57-INFO->> Step 448890 run_train: loss = 5.3750  (0.145 sec)
18-06-05 01:57-INFO->> Step 448900 run_train: loss = 5.3456  (0.136 sec)
18-06-05 01:57-INFO->> Step 448910 run_train: loss = 5.4436  (0.163 sec)
18-06-05 01:57-INFO->> Step 448920 run_train: loss = 5.4395  (0.164 sec)
18-06-05 01:57-INFO->> Step 448930 run_train: loss = 5.4514  (0.163 sec)
18-06-05 01:57-INFO->> Step 448940 run_train: loss = 5.4242  (0.143 sec)
18-06-05 01:57-INFO->> Step 448950 run_train: loss = 5.4184  (0.144 sec)
18-06-05 01:57-INFO->> Step 448960 run_train: loss = 5.3622  (0.154 sec)
18-06-05 01:57-INFO->> Step 448970 run_train: loss = 5.4050  (0.220 sec)
18-06-05 01:58-INFO->> Step 448980 run_train: loss = 5.4560  (0.153 sec)
18-06-05 01:58-INFO->> Step 448990 run_train: loss = 5.3902  (0.159 sec)
18-06-05 01:58-INFO->> Step 449000 run_train: loss = 5.4572  (0.183 sec)
18-06-05 01:58-INFO->> 2018-06-05 01:58:04.457932 Saving in ckpt
18-06-05 01:58-INFO-Test Data Eval:
18-06-05 01:58-INFO-fpr95 = 0.17269693145589798 and auc = 0.9692960097875897
18-06-05 01:58-INFO->> Step 449010 run_train: loss = 5.3866  (0.165 sec)
18-06-05 01:58-INFO->> Step 449020 run_train: loss = 5.5146  (0.160 sec)
18-06-05 01:58-INFO->> Step 449030 run_train: loss = 5.3789  (0.169 sec)
18-06-05 01:58-INFO->> Step 449040 run_train: loss = 5.4249  (0.165 sec)
18-06-05 01:58-INFO->> Step 449050 run_train: loss = 5.4810  (0.173 sec)
18-06-05 01:58-INFO->> Step 449060 run_train: loss = 5.3909  (0.159 sec)
18-06-05 01:58-INFO->> Step 449070 run_train: loss = 5.4739  (0.164 sec)
18-06-05 01:58-INFO->> Step 449080 run_train: loss = 5.4042  (0.166 sec)
18-06-05 01:58-INFO->> Step 449090 run_train: loss = 5.3776  (0.183 sec)
18-06-05 01:59-INFO->> Step 449100 run_train: loss = 5.4319  (0.134 sec)
18-06-05 01:59-INFO->> Step 449110 run_train: loss = 5.3820  (0.207 sec)
18-06-05 01:59-INFO->> Step 449120 run_train: loss = 5.4507  (0.140 sec)
18-06-05 01:59-INFO->> Step 449130 run_train: loss = 5.4638  (0.188 sec)
18-06-05 01:59-INFO->> Step 449140 run_train: loss = 5.4665  (0.150 sec)
18-06-05 01:59-INFO->> Step 449150 run_train: loss = 5.3520  (0.148 sec)
18-06-05 01:59-INFO->> Step 449160 run_train: loss = 5.3872  (0.140 sec)
18-06-05 01:59-INFO->> Step 449170 run_train: loss = 5.3861  (0.146 sec)
18-06-05 01:59-INFO->> Step 449180 run_train: loss = 5.4546  (0.160 sec)
18-06-05 01:59-INFO->> Step 449190 run_train: loss = 5.4397  (0.144 sec)
18-06-05 01:59-INFO->> Step 449200 run_train: loss = 5.3798  (0.136 sec)
18-06-05 01:59-INFO->> Step 449210 run_train: loss = 5.3700  (0.155 sec)
18-06-05 01:59-INFO->> Step 449220 run_train: loss = 5.3621  (0.122 sec)
18-06-05 01:59-INFO->> Step 449230 run_train: loss = 5.4343  (0.158 sec)
18-06-05 01:59-INFO->> Step 449240 run_train: loss = 5.3721  (0.144 sec)
18-06-05 01:59-INFO->> Step 449250 run_train: loss = 5.4453  (0.151 sec)
18-06-05 01:59-INFO->> Step 449260 run_train: loss = 5.4608  (0.164 sec)
18-06-05 01:59-INFO->> Step 449270 run_train: loss = 5.4221  (0.195 sec)
18-06-05 01:59-INFO->> Step 449280 run_train: loss = 5.3956  (0.118 sec)
18-06-05 01:59-INFO->> Step 449290 run_train: loss = 5.4060  (0.153 sec)
18-06-05 01:59-INFO->> Step 449300 run_train: loss = 5.3818  (0.216 sec)
18-06-05 01:59-INFO->> Step 449310 run_train: loss = 5.3540  (0.153 sec)
18-06-05 01:59-INFO->> Step 449320 run_train: loss = 5.4072  (0.162 sec)
18-06-05 01:59-INFO->> Step 449330 run_train: loss = 5.3882  (0.162 sec)
18-06-05 01:59-INFO->> Step 449340 run_train: loss = 5.4224  (0.151 sec)
18-06-05 01:59-INFO->> Step 449350 run_train: loss = 5.4054  (0.161 sec)
18-06-05 01:59-INFO->> Step 449360 run_train: loss = 5.4755  (0.164 sec)
18-06-05 01:59-INFO->> Step 449370 run_train: loss = 5.4445  (0.172 sec)
18-06-05 01:59-INFO->> Step 449380 run_train: loss = 5.4477  (0.149 sec)
18-06-05 01:59-INFO->> Step 449390 run_train: loss = 5.4139  (0.161 sec)
18-06-05 01:59-INFO->> Step 449400 run_train: loss = 5.3947  (0.172 sec)
18-06-05 01:59-INFO->> Step 449410 run_train: loss = 5.5243  (0.162 sec)
18-06-05 01:59-INFO->> Step 449420 run_train: loss = 5.4792  (0.192 sec)
18-06-05 01:59-INFO->> Step 449430 run_train: loss = 5.4040  (0.153 sec)
18-06-05 01:59-INFO->> Step 449440 run_train: loss = 5.4463  (0.148 sec)
18-06-05 01:59-INFO->> Step 449450 run_train: loss = 5.4404  (0.159 sec)
18-06-05 01:59-INFO->> Step 449460 run_train: loss = 5.4451  (0.162 sec)
18-06-05 01:59-INFO->> Step 449470 run_train: loss = 5.4056  (0.195 sec)
18-06-05 02:00-INFO->> Step 449480 run_train: loss = 5.3994  (0.182 sec)
18-06-05 02:00-INFO->> Step 449490 run_train: loss = 5.4329  (0.161 sec)
18-06-05 02:00-INFO->> Step 449500 run_train: loss = 5.3414  (0.148 sec)
18-06-05 02:00-INFO->> Step 449510 run_train: loss = 5.3594  (0.164 sec)
18-06-05 02:00-INFO->> Step 449520 run_train: loss = 5.3515  (0.154 sec)
18-06-05 02:00-INFO->> Step 449530 run_train: loss = 5.3736  (0.144 sec)
18-06-05 02:00-INFO->> Step 449540 run_train: loss = 5.3716  (0.203 sec)
18-06-05 02:00-INFO->> Step 449550 run_train: loss = 5.4651  (0.154 sec)
18-06-05 02:00-INFO->> Step 449560 run_train: loss = 5.4229  (0.158 sec)
18-06-05 02:00-INFO->> Step 449570 run_train: loss = 5.4107  (0.152 sec)
18-06-05 02:00-INFO->> Step 449580 run_train: loss = 5.4372  (0.182 sec)
18-06-05 02:00-INFO->> Step 449590 run_train: loss = 5.3963  (0.175 sec)
18-06-05 02:00-INFO->> Step 449600 run_train: loss = 5.4117  (0.138 sec)
18-06-05 02:00-INFO->> Step 449610 run_train: loss = 5.2994  (0.122 sec)
18-06-05 02:00-INFO->> Step 449620 run_train: loss = 5.3917  (0.158 sec)
18-06-05 02:00-INFO->> Step 449630 run_train: loss = 5.4510  (0.167 sec)
18-06-05 02:00-INFO->> Step 449640 run_train: loss = 5.4439  (0.194 sec)
18-06-05 02:00-INFO->> Step 449650 run_train: loss = 5.4243  (0.153 sec)
18-06-05 02:00-INFO->> Step 449660 run_train: loss = 5.3972  (0.143 sec)
18-06-05 02:00-INFO->> Step 449670 run_train: loss = 5.4293  (0.133 sec)
18-06-05 02:00-INFO->> Step 449680 run_train: loss = 5.4002  (0.178 sec)
18-06-05 02:00-INFO->> Step 449690 run_train: loss = 5.3602  (0.167 sec)
18-06-05 02:00-INFO->> Step 449700 run_train: loss = 5.4573  (0.148 sec)
18-06-05 02:00-INFO->> Step 449710 run_train: loss = 5.4056  (0.166 sec)
18-06-05 02:00-INFO->> Step 449720 run_train: loss = 5.4245  (0.146 sec)
18-06-05 02:00-INFO->> Step 449730 run_train: loss = 5.3958  (0.171 sec)
18-06-05 02:00-INFO->> Step 449740 run_train: loss = 5.3654  (0.127 sec)
18-06-05 02:00-INFO->> Step 449750 run_train: loss = 5.3786  (0.182 sec)
18-06-05 02:00-INFO->> Step 449760 run_train: loss = 5.3842  (0.186 sec)
18-06-05 02:00-INFO->> Step 449770 run_train: loss = 5.4070  (0.137 sec)
18-06-05 02:00-INFO->> Step 449780 run_train: loss = 5.4155  (0.185 sec)
18-06-05 02:00-INFO->> Step 449790 run_train: loss = 5.3623  (0.173 sec)
18-06-05 02:00-INFO->> Step 449800 run_train: loss = 5.4088  (0.141 sec)
18-06-05 02:00-INFO->> Step 449810 run_train: loss = 5.3821  (0.140 sec)
18-06-05 02:00-INFO->> Step 449820 run_train: loss = 5.4062  (0.165 sec)
18-06-05 02:00-INFO->> Step 449830 run_train: loss = 5.4442  (0.156 sec)
18-06-05 02:00-INFO->> Step 449840 run_train: loss = 5.4024  (0.156 sec)
18-06-05 02:00-INFO->> Step 449850 run_train: loss = 5.3639  (0.169 sec)
18-06-05 02:01-INFO->> Step 449860 run_train: loss = 5.4083  (0.181 sec)
18-06-05 02:01-INFO->> Step 449870 run_train: loss = 5.4255  (0.188 sec)
18-06-05 02:01-INFO->> Step 449880 run_train: loss = 5.4619  (0.175 sec)
18-06-05 02:01-INFO->> Step 449890 run_train: loss = 5.4131  (0.148 sec)
18-06-05 02:01-INFO->> Step 449900 run_train: loss = 5.4435  (0.161 sec)
18-06-05 02:01-INFO->> Step 449910 run_train: loss = 5.3768  (0.167 sec)
18-06-05 02:01-INFO->> Step 449920 run_train: loss = 5.4168  (0.162 sec)
18-06-05 02:01-INFO->> Step 449930 run_train: loss = 5.4548  (0.125 sec)
18-06-05 02:01-INFO->> Step 449940 run_train: loss = 5.4749  (0.164 sec)
18-06-05 02:01-INFO->> Step 449950 run_train: loss = 5.4455  (0.175 sec)
18-06-05 02:01-INFO->> Step 449960 run_train: loss = 5.3901  (0.184 sec)
18-06-05 02:01-INFO->> Step 449970 run_train: loss = 5.3990  (0.177 sec)
18-06-05 02:01-INFO->> Step 449980 run_train: loss = 5.4400  (0.166 sec)
18-06-05 02:01-INFO->> Step 449990 run_train: loss = 5.4834  (0.176 sec)
18-06-05 02:01-INFO->> Step 450000 run_train: loss = 5.4137  (0.170 sec)
18-06-05 02:01-INFO->> 2018-06-05 02:01:23.036657 Saving in ckpt
18-06-05 02:01-INFO-Test Data Eval:
18-06-05 02:02-INFO-fpr95 = 0.17593484325185973 and auc = 0.9690980819008741
18-06-05 02:02-INFO->> Step 450010 run_train: loss = 5.4437  (0.123 sec)
18-06-05 02:02-INFO->> Step 450020 run_train: loss = 5.3834  (0.171 sec)
18-06-05 02:02-INFO->> Step 450030 run_train: loss = 5.4199  (0.161 sec)
18-06-05 02:02-INFO->> Step 450040 run_train: loss = 5.4634  (0.152 sec)
18-06-05 02:02-INFO->> Step 450050 run_train: loss = 5.3825  (0.128 sec)
18-06-05 02:02-INFO->> Step 450060 run_train: loss = 5.4352  (0.185 sec)
18-06-05 02:02-INFO->> Step 450070 run_train: loss = 5.3887  (0.129 sec)
18-06-05 02:02-INFO->> Step 450080 run_train: loss = 5.4487  (0.179 sec)
18-06-05 02:02-INFO->> Step 450090 run_train: loss = 5.3644  (0.166 sec)
18-06-05 02:02-INFO->> Step 450100 run_train: loss = 5.4363  (0.183 sec)
18-06-05 02:02-INFO->> Step 450110 run_train: loss = 5.4152  (0.149 sec)
18-06-05 02:02-INFO->> Step 450120 run_train: loss = 5.4511  (0.138 sec)
18-06-05 02:02-INFO->> Step 450130 run_train: loss = 5.4418  (0.151 sec)
18-06-05 02:02-INFO->> Step 450140 run_train: loss = 5.4215  (0.183 sec)
18-06-05 02:02-INFO->> Step 450150 run_train: loss = 5.4628  (0.158 sec)
18-06-05 02:02-INFO->> Step 450160 run_train: loss = 5.3995  (0.154 sec)
18-06-05 02:02-INFO->> Step 450170 run_train: loss = 5.4256  (0.143 sec)
18-06-05 02:02-INFO->> Step 450180 run_train: loss = 5.3416  (0.176 sec)
18-06-05 02:02-INFO->> Step 450190 run_train: loss = 5.3719  (0.136 sec)
18-06-05 02:02-INFO->> Step 450200 run_train: loss = 5.4574  (0.161 sec)
18-06-05 02:02-INFO->> Step 450210 run_train: loss = 5.4037  (0.135 sec)
18-06-05 02:02-INFO->> Step 450220 run_train: loss = 5.4278  (0.141 sec)
18-06-05 02:02-INFO->> Step 450230 run_train: loss = 5.3426  (0.162 sec)
18-06-05 02:02-INFO->> Step 450240 run_train: loss = 5.4262  (0.185 sec)
18-06-05 02:02-INFO->> Step 450250 run_train: loss = 5.3723  (0.153 sec)
18-06-05 02:02-INFO->> Step 450260 run_train: loss = 5.4307  (0.159 sec)
18-06-05 02:02-INFO->> Step 450270 run_train: loss = 5.4316  (0.129 sec)
18-06-05 02:02-INFO->> Step 450280 run_train: loss = 5.3648  (0.152 sec)
18-06-05 02:02-INFO->> Step 450290 run_train: loss = 5.4656  (0.196 sec)
18-06-05 02:02-INFO->> Step 450300 run_train: loss = 5.3912  (0.171 sec)
18-06-05 02:02-INFO->> Step 450310 run_train: loss = 5.4281  (0.162 sec)
18-06-05 02:02-INFO->> Step 450320 run_train: loss = 5.3915  (0.144 sec)
18-06-05 02:02-INFO->> Step 450330 run_train: loss = 5.4004  (0.128 sec)
18-06-05 02:02-INFO->> Step 450340 run_train: loss = 5.3957  (0.165 sec)
18-06-05 02:02-INFO->> Step 450350 run_train: loss = 5.4480  (0.107 sec)
18-06-05 02:03-INFO->> Step 450360 run_train: loss = 5.3560  (0.153 sec)
18-06-05 02:03-INFO->> Step 450370 run_train: loss = 5.3432  (0.165 sec)
18-06-05 02:03-INFO->> Step 450380 run_train: loss = 5.4234  (0.171 sec)
18-06-05 02:03-INFO->> Step 450390 run_train: loss = 5.4050  (0.143 sec)
18-06-05 02:03-INFO->> Step 450400 run_train: loss = 5.4125  (0.153 sec)
18-06-05 02:03-INFO->> Step 450410 run_train: loss = 5.4070  (0.155 sec)
18-06-05 02:03-INFO->> Step 450420 run_train: loss = 5.3992  (0.157 sec)
18-06-05 02:03-INFO->> Step 450430 run_train: loss = 5.4219  (0.147 sec)
18-06-05 02:03-INFO->> Step 450440 run_train: loss = 5.3549  (0.189 sec)
18-06-05 02:03-INFO->> Step 450450 run_train: loss = 5.4384  (0.188 sec)
18-06-05 02:03-INFO->> Step 450460 run_train: loss = 5.3593  (0.162 sec)
18-06-05 02:03-INFO->> Step 450470 run_train: loss = 5.4951  (0.168 sec)
18-06-05 02:03-INFO->> Step 450480 run_train: loss = 5.3852  (0.164 sec)
18-06-05 02:03-INFO->> Step 450490 run_train: loss = 5.4556  (0.127 sec)
18-06-05 02:03-INFO->> Step 450500 run_train: loss = 5.3860  (0.195 sec)
18-06-05 02:03-INFO->> Step 450510 run_train: loss = 5.4605  (0.154 sec)
18-06-05 02:03-INFO->> Step 450520 run_train: loss = 5.4772  (0.119 sec)
18-06-05 02:03-INFO->> Step 450530 run_train: loss = 5.4160  (0.148 sec)
18-06-05 02:03-INFO->> Step 450540 run_train: loss = 5.3658  (0.213 sec)
18-06-05 02:03-INFO->> Step 450550 run_train: loss = 5.4190  (0.148 sec)
18-06-05 02:03-INFO->> Step 450560 run_train: loss = 5.4717  (0.168 sec)
18-06-05 02:03-INFO->> Step 450570 run_train: loss = 5.3592  (0.156 sec)
18-06-05 02:03-INFO->> Step 450580 run_train: loss = 5.4063  (0.183 sec)
18-06-05 02:03-INFO->> Step 450590 run_train: loss = 5.3592  (0.153 sec)
18-06-05 02:03-INFO->> Step 450600 run_train: loss = 5.3479  (0.189 sec)
18-06-05 02:03-INFO->> Step 450610 run_train: loss = 5.3720  (0.160 sec)
18-06-05 02:03-INFO->> Step 450620 run_train: loss = 5.4354  (0.126 sec)
18-06-05 02:03-INFO->> Step 450630 run_train: loss = 5.4212  (0.173 sec)
18-06-05 02:03-INFO->> Step 450640 run_train: loss = 5.4585  (0.158 sec)
18-06-05 02:03-INFO->> Step 450650 run_train: loss = 5.4142  (0.190 sec)
18-06-05 02:03-INFO->> Step 450660 run_train: loss = 5.3838  (0.167 sec)
18-06-05 02:03-INFO->> Step 450670 run_train: loss = 5.4129  (0.182 sec)
18-06-05 02:03-INFO->> Step 450680 run_train: loss = 5.4284  (0.183 sec)
18-06-05 02:03-INFO->> Step 450690 run_train: loss = 5.4468  (0.153 sec)
18-06-05 02:03-INFO->> Step 450700 run_train: loss = 5.4389  (0.159 sec)
18-06-05 02:03-INFO->> Step 450710 run_train: loss = 5.3912  (0.150 sec)
18-06-05 02:03-INFO->> Step 450720 run_train: loss = 5.4282  (0.185 sec)
18-06-05 02:03-INFO->> Step 450730 run_train: loss = 5.3776  (0.166 sec)
18-06-05 02:04-INFO->> Step 450740 run_train: loss = 5.4259  (0.155 sec)
18-06-05 02:04-INFO->> Step 450750 run_train: loss = 5.4032  (0.195 sec)
18-06-05 02:04-INFO->> Step 450760 run_train: loss = 5.3576  (0.180 sec)
18-06-05 02:04-INFO->> Step 450770 run_train: loss = 5.3768  (0.136 sec)
18-06-05 02:04-INFO->> Step 450780 run_train: loss = 5.4296  (0.145 sec)
18-06-05 02:04-INFO->> Step 450790 run_train: loss = 5.4030  (0.153 sec)
18-06-05 02:04-INFO->> Step 450800 run_train: loss = 5.4278  (0.166 sec)
18-06-05 02:04-INFO->> Step 450810 run_train: loss = 5.4876  (0.206 sec)
18-06-05 02:04-INFO->> Step 450820 run_train: loss = 5.3599  (0.187 sec)
18-06-05 02:04-INFO->> Step 450830 run_train: loss = 5.3232  (0.158 sec)
18-06-05 02:04-INFO->> Step 450840 run_train: loss = 5.4255  (0.195 sec)
18-06-05 02:04-INFO->> Step 450850 run_train: loss = 5.3544  (0.156 sec)
18-06-05 02:04-INFO->> Step 450860 run_train: loss = 5.4497  (0.198 sec)
18-06-05 02:04-INFO->> Step 450870 run_train: loss = 5.4180  (0.148 sec)
18-06-05 02:04-INFO->> Step 450880 run_train: loss = 5.4401  (0.186 sec)
18-06-05 02:04-INFO->> Step 450890 run_train: loss = 5.3924  (0.155 sec)
18-06-05 02:04-INFO->> Step 450900 run_train: loss = 5.4436  (0.145 sec)
18-06-05 02:04-INFO->> Step 450910 run_train: loss = 5.4998  (0.145 sec)
18-06-05 02:04-INFO->> Step 450920 run_train: loss = 5.3943  (0.189 sec)
18-06-05 02:04-INFO->> Step 450930 run_train: loss = 5.4844  (0.176 sec)
18-06-05 02:04-INFO->> Step 450940 run_train: loss = 5.4395  (0.155 sec)
18-06-05 02:04-INFO->> Step 450950 run_train: loss = 5.4152  (0.150 sec)
18-06-05 02:04-INFO->> Step 450960 run_train: loss = 5.3698  (0.168 sec)
18-06-05 02:04-INFO->> Step 450970 run_train: loss = 5.4145  (0.178 sec)
18-06-05 02:04-INFO->> Step 450980 run_train: loss = 5.4283  (0.227 sec)
18-06-05 02:04-INFO->> Step 450990 run_train: loss = 5.4623  (0.151 sec)
18-06-05 02:04-INFO->> Step 451000 run_train: loss = 5.4082  (0.162 sec)
18-06-05 02:04-INFO->> 2018-06-05 02:04:42.736546 Saving in ckpt
18-06-05 02:04-INFO-Test Data Eval:
18-06-05 02:05-INFO-fpr95 = 0.17765342720510097 and auc = 0.9688597497003483
18-06-05 02:05-INFO->> Step 451010 run_train: loss = 5.4362  (0.163 sec)
18-06-05 02:05-INFO->> Step 451020 run_train: loss = 5.3847  (0.162 sec)
18-06-05 02:05-INFO->> Step 451030 run_train: loss = 5.4069  (0.177 sec)
18-06-05 02:05-INFO->> Step 451040 run_train: loss = 5.4073  (0.159 sec)
18-06-05 02:05-INFO->> Step 451050 run_train: loss = 5.3745  (0.178 sec)
18-06-05 02:05-INFO->> Step 451060 run_train: loss = 5.4057  (0.159 sec)
18-06-05 02:05-INFO->> Step 451070 run_train: loss = 5.4345  (0.183 sec)
18-06-05 02:05-INFO->> Step 451080 run_train: loss = 5.4446  (0.180 sec)
18-06-05 02:05-INFO->> Step 451090 run_train: loss = 5.4284  (0.139 sec)
18-06-05 02:05-INFO->> Step 451100 run_train: loss = 5.3917  (0.185 sec)
18-06-05 02:05-INFO->> Step 451110 run_train: loss = 5.4788  (0.133 sec)
18-06-05 02:05-INFO->> Step 451120 run_train: loss = 5.4039  (0.156 sec)
18-06-05 02:05-INFO->> Step 451130 run_train: loss = 5.4194  (0.156 sec)
18-06-05 02:05-INFO->> Step 451140 run_train: loss = 5.3768  (0.175 sec)
18-06-05 02:05-INFO->> Step 451150 run_train: loss = 5.3701  (0.137 sec)
18-06-05 02:05-INFO->> Step 451160 run_train: loss = 5.4092  (0.158 sec)
18-06-05 02:05-INFO->> Step 451170 run_train: loss = 5.4197  (0.158 sec)
18-06-05 02:05-INFO->> Step 451180 run_train: loss = 5.3845  (0.175 sec)
18-06-05 02:05-INFO->> Step 451190 run_train: loss = 5.3676  (0.139 sec)
18-06-05 02:05-INFO->> Step 451200 run_train: loss = 5.4780  (0.200 sec)
18-06-05 02:05-INFO->> Step 451210 run_train: loss = 5.4256  (0.180 sec)
18-06-05 02:05-INFO->> Step 451220 run_train: loss = 5.4672  (0.161 sec)
18-06-05 02:05-INFO->> Step 451230 run_train: loss = 5.4206  (0.143 sec)
18-06-05 02:06-INFO->> Step 451240 run_train: loss = 5.3421  (0.138 sec)
18-06-05 02:06-INFO->> Step 451250 run_train: loss = 5.4289  (0.171 sec)
18-06-05 02:06-INFO->> Step 451260 run_train: loss = 5.3971  (0.142 sec)
18-06-05 02:06-INFO->> Step 451270 run_train: loss = 5.4120  (0.159 sec)
18-06-05 02:06-INFO->> Step 451280 run_train: loss = 5.3675  (0.185 sec)
18-06-05 02:06-INFO->> Step 451290 run_train: loss = 5.4364  (0.194 sec)
18-06-05 02:06-INFO->> Step 451300 run_train: loss = 5.4028  (0.173 sec)
18-06-05 02:06-INFO->> Step 451310 run_train: loss = 5.4052  (0.099 sec)
18-06-05 02:06-INFO->> Step 451320 run_train: loss = 5.3523  (0.177 sec)
18-06-05 02:06-INFO->> Step 451330 run_train: loss = 5.3739  (0.148 sec)
18-06-05 02:06-INFO->> Step 451340 run_train: loss = 5.3770  (0.133 sec)
18-06-05 02:06-INFO->> Step 451350 run_train: loss = 5.3882  (0.172 sec)
18-06-05 02:06-INFO->> Step 451360 run_train: loss = 5.4296  (0.151 sec)
18-06-05 02:06-INFO->> Step 451370 run_train: loss = 5.3967  (0.134 sec)
18-06-05 02:06-INFO->> Step 451380 run_train: loss = 5.4559  (0.149 sec)
18-06-05 02:06-INFO->> Step 451390 run_train: loss = 5.3656  (0.145 sec)
18-06-05 02:06-INFO->> Step 451400 run_train: loss = 5.3859  (0.157 sec)
18-06-05 02:06-INFO->> Step 451410 run_train: loss = 5.3619  (0.163 sec)
18-06-05 02:06-INFO->> Step 451420 run_train: loss = 5.3917  (0.151 sec)
18-06-05 02:06-INFO->> Step 451430 run_train: loss = 5.3372  (0.137 sec)
18-06-05 02:06-INFO->> Step 451440 run_train: loss = 5.3847  (0.176 sec)
18-06-05 02:06-INFO->> Step 451450 run_train: loss = 5.3391  (0.160 sec)
18-06-05 02:06-INFO->> Step 451460 run_train: loss = 5.4196  (0.141 sec)
18-06-05 02:06-INFO->> Step 451470 run_train: loss = 5.3973  (0.154 sec)
18-06-05 02:06-INFO->> Step 451480 run_train: loss = 5.4503  (0.172 sec)
18-06-05 02:06-INFO->> Step 451490 run_train: loss = 5.4275  (0.138 sec)
18-06-05 02:06-INFO->> Step 451500 run_train: loss = 5.3993  (0.140 sec)
18-06-05 02:06-INFO->> Step 451510 run_train: loss = 5.4162  (0.160 sec)
18-06-05 02:06-INFO->> Step 451520 run_train: loss = 5.4286  (0.151 sec)
18-06-05 02:06-INFO->> Step 451530 run_train: loss = 5.4079  (0.156 sec)
18-06-05 02:06-INFO->> Step 451540 run_train: loss = 5.4199  (0.172 sec)
18-06-05 02:06-INFO->> Step 451550 run_train: loss = 5.3879  (0.215 sec)
18-06-05 02:06-INFO->> Step 451560 run_train: loss = 5.3845  (0.161 sec)
18-06-05 02:06-INFO->> Step 451570 run_train: loss = 5.3844  (0.161 sec)
18-06-05 02:06-INFO->> Step 451580 run_train: loss = 5.3638  (0.168 sec)
18-06-05 02:06-INFO->> Step 451590 run_train: loss = 5.4605  (0.177 sec)
18-06-05 02:06-INFO->> Step 451600 run_train: loss = 5.4256  (0.169 sec)
18-06-05 02:07-INFO->> Step 451610 run_train: loss = 5.3828  (0.132 sec)
18-06-05 02:07-INFO->> Step 451620 run_train: loss = 5.4471  (0.145 sec)
18-06-05 02:07-INFO->> Step 451630 run_train: loss = 5.4505  (0.182 sec)
18-06-05 02:07-INFO->> Step 451640 run_train: loss = 5.3377  (0.146 sec)
18-06-05 02:07-INFO->> Step 451650 run_train: loss = 5.3976  (0.179 sec)
18-06-05 02:07-INFO->> Step 451660 run_train: loss = 5.4190  (0.144 sec)
18-06-05 02:07-INFO->> Step 451670 run_train: loss = 5.4194  (0.142 sec)
18-06-05 02:07-INFO->> Step 451680 run_train: loss = 5.4381  (0.169 sec)
18-06-05 02:07-INFO->> Step 451690 run_train: loss = 5.4476  (0.144 sec)
18-06-05 02:07-INFO->> Step 451700 run_train: loss = 5.4221  (0.142 sec)
18-06-05 02:07-INFO->> Step 451710 run_train: loss = 5.3691  (0.172 sec)
18-06-05 02:07-INFO->> Step 451720 run_train: loss = 5.3174  (0.159 sec)
18-06-05 02:07-INFO->> Step 451730 run_train: loss = 5.4800  (0.151 sec)
18-06-05 02:07-INFO->> Step 451740 run_train: loss = 5.4262  (0.193 sec)
18-06-05 02:07-INFO->> Step 451750 run_train: loss = 5.4827  (0.160 sec)
18-06-05 02:07-INFO->> Step 451760 run_train: loss = 5.3406  (0.195 sec)
18-06-05 02:07-INFO->> Step 451770 run_train: loss = 5.3630  (0.198 sec)
18-06-05 02:07-INFO->> Step 451780 run_train: loss = 5.3385  (0.124 sec)
18-06-05 02:07-INFO->> Step 451790 run_train: loss = 5.4419  (0.180 sec)
18-06-05 02:07-INFO->> Step 451800 run_train: loss = 5.2931  (0.155 sec)
18-06-05 02:07-INFO->> Step 451810 run_train: loss = 5.3815  (0.156 sec)
18-06-05 02:07-INFO->> Step 451820 run_train: loss = 5.3357  (0.204 sec)
18-06-05 02:07-INFO->> Step 451830 run_train: loss = 5.4381  (0.149 sec)
18-06-05 02:07-INFO->> Step 451840 run_train: loss = 5.3650  (0.160 sec)
18-06-05 02:07-INFO->> Step 451850 run_train: loss = 5.3967  (0.155 sec)
18-06-05 02:07-INFO->> Step 451860 run_train: loss = 5.3918  (0.140 sec)
18-06-05 02:07-INFO->> Step 451870 run_train: loss = 5.4070  (0.118 sec)
18-06-05 02:07-INFO->> Step 451880 run_train: loss = 5.4201  (0.161 sec)
18-06-05 02:07-INFO->> Step 451890 run_train: loss = 5.3871  (0.162 sec)
18-06-05 02:07-INFO->> Step 451900 run_train: loss = 5.3073  (0.155 sec)
18-06-05 02:07-INFO->> Step 451910 run_train: loss = 5.3313  (0.183 sec)
18-06-05 02:07-INFO->> Step 451920 run_train: loss = 5.3939  (0.199 sec)
18-06-05 02:07-INFO->> Step 451930 run_train: loss = 5.3531  (0.142 sec)
18-06-05 02:07-INFO->> Step 451940 run_train: loss = 5.4078  (0.122 sec)
18-06-05 02:07-INFO->> Step 451950 run_train: loss = 5.4010  (0.154 sec)
18-06-05 02:07-INFO->> Step 451960 run_train: loss = 5.4666  (0.151 sec)
18-06-05 02:07-INFO->> Step 451970 run_train: loss = 5.3751  (0.155 sec)
18-06-05 02:07-INFO->> Step 451980 run_train: loss = 5.4082  (0.115 sec)
18-06-05 02:08-INFO->> Step 451990 run_train: loss = 5.4395  (0.119 sec)
18-06-05 02:08-INFO->> Step 452000 run_train: loss = 5.3767  (0.211 sec)
18-06-05 02:08-INFO->> 2018-06-05 02:08:02.155246 Saving in ckpt
18-06-05 02:08-INFO-Test Data Eval:
18-06-05 02:08-INFO-fpr95 = 0.17272183846971306 and auc = 0.9690391305661022
18-06-05 02:08-INFO->> Step 452010 run_train: loss = 5.4872  (0.145 sec)
18-06-05 02:08-INFO->> Step 452020 run_train: loss = 5.3702  (0.160 sec)
18-06-05 02:08-INFO->> Step 452030 run_train: loss = 5.4180  (0.152 sec)
18-06-05 02:08-INFO->> Step 452040 run_train: loss = 5.3991  (0.136 sec)
18-06-05 02:08-INFO->> Step 452050 run_train: loss = 5.4181  (0.120 sec)
18-06-05 02:08-INFO->> Step 452060 run_train: loss = 5.3871  (0.178 sec)
18-06-05 02:08-INFO->> Step 452070 run_train: loss = 5.3726  (0.146 sec)
18-06-05 02:08-INFO->> Step 452080 run_train: loss = 5.4262  (0.171 sec)
18-06-05 02:08-INFO->> Step 452090 run_train: loss = 5.3646  (0.140 sec)
18-06-05 02:08-INFO->> Step 452100 run_train: loss = 5.4438  (0.156 sec)
18-06-05 02:08-INFO->> Step 452110 run_train: loss = 5.3949  (0.163 sec)
18-06-05 02:09-INFO->> Step 452120 run_train: loss = 5.3740  (0.150 sec)
18-06-05 02:09-INFO->> Step 452130 run_train: loss = 5.3754  (0.152 sec)
18-06-05 02:09-INFO->> Step 452140 run_train: loss = 5.4426  (0.184 sec)
18-06-05 02:09-INFO->> Step 452150 run_train: loss = 5.4437  (0.155 sec)
18-06-05 02:09-INFO->> Step 452160 run_train: loss = 5.4187  (0.138 sec)
18-06-05 02:09-INFO->> Step 452170 run_train: loss = 5.4600  (0.158 sec)
18-06-05 02:09-INFO->> Step 452180 run_train: loss = 5.4171  (0.152 sec)
18-06-05 02:09-INFO->> Step 452190 run_train: loss = 5.4635  (0.175 sec)
18-06-05 02:09-INFO->> Step 452200 run_train: loss = 5.4141  (0.145 sec)
18-06-05 02:09-INFO->> Step 452210 run_train: loss = 5.3853  (0.152 sec)
18-06-05 02:09-INFO->> Step 452220 run_train: loss = 5.4193  (0.168 sec)
18-06-05 02:09-INFO->> Step 452230 run_train: loss = 5.3720  (0.184 sec)
18-06-05 02:09-INFO->> Step 452240 run_train: loss = 5.4883  (0.147 sec)
18-06-05 02:09-INFO->> Step 452250 run_train: loss = 5.4232  (0.158 sec)
18-06-05 02:09-INFO->> Step 452260 run_train: loss = 5.3797  (0.157 sec)
18-06-05 02:09-INFO->> Step 452270 run_train: loss = 5.3643  (0.153 sec)
18-06-05 02:09-INFO->> Step 452280 run_train: loss = 5.4089  (0.142 sec)
18-06-05 02:09-INFO->> Step 452290 run_train: loss = 5.3689  (0.155 sec)
18-06-05 02:09-INFO->> Step 452300 run_train: loss = 5.4717  (0.165 sec)
18-06-05 02:09-INFO->> Step 452310 run_train: loss = 5.3537  (0.140 sec)
18-06-05 02:09-INFO->> Step 452320 run_train: loss = 5.4940  (0.135 sec)
18-06-05 02:09-INFO->> Step 452330 run_train: loss = 5.5007  (0.153 sec)
18-06-05 02:09-INFO->> Step 452340 run_train: loss = 5.4246  (0.149 sec)
18-06-05 02:09-INFO->> Step 452350 run_train: loss = 5.4385  (0.163 sec)
18-06-05 02:09-INFO->> Step 452360 run_train: loss = 5.3980  (0.187 sec)
18-06-05 02:09-INFO->> Step 452370 run_train: loss = 5.4214  (0.157 sec)
18-06-05 02:09-INFO->> Step 452380 run_train: loss = 5.4010  (0.148 sec)
18-06-05 02:09-INFO->> Step 452390 run_train: loss = 5.4150  (0.169 sec)
18-06-05 02:09-INFO->> Step 452400 run_train: loss = 5.4174  (0.139 sec)
18-06-05 02:09-INFO->> Step 452410 run_train: loss = 5.4375  (0.164 sec)
18-06-05 02:09-INFO->> Step 452420 run_train: loss = 5.4119  (0.168 sec)
18-06-05 02:09-INFO->> Step 452430 run_train: loss = 5.4924  (0.161 sec)
18-06-05 02:09-INFO->> Step 452440 run_train: loss = 5.3842  (0.162 sec)
18-06-05 02:09-INFO->> Step 452450 run_train: loss = 5.3832  (0.188 sec)
18-06-05 02:09-INFO->> Step 452460 run_train: loss = 5.3105  (0.170 sec)
18-06-05 02:09-INFO->> Step 452470 run_train: loss = 5.4023  (0.173 sec)
18-06-05 02:09-INFO->> Step 452480 run_train: loss = 5.4179  (0.126 sec)
18-06-05 02:10-INFO->> Step 452490 run_train: loss = 5.3925  (0.155 sec)
18-06-05 02:10-INFO->> Step 452500 run_train: loss = 5.3555  (0.110 sec)
18-06-05 02:10-INFO->> Step 452510 run_train: loss = 5.4182  (0.140 sec)
18-06-05 02:10-INFO->> Step 452520 run_train: loss = 5.3974  (0.169 sec)
18-06-05 02:10-INFO->> Step 452530 run_train: loss = 5.4391  (0.130 sec)
18-06-05 02:10-INFO->> Step 452540 run_train: loss = 5.4430  (0.139 sec)
18-06-05 02:10-INFO->> Step 452550 run_train: loss = 5.3754  (0.149 sec)
18-06-05 02:10-INFO->> Step 452560 run_train: loss = 5.4226  (0.165 sec)
18-06-05 02:10-INFO->> Step 452570 run_train: loss = 5.4511  (0.184 sec)
18-06-05 02:10-INFO->> Step 452580 run_train: loss = 5.4589  (0.139 sec)
18-06-05 02:10-INFO->> Step 452590 run_train: loss = 5.4402  (0.130 sec)
18-06-05 02:10-INFO->> Step 452600 run_train: loss = 5.4866  (0.158 sec)
18-06-05 02:10-INFO->> Step 452610 run_train: loss = 5.4353  (0.161 sec)
18-06-05 02:10-INFO->> Step 452620 run_train: loss = 5.3898  (0.157 sec)
18-06-05 02:10-INFO->> Step 452630 run_train: loss = 5.3917  (0.166 sec)
18-06-05 02:10-INFO->> Step 452640 run_train: loss = 5.3661  (0.158 sec)
18-06-05 02:10-INFO->> Step 452650 run_train: loss = 5.4240  (0.172 sec)
18-06-05 02:10-INFO->> Step 452660 run_train: loss = 5.3557  (0.152 sec)
18-06-05 02:10-INFO->> Step 452670 run_train: loss = 5.3859  (0.193 sec)
18-06-05 02:10-INFO->> Step 452680 run_train: loss = 5.3586  (0.157 sec)
18-06-05 02:10-INFO->> Step 452690 run_train: loss = 5.3772  (0.177 sec)
18-06-05 02:10-INFO->> Step 452700 run_train: loss = 5.4622  (0.160 sec)
18-06-05 02:10-INFO->> Step 452710 run_train: loss = 5.3131  (0.167 sec)
18-06-05 02:10-INFO->> Step 452720 run_train: loss = 5.3893  (0.174 sec)
18-06-05 02:10-INFO->> Step 452730 run_train: loss = 5.4020  (0.163 sec)
18-06-05 02:10-INFO->> Step 452740 run_train: loss = 5.4418  (0.183 sec)
18-06-05 02:10-INFO->> Step 452750 run_train: loss = 5.3886  (0.168 sec)
18-06-05 02:10-INFO->> Step 452760 run_train: loss = 5.4140  (0.169 sec)
18-06-05 02:10-INFO->> Step 452770 run_train: loss = 5.4284  (0.161 sec)
18-06-05 02:10-INFO->> Step 452780 run_train: loss = 5.4095  (0.136 sec)
18-06-05 02:10-INFO->> Step 452790 run_train: loss = 5.3562  (0.151 sec)
18-06-05 02:10-INFO->> Step 452800 run_train: loss = 5.4894  (0.153 sec)
18-06-05 02:10-INFO->> Step 452810 run_train: loss = 5.4143  (0.180 sec)
18-06-05 02:10-INFO->> Step 452820 run_train: loss = 5.5291  (0.158 sec)
18-06-05 02:10-INFO->> Step 452830 run_train: loss = 5.4654  (0.194 sec)
18-06-05 02:10-INFO->> Step 452840 run_train: loss = 5.3954  (0.139 sec)
18-06-05 02:10-INFO->> Step 452850 run_train: loss = 5.4240  (0.170 sec)
18-06-05 02:10-INFO->> Step 452860 run_train: loss = 5.4435  (0.163 sec)
18-06-05 02:11-INFO->> Step 452870 run_train: loss = 5.4193  (0.140 sec)
18-06-05 02:11-INFO->> Step 452880 run_train: loss = 5.3983  (0.163 sec)
18-06-05 02:11-INFO->> Step 452890 run_train: loss = 5.3873  (0.104 sec)
18-06-05 02:11-INFO->> Step 452900 run_train: loss = 5.4155  (0.135 sec)
18-06-05 02:11-INFO->> Step 452910 run_train: loss = 5.4176  (0.176 sec)
18-06-05 02:11-INFO->> Step 452920 run_train: loss = 5.3851  (0.175 sec)
18-06-05 02:11-INFO->> Step 452930 run_train: loss = 5.4326  (0.162 sec)
18-06-05 02:11-INFO->> Step 452940 run_train: loss = 5.4226  (0.141 sec)
18-06-05 02:11-INFO->> Step 452950 run_train: loss = 5.4090  (0.161 sec)
18-06-05 02:11-INFO->> Step 452960 run_train: loss = 5.3993  (0.163 sec)
18-06-05 02:11-INFO->> Step 452970 run_train: loss = 5.4358  (0.176 sec)
18-06-05 02:11-INFO->> Step 452980 run_train: loss = 5.4080  (0.168 sec)
18-06-05 02:11-INFO->> Step 452990 run_train: loss = 5.3903  (0.158 sec)
18-06-05 02:11-INFO->> Step 453000 run_train: loss = 5.3445  (0.176 sec)
18-06-05 02:11-INFO->> 2018-06-05 02:11:21.636891 Saving in ckpt
18-06-05 02:11-INFO-Test Data Eval:
18-06-05 02:12-INFO-fpr95 = 0.17440721307120086 and auc = 0.9690670939869784
18-06-05 02:12-INFO->> Step 453010 run_train: loss = 5.3584  (0.167 sec)
18-06-05 02:12-INFO->> Step 453020 run_train: loss = 5.3518  (0.164 sec)
18-06-05 02:12-INFO->> Step 453030 run_train: loss = 5.4033  (0.160 sec)
18-06-05 02:12-INFO->> Step 453040 run_train: loss = 5.3793  (0.151 sec)
18-06-05 02:12-INFO->> Step 453050 run_train: loss = 5.4138  (0.157 sec)
18-06-05 02:12-INFO->> Step 453060 run_train: loss = 5.4421  (0.158 sec)
18-06-05 02:12-INFO->> Step 453070 run_train: loss = 5.3992  (0.187 sec)
18-06-05 02:12-INFO->> Step 453080 run_train: loss = 5.4317  (0.185 sec)
18-06-05 02:12-INFO->> Step 453090 run_train: loss = 5.3744  (0.149 sec)
18-06-05 02:12-INFO->> Step 453100 run_train: loss = 5.4846  (0.159 sec)
18-06-05 02:12-INFO->> Step 453110 run_train: loss = 5.3897  (0.167 sec)
18-06-05 02:12-INFO->> Step 453120 run_train: loss = 5.3999  (0.152 sec)
18-06-05 02:12-INFO->> Step 453130 run_train: loss = 5.3799  (0.176 sec)
18-06-05 02:12-INFO->> Step 453140 run_train: loss = 5.4224  (0.161 sec)
18-06-05 02:12-INFO->> Step 453150 run_train: loss = 5.4075  (0.159 sec)
18-06-05 02:12-INFO->> Step 453160 run_train: loss = 5.3993  (0.156 sec)
18-06-05 02:12-INFO->> Step 453170 run_train: loss = 5.4280  (0.174 sec)
18-06-05 02:12-INFO->> Step 453180 run_train: loss = 5.4354  (0.157 sec)
18-06-05 02:12-INFO->> Step 453190 run_train: loss = 5.4177  (0.154 sec)
18-06-05 02:12-INFO->> Step 453200 run_train: loss = 5.4322  (0.143 sec)
18-06-05 02:12-INFO->> Step 453210 run_train: loss = 5.3880  (0.222 sec)
18-06-05 02:12-INFO->> Step 453220 run_train: loss = 5.3914  (0.179 sec)
18-06-05 02:12-INFO->> Step 453230 run_train: loss = 5.4692  (0.171 sec)
18-06-05 02:12-INFO->> Step 453240 run_train: loss = 5.4542  (0.147 sec)
18-06-05 02:12-INFO->> Step 453250 run_train: loss = 5.4335  (0.158 sec)
18-06-05 02:12-INFO->> Step 453260 run_train: loss = 5.4152  (0.211 sec)
18-06-05 02:12-INFO->> Step 453270 run_train: loss = 5.3724  (0.150 sec)
18-06-05 02:12-INFO->> Step 453280 run_train: loss = 5.3818  (0.199 sec)
18-06-05 02:12-INFO->> Step 453290 run_train: loss = 5.3692  (0.173 sec)
18-06-05 02:12-INFO->> Step 453300 run_train: loss = 5.3258  (0.156 sec)
18-06-05 02:12-INFO->> Step 453310 run_train: loss = 5.3990  (0.126 sec)
18-06-05 02:12-INFO->> Step 453320 run_train: loss = 5.3915  (0.171 sec)
18-06-05 02:12-INFO->> Step 453330 run_train: loss = 5.4306  (0.161 sec)
18-06-05 02:12-INFO->> Step 453340 run_train: loss = 5.4183  (0.173 sec)
18-06-05 02:12-INFO->> Step 453350 run_train: loss = 5.4290  (0.144 sec)
18-06-05 02:12-INFO->> Step 453360 run_train: loss = 5.4269  (0.169 sec)
18-06-05 02:13-INFO->> Step 453370 run_train: loss = 5.4064  (0.144 sec)
18-06-05 02:13-INFO->> Step 453380 run_train: loss = 5.3727  (0.126 sec)
18-06-05 02:13-INFO->> Step 453390 run_train: loss = 5.4017  (0.164 sec)
18-06-05 02:13-INFO->> Step 453400 run_train: loss = 5.3648  (0.145 sec)
18-06-05 02:13-INFO->> Step 453410 run_train: loss = 5.4013  (0.112 sec)
18-06-05 02:13-INFO->> Step 453420 run_train: loss = 5.4524  (0.156 sec)
18-06-05 02:13-INFO->> Step 453430 run_train: loss = 5.3247  (0.120 sec)
18-06-05 02:13-INFO->> Step 453440 run_train: loss = 5.4021  (0.165 sec)
18-06-05 02:13-INFO->> Step 453450 run_train: loss = 5.3241  (0.161 sec)
18-06-05 02:13-INFO->> Step 453460 run_train: loss = 5.4301  (0.198 sec)
18-06-05 02:13-INFO->> Step 453470 run_train: loss = 5.4317  (0.158 sec)
18-06-05 02:13-INFO->> Step 453480 run_train: loss = 5.4002  (0.147 sec)
18-06-05 02:13-INFO->> Step 453490 run_train: loss = 5.3613  (0.182 sec)
18-06-05 02:13-INFO->> Step 453500 run_train: loss = 5.4204  (0.158 sec)
18-06-05 02:13-INFO->> Step 453510 run_train: loss = 5.4104  (0.197 sec)
18-06-05 02:13-INFO->> Step 453520 run_train: loss = 5.3438  (0.150 sec)
18-06-05 02:13-INFO->> Step 453530 run_train: loss = 5.3279  (0.153 sec)
18-06-05 02:13-INFO->> Step 453540 run_train: loss = 5.3722  (0.174 sec)
18-06-05 02:13-INFO->> Step 453550 run_train: loss = 5.3390  (0.136 sec)
18-06-05 02:13-INFO->> Step 453560 run_train: loss = 5.3571  (0.153 sec)
18-06-05 02:13-INFO->> Step 453570 run_train: loss = 5.3684  (0.163 sec)
18-06-05 02:13-INFO->> Step 453580 run_train: loss = 5.4594  (0.160 sec)
18-06-05 02:13-INFO->> Step 453590 run_train: loss = 5.4681  (0.183 sec)
18-06-05 02:13-INFO->> Step 453600 run_train: loss = 5.4471  (0.153 sec)
18-06-05 02:13-INFO->> Step 453610 run_train: loss = 5.4134  (0.149 sec)
18-06-05 02:13-INFO->> Step 453620 run_train: loss = 5.4054  (0.154 sec)
18-06-05 02:13-INFO->> Step 453630 run_train: loss = 5.4436  (0.176 sec)
18-06-05 02:13-INFO->> Step 453640 run_train: loss = 5.4184  (0.177 sec)
18-06-05 02:13-INFO->> Step 453650 run_train: loss = 5.3728  (0.150 sec)
18-06-05 02:13-INFO->> Step 453660 run_train: loss = 5.4564  (0.158 sec)
18-06-05 02:13-INFO->> Step 453670 run_train: loss = 5.4492  (0.145 sec)
18-06-05 02:13-INFO->> Step 453680 run_train: loss = 5.4422  (0.152 sec)
18-06-05 02:13-INFO->> Step 453690 run_train: loss = 5.3787  (0.160 sec)
18-06-05 02:13-INFO->> Step 453700 run_train: loss = 5.4377  (0.154 sec)
18-06-05 02:13-INFO->> Step 453710 run_train: loss = 5.3914  (0.152 sec)
18-06-05 02:13-INFO->> Step 453720 run_train: loss = 5.4550  (0.124 sec)
18-06-05 02:13-INFO->> Step 453730 run_train: loss = 5.4326  (0.129 sec)
18-06-05 02:13-INFO->> Step 453740 run_train: loss = 5.3696  (0.185 sec)
18-06-05 02:14-INFO->> Step 453750 run_train: loss = 5.4037  (0.183 sec)
18-06-05 02:14-INFO->> Step 453760 run_train: loss = 5.4057  (0.150 sec)
18-06-05 02:14-INFO->> Step 453770 run_train: loss = 5.4122  (0.158 sec)
18-06-05 02:14-INFO->> Step 453780 run_train: loss = 5.4466  (0.156 sec)
18-06-05 02:14-INFO->> Step 453790 run_train: loss = 5.4939  (0.162 sec)
18-06-05 02:14-INFO->> Step 453800 run_train: loss = 5.4705  (0.160 sec)
18-06-05 02:14-INFO->> Step 453810 run_train: loss = 5.3823  (0.143 sec)
18-06-05 02:14-INFO->> Step 453820 run_train: loss = 5.4455  (0.143 sec)
18-06-05 02:14-INFO->> Step 453830 run_train: loss = 5.3540  (0.151 sec)
18-06-05 02:14-INFO->> Step 453840 run_train: loss = 5.4899  (0.184 sec)
18-06-05 02:14-INFO->> Step 453850 run_train: loss = 5.3800  (0.155 sec)
18-06-05 02:14-INFO->> Step 453860 run_train: loss = 5.3780  (0.159 sec)
18-06-05 02:14-INFO->> Step 453870 run_train: loss = 5.4126  (0.191 sec)
18-06-05 02:14-INFO->> Step 453880 run_train: loss = 5.3905  (0.131 sec)
18-06-05 02:14-INFO->> Step 453890 run_train: loss = 5.4195  (0.147 sec)
18-06-05 02:14-INFO->> Step 453900 run_train: loss = 5.3461  (0.148 sec)
18-06-05 02:14-INFO->> Step 453910 run_train: loss = 5.4347  (0.158 sec)
18-06-05 02:14-INFO->> Step 453920 run_train: loss = 5.3900  (0.162 sec)
18-06-05 02:14-INFO->> Step 453930 run_train: loss = 5.3997  (0.186 sec)
18-06-05 02:14-INFO->> Step 453940 run_train: loss = 5.2927  (0.183 sec)
18-06-05 02:14-INFO->> Step 453950 run_train: loss = 5.3831  (0.161 sec)
18-06-05 02:14-INFO->> Step 453960 run_train: loss = 5.4730  (0.198 sec)
18-06-05 02:14-INFO->> Step 453970 run_train: loss = 5.3664  (0.123 sec)
18-06-05 02:14-INFO->> Step 453980 run_train: loss = 5.4100  (0.117 sec)
18-06-05 02:14-INFO->> Step 453990 run_train: loss = 5.4523  (0.155 sec)
18-06-05 02:14-INFO->> Step 454000 run_train: loss = 5.4071  (0.140 sec)
18-06-05 02:14-INFO->> 2018-06-05 02:14:40.343539 Saving in ckpt
18-06-05 02:14-INFO-Test Data Eval:
18-06-05 02:15-INFO-fpr95 = 0.17317846705632306 and auc = 0.9691114630517772
18-06-05 02:15-INFO->> Step 454010 run_train: loss = 5.3808  (0.166 sec)
18-06-05 02:15-INFO->> Step 454020 run_train: loss = 5.4300  (0.200 sec)
18-06-05 02:15-INFO->> Step 454030 run_train: loss = 5.3629  (0.169 sec)
18-06-05 02:15-INFO->> Step 454040 run_train: loss = 5.4397  (0.159 sec)
18-06-05 02:15-INFO->> Step 454050 run_train: loss = 5.4241  (0.152 sec)
18-06-05 02:15-INFO->> Step 454060 run_train: loss = 5.3579  (0.152 sec)
18-06-05 02:15-INFO->> Step 454070 run_train: loss = 5.3513  (0.140 sec)
18-06-05 02:15-INFO->> Step 454080 run_train: loss = 5.3767  (0.183 sec)
18-06-05 02:15-INFO->> Step 454090 run_train: loss = 5.3666  (0.168 sec)
18-06-05 02:15-INFO->> Step 454100 run_train: loss = 5.4137  (0.185 sec)
18-06-05 02:15-INFO->> Step 454110 run_train: loss = 5.3904  (0.134 sec)
18-06-05 02:15-INFO->> Step 454120 run_train: loss = 5.4582  (0.162 sec)
18-06-05 02:15-INFO->> Step 454130 run_train: loss = 5.3621  (0.177 sec)
18-06-05 02:15-INFO->> Step 454140 run_train: loss = 5.4286  (0.162 sec)
18-06-05 02:15-INFO->> Step 454150 run_train: loss = 5.3121  (0.185 sec)
18-06-05 02:15-INFO->> Step 454160 run_train: loss = 5.3923  (0.159 sec)
18-06-05 02:15-INFO->> Step 454170 run_train: loss = 5.4429  (0.149 sec)
18-06-05 02:15-INFO->> Step 454180 run_train: loss = 5.4256  (0.183 sec)
18-06-05 02:15-INFO->> Step 454190 run_train: loss = 5.3962  (0.162 sec)
18-06-05 02:15-INFO->> Step 454200 run_train: loss = 5.3911  (0.204 sec)
18-06-05 02:15-INFO->> Step 454210 run_train: loss = 5.4281  (0.138 sec)
18-06-05 02:15-INFO->> Step 454220 run_train: loss = 5.3612  (0.153 sec)
18-06-05 02:15-INFO->> Step 454230 run_train: loss = 5.4105  (0.161 sec)
18-06-05 02:15-INFO->> Step 454240 run_train: loss = 5.3653  (0.155 sec)
18-06-05 02:16-INFO->> Step 454250 run_train: loss = 5.4445  (0.195 sec)
18-06-05 02:16-INFO->> Step 454260 run_train: loss = 5.4515  (0.161 sec)
18-06-05 02:16-INFO->> Step 454270 run_train: loss = 5.4425  (0.154 sec)
18-06-05 02:16-INFO->> Step 454280 run_train: loss = 5.3968  (0.158 sec)
18-06-05 02:16-INFO->> Step 454290 run_train: loss = 5.4096  (0.149 sec)
18-06-05 02:16-INFO->> Step 454300 run_train: loss = 5.4602  (0.182 sec)
18-06-05 02:16-INFO->> Step 454310 run_train: loss = 5.4191  (0.183 sec)
18-06-05 02:16-INFO->> Step 454320 run_train: loss = 5.4726  (0.145 sec)
18-06-05 02:16-INFO->> Step 454330 run_train: loss = 5.4203  (0.194 sec)
18-06-05 02:16-INFO->> Step 454340 run_train: loss = 5.4477  (0.144 sec)
18-06-05 02:16-INFO->> Step 454350 run_train: loss = 5.4551  (0.167 sec)
18-06-05 02:16-INFO->> Step 454360 run_train: loss = 5.4332  (0.202 sec)
18-06-05 02:16-INFO->> Step 454370 run_train: loss = 5.4422  (0.178 sec)
18-06-05 02:16-INFO->> Step 454380 run_train: loss = 5.4578  (0.182 sec)
18-06-05 02:16-INFO->> Step 454390 run_train: loss = 5.3744  (0.136 sec)
18-06-05 02:16-INFO->> Step 454400 run_train: loss = 5.4446  (0.165 sec)
18-06-05 02:16-INFO->> Step 454410 run_train: loss = 5.3672  (0.164 sec)
18-06-05 02:16-INFO->> Step 454420 run_train: loss = 5.4350  (0.165 sec)
18-06-05 02:16-INFO->> Step 454430 run_train: loss = 5.4393  (0.196 sec)
18-06-05 02:16-INFO->> Step 454440 run_train: loss = 5.4188  (0.188 sec)
18-06-05 02:16-INFO->> Step 454450 run_train: loss = 5.3926  (0.143 sec)
18-06-05 02:16-INFO->> Step 454460 run_train: loss = 5.4505  (0.171 sec)
18-06-05 02:16-INFO->> Step 454470 run_train: loss = 5.4343  (0.160 sec)
18-06-05 02:16-INFO->> Step 454480 run_train: loss = 5.3745  (0.155 sec)
18-06-05 02:16-INFO->> Step 454490 run_train: loss = 5.4787  (0.148 sec)
18-06-05 02:16-INFO->> Step 454500 run_train: loss = 5.4241  (0.168 sec)
18-06-05 02:16-INFO->> Step 454510 run_train: loss = 5.4357  (0.144 sec)
18-06-05 02:16-INFO->> Step 454520 run_train: loss = 5.4099  (0.164 sec)
18-06-05 02:16-INFO->> Step 454530 run_train: loss = 5.4523  (0.161 sec)
18-06-05 02:16-INFO->> Step 454540 run_train: loss = 5.3646  (0.112 sec)
18-06-05 02:16-INFO->> Step 454550 run_train: loss = 5.4205  (0.157 sec)
18-06-05 02:16-INFO->> Step 454560 run_train: loss = 5.3838  (0.141 sec)
18-06-05 02:16-INFO->> Step 454570 run_train: loss = 5.3849  (0.169 sec)
18-06-05 02:16-INFO->> Step 454580 run_train: loss = 5.4551  (0.155 sec)
18-06-05 02:16-INFO->> Step 454590 run_train: loss = 5.4109  (0.147 sec)
18-06-05 02:16-INFO->> Step 454600 run_train: loss = 5.4103  (0.137 sec)
18-06-05 02:16-INFO->> Step 454610 run_train: loss = 5.4167  (0.145 sec)
18-06-05 02:16-INFO->> Step 454620 run_train: loss = 5.3574  (0.174 sec)
18-06-05 02:17-INFO->> Step 454630 run_train: loss = 5.4243  (0.151 sec)
18-06-05 02:17-INFO->> Step 454640 run_train: loss = 5.4639  (0.176 sec)
18-06-05 02:17-INFO->> Step 454650 run_train: loss = 5.4044  (0.154 sec)
18-06-05 02:17-INFO->> Step 454660 run_train: loss = 5.3959  (0.154 sec)
18-06-05 02:17-INFO->> Step 454670 run_train: loss = 5.4897  (0.134 sec)
18-06-05 02:17-INFO->> Step 454680 run_train: loss = 5.3711  (0.193 sec)
18-06-05 02:17-INFO->> Step 454690 run_train: loss = 5.3573  (0.186 sec)
18-06-05 02:17-INFO->> Step 454700 run_train: loss = 5.3804  (0.116 sec)
18-06-05 02:17-INFO->> Step 454710 run_train: loss = 5.3911  (0.164 sec)
18-06-05 02:17-INFO->> Step 454720 run_train: loss = 5.3996  (0.159 sec)
18-06-05 02:17-INFO->> Step 454730 run_train: loss = 5.4619  (0.129 sec)
18-06-05 02:17-INFO->> Step 454740 run_train: loss = 5.4258  (0.149 sec)
18-06-05 02:17-INFO->> Step 454750 run_train: loss = 5.4872  (0.160 sec)
18-06-05 02:17-INFO->> Step 454760 run_train: loss = 5.3764  (0.127 sec)
18-06-05 02:17-INFO->> Step 454770 run_train: loss = 5.4071  (0.106 sec)
18-06-05 02:17-INFO->> Step 454780 run_train: loss = 5.3601  (0.152 sec)
18-06-05 02:17-INFO->> Step 454790 run_train: loss = 5.4618  (0.159 sec)
18-06-05 02:17-INFO->> Step 454800 run_train: loss = 5.3956  (0.160 sec)
18-06-05 02:17-INFO->> Step 454810 run_train: loss = 5.3487  (0.157 sec)
18-06-05 02:17-INFO->> Step 454820 run_train: loss = 5.4077  (0.166 sec)
18-06-05 02:17-INFO->> Step 454830 run_train: loss = 5.3601  (0.138 sec)
18-06-05 02:17-INFO->> Step 454840 run_train: loss = 5.3788  (0.179 sec)
18-06-05 02:17-INFO->> Step 454850 run_train: loss = 5.3927  (0.152 sec)
18-06-05 02:17-INFO->> Step 454860 run_train: loss = 5.3796  (0.140 sec)
18-06-05 02:17-INFO->> Step 454870 run_train: loss = 5.3726  (0.155 sec)
18-06-05 02:17-INFO->> Step 454880 run_train: loss = 5.4643  (0.164 sec)
18-06-05 02:17-INFO->> Step 454890 run_train: loss = 5.4723  (0.187 sec)
18-06-05 02:17-INFO->> Step 454900 run_train: loss = 5.3569  (0.144 sec)
18-06-05 02:17-INFO->> Step 454910 run_train: loss = 5.4181  (0.157 sec)
18-06-05 02:17-INFO->> Step 454920 run_train: loss = 5.4040  (0.158 sec)
18-06-05 02:17-INFO->> Step 454930 run_train: loss = 5.4240  (0.160 sec)
18-06-05 02:17-INFO->> Step 454940 run_train: loss = 5.3964  (0.163 sec)
18-06-05 02:17-INFO->> Step 454950 run_train: loss = 5.3874  (0.171 sec)
18-06-05 02:17-INFO->> Step 454960 run_train: loss = 5.3763  (0.163 sec)
18-06-05 02:17-INFO->> Step 454970 run_train: loss = 5.3338  (0.153 sec)
18-06-05 02:17-INFO->> Step 454980 run_train: loss = 5.4790  (0.130 sec)
18-06-05 02:17-INFO->> Step 454990 run_train: loss = 5.4640  (0.156 sec)
18-06-05 02:17-INFO->> Step 455000 run_train: loss = 5.4076  (0.166 sec)
18-06-05 02:17-INFO->> 2018-06-05 02:17:59.381498 Saving in ckpt
18-06-05 02:17-INFO-Test Data Eval:
18-06-05 02:18-INFO-fpr95 = 0.1743324920297556 and auc = 0.9688917245025562
18-06-05 02:18-INFO->> Step 455010 run_train: loss = 5.4324  (0.137 sec)
18-06-05 02:18-INFO->> Step 455020 run_train: loss = 5.3808  (0.164 sec)
18-06-05 02:18-INFO->> Step 455030 run_train: loss = 5.3881  (0.165 sec)
18-06-05 02:18-INFO->> Step 455040 run_train: loss = 5.3764  (0.148 sec)
18-06-05 02:18-INFO->> Step 455050 run_train: loss = 5.4481  (0.161 sec)
18-06-05 02:18-INFO->> Step 455060 run_train: loss = 5.4723  (0.179 sec)
18-06-05 02:18-INFO->> Step 455070 run_train: loss = 5.4182  (0.154 sec)
18-06-05 02:18-INFO->> Step 455080 run_train: loss = 5.3744  (0.170 sec)
18-06-05 02:18-INFO->> Step 455090 run_train: loss = 5.4643  (0.178 sec)
18-06-05 02:18-INFO->> Step 455100 run_train: loss = 5.3732  (0.156 sec)
18-06-05 02:18-INFO->> Step 455110 run_train: loss = 5.3784  (0.146 sec)
18-06-05 02:18-INFO->> Step 455120 run_train: loss = 5.3622  (0.163 sec)
18-06-05 02:19-INFO->> Step 455130 run_train: loss = 5.4211  (0.144 sec)
18-06-05 02:19-INFO->> Step 455140 run_train: loss = 5.3783  (0.144 sec)
18-06-05 02:19-INFO->> Step 455150 run_train: loss = 5.4551  (0.169 sec)
18-06-05 02:19-INFO->> Step 455160 run_train: loss = 5.4135  (0.186 sec)
18-06-05 02:19-INFO->> Step 455170 run_train: loss = 5.4131  (0.167 sec)
18-06-05 02:19-INFO->> Step 455180 run_train: loss = 5.4183  (0.163 sec)
18-06-05 02:19-INFO->> Step 455190 run_train: loss = 5.4453  (0.129 sec)
18-06-05 02:19-INFO->> Step 455200 run_train: loss = 5.3496  (0.130 sec)
18-06-05 02:19-INFO->> Step 455210 run_train: loss = 5.4315  (0.136 sec)
18-06-05 02:19-INFO->> Step 455220 run_train: loss = 5.3878  (0.163 sec)
18-06-05 02:19-INFO->> Step 455230 run_train: loss = 5.3710  (0.136 sec)
18-06-05 02:19-INFO->> Step 455240 run_train: loss = 5.4589  (0.150 sec)
18-06-05 02:19-INFO->> Step 455250 run_train: loss = 5.3644  (0.134 sec)
18-06-05 02:19-INFO->> Step 455260 run_train: loss = 5.3853  (0.146 sec)
18-06-05 02:19-INFO->> Step 455270 run_train: loss = 5.4208  (0.158 sec)
18-06-05 02:19-INFO->> Step 455280 run_train: loss = 5.4055  (0.167 sec)
18-06-05 02:19-INFO->> Step 455290 run_train: loss = 5.3818  (0.151 sec)
18-06-05 02:19-INFO->> Step 455300 run_train: loss = 5.4047  (0.167 sec)
18-06-05 02:19-INFO->> Step 455310 run_train: loss = 5.3500  (0.146 sec)
18-06-05 02:19-INFO->> Step 455320 run_train: loss = 5.4273  (0.142 sec)
18-06-05 02:19-INFO->> Step 455330 run_train: loss = 5.4572  (0.158 sec)
18-06-05 02:19-INFO->> Step 455340 run_train: loss = 5.3936  (0.154 sec)
18-06-05 02:19-INFO->> Step 455350 run_train: loss = 5.3820  (0.138 sec)
18-06-05 02:19-INFO->> Step 455360 run_train: loss = 5.4255  (0.143 sec)
18-06-05 02:19-INFO->> Step 455370 run_train: loss = 5.3802  (0.150 sec)
18-06-05 02:19-INFO->> Step 455380 run_train: loss = 5.3747  (0.158 sec)
18-06-05 02:19-INFO->> Step 455390 run_train: loss = 5.4485  (0.154 sec)
18-06-05 02:19-INFO->> Step 455400 run_train: loss = 5.3853  (0.145 sec)
18-06-05 02:19-INFO->> Step 455410 run_train: loss = 5.3971  (0.165 sec)
18-06-05 02:19-INFO->> Step 455420 run_train: loss = 5.3702  (0.151 sec)
18-06-05 02:19-INFO->> Step 455430 run_train: loss = 5.4392  (0.165 sec)
18-06-05 02:19-INFO->> Step 455440 run_train: loss = 5.4571  (0.140 sec)
18-06-05 02:19-INFO->> Step 455450 run_train: loss = 5.4233  (0.154 sec)
18-06-05 02:19-INFO->> Step 455460 run_train: loss = 5.3867  (0.175 sec)
18-06-05 02:19-INFO->> Step 455470 run_train: loss = 5.4324  (0.131 sec)
18-06-05 02:19-INFO->> Step 455480 run_train: loss = 5.4555  (0.193 sec)
18-06-05 02:19-INFO->> Step 455490 run_train: loss = 5.4186  (0.152 sec)
18-06-05 02:19-INFO->> Step 455500 run_train: loss = 5.3425  (0.164 sec)
18-06-05 02:20-INFO->> Step 455510 run_train: loss = 5.3920  (0.187 sec)
18-06-05 02:20-INFO->> Step 455520 run_train: loss = 5.4087  (0.167 sec)
18-06-05 02:20-INFO->> Step 455530 run_train: loss = 5.3419  (0.192 sec)
18-06-05 02:20-INFO->> Step 455540 run_train: loss = 5.4316  (0.190 sec)
18-06-05 02:20-INFO->> Step 455550 run_train: loss = 5.3828  (0.162 sec)
18-06-05 02:20-INFO->> Step 455560 run_train: loss = 5.4275  (0.142 sec)
18-06-05 02:20-INFO->> Step 455570 run_train: loss = 5.4285  (0.154 sec)
18-06-05 02:20-INFO->> Step 455580 run_train: loss = 5.4620  (0.150 sec)
18-06-05 02:20-INFO->> Step 455590 run_train: loss = 5.3811  (0.160 sec)
18-06-05 02:20-INFO->> Step 455600 run_train: loss = 5.3428  (0.157 sec)
18-06-05 02:20-INFO->> Step 455610 run_train: loss = 5.4460  (0.157 sec)
18-06-05 02:20-INFO->> Step 455620 run_train: loss = 5.4231  (0.157 sec)
18-06-05 02:20-INFO->> Step 455630 run_train: loss = 5.4053  (0.131 sec)
18-06-05 02:20-INFO->> Step 455640 run_train: loss = 5.3811  (0.200 sec)
18-06-05 02:20-INFO->> Step 455650 run_train: loss = 5.4335  (0.173 sec)
18-06-05 02:20-INFO->> Step 455660 run_train: loss = 5.4476  (0.158 sec)
18-06-05 02:20-INFO->> Step 455670 run_train: loss = 5.4260  (0.177 sec)
18-06-05 02:20-INFO->> Step 455680 run_train: loss = 5.3846  (0.133 sec)
18-06-05 02:20-INFO->> Step 455690 run_train: loss = 5.3912  (0.171 sec)
18-06-05 02:20-INFO->> Step 455700 run_train: loss = 5.3858  (0.163 sec)
18-06-05 02:20-INFO->> Step 455710 run_train: loss = 5.3579  (0.157 sec)
18-06-05 02:20-INFO->> Step 455720 run_train: loss = 5.4447  (0.162 sec)
18-06-05 02:20-INFO->> Step 455730 run_train: loss = 5.4382  (0.161 sec)
18-06-05 02:20-INFO->> Step 455740 run_train: loss = 5.4235  (0.172 sec)
18-06-05 02:20-INFO->> Step 455750 run_train: loss = 5.3778  (0.129 sec)
18-06-05 02:20-INFO->> Step 455760 run_train: loss = 5.4200  (0.130 sec)
18-06-05 02:20-INFO->> Step 455770 run_train: loss = 5.4200  (0.151 sec)
18-06-05 02:20-INFO->> Step 455780 run_train: loss = 5.4256  (0.152 sec)
18-06-05 02:20-INFO->> Step 455790 run_train: loss = 5.3938  (0.183 sec)
18-06-05 02:20-INFO->> Step 455800 run_train: loss = 5.4135  (0.142 sec)
18-06-05 02:20-INFO->> Step 455810 run_train: loss = 5.3941  (0.150 sec)
18-06-05 02:20-INFO->> Step 455820 run_train: loss = 5.4758  (0.163 sec)
18-06-05 02:20-INFO->> Step 455830 run_train: loss = 5.4069  (0.141 sec)
18-06-05 02:20-INFO->> Step 455840 run_train: loss = 5.3518  (0.133 sec)
18-06-05 02:20-INFO->> Step 455850 run_train: loss = 5.4580  (0.143 sec)
18-06-05 02:20-INFO->> Step 455860 run_train: loss = 5.3340  (0.147 sec)
18-06-05 02:20-INFO->> Step 455870 run_train: loss = 5.4277  (0.153 sec)
18-06-05 02:20-INFO->> Step 455880 run_train: loss = 5.4189  (0.191 sec)
18-06-05 02:21-INFO->> Step 455890 run_train: loss = 5.4326  (0.122 sec)
18-06-05 02:21-INFO->> Step 455900 run_train: loss = 5.3821  (0.139 sec)
18-06-05 02:21-INFO->> Step 455910 run_train: loss = 5.4273  (0.188 sec)
18-06-05 02:21-INFO->> Step 455920 run_train: loss = 5.4190  (0.164 sec)
18-06-05 02:21-INFO->> Step 455930 run_train: loss = 5.4288  (0.151 sec)
18-06-05 02:21-INFO->> Step 455940 run_train: loss = 5.3920  (0.157 sec)
18-06-05 02:21-INFO->> Step 455950 run_train: loss = 5.3928  (0.208 sec)
18-06-05 02:21-INFO->> Step 455960 run_train: loss = 5.4926  (0.161 sec)
18-06-05 02:21-INFO->> Step 455970 run_train: loss = 5.4230  (0.159 sec)
18-06-05 02:21-INFO->> Step 455980 run_train: loss = 5.4001  (0.126 sec)
18-06-05 02:21-INFO->> Step 455990 run_train: loss = 5.4779  (0.131 sec)
18-06-05 02:21-INFO->> Step 456000 run_train: loss = 5.4017  (0.163 sec)
18-06-05 02:21-INFO->> 2018-06-05 02:21:17.753756 Saving in ckpt
18-06-05 02:21-INFO-Test Data Eval:
18-06-05 02:21-INFO-fpr95 = 0.16812234325185973 and auc = 0.9694105119574268
18-06-05 02:21-INFO->> Step 456010 run_train: loss = 5.4007  (0.203 sec)
18-06-05 02:22-INFO->> Step 456020 run_train: loss = 5.4299  (0.168 sec)
18-06-05 02:22-INFO->> Step 456030 run_train: loss = 5.4166  (0.148 sec)
18-06-05 02:22-INFO->> Step 456040 run_train: loss = 5.3898  (0.150 sec)
18-06-05 02:22-INFO->> Step 456050 run_train: loss = 5.4252  (0.186 sec)
18-06-05 02:22-INFO->> Step 456060 run_train: loss = 5.4160  (0.158 sec)
18-06-05 02:22-INFO->> Step 456070 run_train: loss = 5.3495  (0.156 sec)
18-06-05 02:22-INFO->> Step 456080 run_train: loss = 5.4108  (0.173 sec)
18-06-05 02:22-INFO->> Step 456090 run_train: loss = 5.3547  (0.125 sec)
18-06-05 02:22-INFO->> Step 456100 run_train: loss = 5.4338  (0.170 sec)
18-06-05 02:22-INFO->> Step 456110 run_train: loss = 5.4299  (0.160 sec)
18-06-05 02:22-INFO->> Step 456120 run_train: loss = 5.4087  (0.154 sec)
18-06-05 02:22-INFO->> Step 456130 run_train: loss = 5.2764  (0.145 sec)
18-06-05 02:22-INFO->> Step 456140 run_train: loss = 5.3601  (0.141 sec)
18-06-05 02:22-INFO->> Step 456150 run_train: loss = 5.4065  (0.155 sec)
18-06-05 02:22-INFO->> Step 456160 run_train: loss = 5.4016  (0.138 sec)
18-06-05 02:22-INFO->> Step 456170 run_train: loss = 5.4765  (0.158 sec)
18-06-05 02:22-INFO->> Step 456180 run_train: loss = 5.4293  (0.174 sec)
18-06-05 02:22-INFO->> Step 456190 run_train: loss = 5.4048  (0.155 sec)
18-06-05 02:22-INFO->> Step 456200 run_train: loss = 5.4062  (0.167 sec)
18-06-05 02:22-INFO->> Step 456210 run_train: loss = 5.4225  (0.194 sec)
18-06-05 02:22-INFO->> Step 456220 run_train: loss = 5.4205  (0.139 sec)
18-06-05 02:22-INFO->> Step 456230 run_train: loss = 5.4307  (0.165 sec)
18-06-05 02:22-INFO->> Step 456240 run_train: loss = 5.3800  (0.192 sec)
18-06-05 02:22-INFO->> Step 456250 run_train: loss = 5.4045  (0.162 sec)
18-06-05 02:22-INFO->> Step 456260 run_train: loss = 5.4379  (0.188 sec)
18-06-05 02:22-INFO->> Step 456270 run_train: loss = 5.3206  (0.159 sec)
18-06-05 02:22-INFO->> Step 456280 run_train: loss = 5.3841  (0.150 sec)
18-06-05 02:22-INFO->> Step 456290 run_train: loss = 5.4165  (0.158 sec)
18-06-05 02:22-INFO->> Step 456300 run_train: loss = 5.4531  (0.150 sec)
18-06-05 02:22-INFO->> Step 456310 run_train: loss = 5.4061  (0.146 sec)
18-06-05 02:22-INFO->> Step 456320 run_train: loss = 5.4163  (0.134 sec)
18-06-05 02:22-INFO->> Step 456330 run_train: loss = 5.4489  (0.144 sec)
18-06-05 02:22-INFO->> Step 456340 run_train: loss = 5.4271  (0.172 sec)
18-06-05 02:22-INFO->> Step 456350 run_train: loss = 5.3496  (0.156 sec)
18-06-05 02:22-INFO->> Step 456360 run_train: loss = 5.4108  (0.141 sec)
18-06-05 02:22-INFO->> Step 456370 run_train: loss = 5.4264  (0.159 sec)
18-06-05 02:22-INFO->> Step 456380 run_train: loss = 5.4454  (0.159 sec)
18-06-05 02:22-INFO->> Step 456390 run_train: loss = 5.4231  (0.138 sec)
18-06-05 02:23-INFO->> Step 456400 run_train: loss = 5.4167  (0.153 sec)
18-06-05 02:23-INFO->> Step 456410 run_train: loss = 5.4099  (0.146 sec)
18-06-05 02:23-INFO->> Step 456420 run_train: loss = 5.4270  (0.158 sec)
18-06-05 02:23-INFO->> Step 456430 run_train: loss = 5.3482  (0.130 sec)
18-06-05 02:23-INFO->> Step 456440 run_train: loss = 5.4085  (0.174 sec)
18-06-05 02:23-INFO->> Step 456450 run_train: loss = 5.4001  (0.166 sec)
18-06-05 02:23-INFO->> Step 456460 run_train: loss = 5.4374  (0.136 sec)
18-06-05 02:23-INFO->> Step 456470 run_train: loss = 5.3775  (0.164 sec)
18-06-05 02:23-INFO->> Step 456480 run_train: loss = 5.3993  (0.155 sec)
18-06-05 02:23-INFO->> Step 456490 run_train: loss = 5.4060  (0.149 sec)
18-06-05 02:23-INFO->> Step 456500 run_train: loss = 5.3572  (0.162 sec)
18-06-05 02:23-INFO->> Step 456510 run_train: loss = 5.4463  (0.147 sec)
18-06-05 02:23-INFO->> Step 456520 run_train: loss = 5.3747  (0.147 sec)
18-06-05 02:23-INFO->> Step 456530 run_train: loss = 5.4671  (0.187 sec)
18-06-05 02:23-INFO->> Step 456540 run_train: loss = 5.4099  (0.170 sec)
18-06-05 02:23-INFO->> Step 456550 run_train: loss = 5.4430  (0.129 sec)
18-06-05 02:23-INFO->> Step 456560 run_train: loss = 5.4754  (0.153 sec)
18-06-05 02:23-INFO->> Step 456570 run_train: loss = 5.4305  (0.178 sec)
18-06-05 02:23-INFO->> Step 456580 run_train: loss = 5.4167  (0.141 sec)
18-06-05 02:23-INFO->> Step 456590 run_train: loss = 5.3823  (0.159 sec)
18-06-05 02:23-INFO->> Step 456600 run_train: loss = 5.4268  (0.125 sec)
18-06-05 02:23-INFO->> Step 456610 run_train: loss = 5.3696  (0.162 sec)
18-06-05 02:23-INFO->> Step 456620 run_train: loss = 5.4053  (0.152 sec)
18-06-05 02:23-INFO->> Step 456630 run_train: loss = 5.4296  (0.175 sec)
18-06-05 02:23-INFO->> Step 456640 run_train: loss = 5.4371  (0.163 sec)
18-06-05 02:23-INFO->> Step 456650 run_train: loss = 5.4173  (0.177 sec)
18-06-05 02:23-INFO->> Step 456660 run_train: loss = 5.4158  (0.136 sec)
18-06-05 02:23-INFO->> Step 456670 run_train: loss = 5.4427  (0.179 sec)
18-06-05 02:23-INFO->> Step 456680 run_train: loss = 5.4285  (0.156 sec)
18-06-05 02:23-INFO->> Step 456690 run_train: loss = 5.5120  (0.158 sec)
18-06-05 02:23-INFO->> Step 456700 run_train: loss = 5.3818  (0.185 sec)
18-06-05 02:23-INFO->> Step 456710 run_train: loss = 5.4097  (0.146 sec)
18-06-05 02:23-INFO->> Step 456720 run_train: loss = 5.4101  (0.153 sec)
18-06-05 02:23-INFO->> Step 456730 run_train: loss = 5.4313  (0.173 sec)
18-06-05 02:23-INFO->> Step 456740 run_train: loss = 5.4286  (0.171 sec)
18-06-05 02:23-INFO->> Step 456750 run_train: loss = 5.3812  (0.159 sec)
18-06-05 02:23-INFO->> Step 456760 run_train: loss = 5.3910  (0.152 sec)
18-06-05 02:23-INFO->> Step 456770 run_train: loss = 5.4810  (0.145 sec)
18-06-05 02:24-INFO->> Step 456780 run_train: loss = 5.3640  (0.165 sec)
18-06-05 02:24-INFO->> Step 456790 run_train: loss = 5.4353  (0.159 sec)
18-06-05 02:24-INFO->> Step 456800 run_train: loss = 5.3823  (0.158 sec)
18-06-05 02:24-INFO->> Step 456810 run_train: loss = 5.4244  (0.145 sec)
18-06-05 02:24-INFO->> Step 456820 run_train: loss = 5.3829  (0.169 sec)
18-06-05 02:24-INFO->> Step 456830 run_train: loss = 5.4000  (0.167 sec)
18-06-05 02:24-INFO->> Step 456840 run_train: loss = 5.3405  (0.193 sec)
18-06-05 02:24-INFO->> Step 456850 run_train: loss = 5.3587  (0.153 sec)
18-06-05 02:24-INFO->> Step 456860 run_train: loss = 5.4481  (0.180 sec)
18-06-05 02:24-INFO->> Step 456870 run_train: loss = 5.3960  (0.173 sec)
18-06-05 02:24-INFO->> Step 456880 run_train: loss = 5.4144  (0.182 sec)
18-06-05 02:24-INFO->> Step 456890 run_train: loss = 5.4433  (0.152 sec)
18-06-05 02:24-INFO->> Step 456900 run_train: loss = 5.4372  (0.141 sec)
18-06-05 02:24-INFO->> Step 456910 run_train: loss = 5.3867  (0.160 sec)
18-06-05 02:24-INFO->> Step 456920 run_train: loss = 5.3722  (0.145 sec)
18-06-05 02:24-INFO->> Step 456930 run_train: loss = 5.4250  (0.147 sec)
18-06-05 02:24-INFO->> Step 456940 run_train: loss = 5.3837  (0.156 sec)
18-06-05 02:24-INFO->> Step 456950 run_train: loss = 5.4554  (0.128 sec)
18-06-05 02:24-INFO->> Step 456960 run_train: loss = 5.3984  (0.159 sec)
18-06-05 02:24-INFO->> Step 456970 run_train: loss = 5.3801  (0.150 sec)
18-06-05 02:24-INFO->> Step 456980 run_train: loss = 5.4165  (0.160 sec)
18-06-05 02:24-INFO->> Step 456990 run_train: loss = 5.3857  (0.152 sec)
18-06-05 02:24-INFO->> Step 457000 run_train: loss = 5.4607  (0.170 sec)
18-06-05 02:24-INFO->> 2018-06-05 02:24:36.399496 Saving in ckpt
18-06-05 02:24-INFO-Test Data Eval:
18-06-05 02:25-INFO-fpr95 = 0.17551142401700318 and auc = 0.9686734988374366
18-06-05 02:25-INFO->> Step 457010 run_train: loss = 5.4796  (0.154 sec)
18-06-05 02:25-INFO->> Step 457020 run_train: loss = 5.3823  (0.139 sec)
18-06-05 02:25-INFO->> Step 457030 run_train: loss = 5.4403  (0.176 sec)
18-06-05 02:25-INFO->> Step 457040 run_train: loss = 5.3427  (0.151 sec)
18-06-05 02:25-INFO->> Step 457050 run_train: loss = 5.4061  (0.143 sec)
18-06-05 02:25-INFO->> Step 457060 run_train: loss = 5.4633  (0.155 sec)
18-06-05 02:25-INFO->> Step 457070 run_train: loss = 5.3984  (0.142 sec)
18-06-05 02:25-INFO->> Step 457080 run_train: loss = 5.4068  (0.156 sec)
18-06-05 02:25-INFO->> Step 457090 run_train: loss = 5.4393  (0.152 sec)
18-06-05 02:25-INFO->> Step 457100 run_train: loss = 5.4025  (0.186 sec)
18-06-05 02:25-INFO->> Step 457110 run_train: loss = 5.3680  (0.164 sec)
18-06-05 02:25-INFO->> Step 457120 run_train: loss = 5.4915  (0.132 sec)
18-06-05 02:25-INFO->> Step 457130 run_train: loss = 5.4794  (0.178 sec)
18-06-05 02:25-INFO->> Step 457140 run_train: loss = 5.4264  (0.130 sec)
18-06-05 02:25-INFO->> Step 457150 run_train: loss = 5.3563  (0.156 sec)
18-06-05 02:25-INFO->> Step 457160 run_train: loss = 5.3771  (0.136 sec)
18-06-05 02:25-INFO->> Step 457170 run_train: loss = 5.4237  (0.187 sec)
18-06-05 02:25-INFO->> Step 457180 run_train: loss = 5.4118  (0.163 sec)
18-06-05 02:25-INFO->> Step 457190 run_train: loss = 5.4455  (0.133 sec)
18-06-05 02:25-INFO->> Step 457200 run_train: loss = 5.4130  (0.132 sec)
18-06-05 02:25-INFO->> Step 457210 run_train: loss = 5.4233  (0.180 sec)
18-06-05 02:25-INFO->> Step 457220 run_train: loss = 5.3911  (0.169 sec)
18-06-05 02:25-INFO->> Step 457230 run_train: loss = 5.3720  (0.146 sec)
18-06-05 02:25-INFO->> Step 457240 run_train: loss = 5.3742  (0.150 sec)
18-06-05 02:25-INFO->> Step 457250 run_train: loss = 5.3890  (0.144 sec)
18-06-05 02:25-INFO->> Step 457260 run_train: loss = 5.3784  (0.170 sec)
18-06-05 02:25-INFO->> Step 457270 run_train: loss = 5.3899  (0.218 sec)
18-06-05 02:26-INFO->> Step 457280 run_train: loss = 5.3748  (0.166 sec)
18-06-05 02:26-INFO->> Step 457290 run_train: loss = 5.3952  (0.164 sec)
18-06-05 02:26-INFO->> Step 457300 run_train: loss = 5.3877  (0.162 sec)
18-06-05 02:26-INFO->> Step 457310 run_train: loss = 5.4952  (0.200 sec)
18-06-05 02:26-INFO->> Step 457320 run_train: loss = 5.4347  (0.170 sec)
18-06-05 02:26-INFO->> Step 457330 run_train: loss = 5.4057  (0.165 sec)
18-06-05 02:26-INFO->> Step 457340 run_train: loss = 5.3850  (0.121 sec)
18-06-05 02:26-INFO->> Step 457350 run_train: loss = 5.4122  (0.160 sec)
18-06-05 02:26-INFO->> Step 457360 run_train: loss = 5.3772  (0.162 sec)
18-06-05 02:26-INFO->> Step 457370 run_train: loss = 5.4054  (0.148 sec)
18-06-05 02:26-INFO->> Step 457380 run_train: loss = 5.4221  (0.163 sec)
18-06-05 02:26-INFO->> Step 457390 run_train: loss = 5.4321  (0.152 sec)
18-06-05 02:26-INFO->> Step 457400 run_train: loss = 5.3836  (0.159 sec)
18-06-05 02:26-INFO->> Step 457410 run_train: loss = 5.3765  (0.157 sec)
18-06-05 02:26-INFO->> Step 457420 run_train: loss = 5.3860  (0.135 sec)
18-06-05 02:26-INFO->> Step 457430 run_train: loss = 5.3601  (0.174 sec)
18-06-05 02:26-INFO->> Step 457440 run_train: loss = 5.4250  (0.151 sec)
18-06-05 02:26-INFO->> Step 457450 run_train: loss = 5.3944  (0.170 sec)
18-06-05 02:26-INFO->> Step 457460 run_train: loss = 5.4184  (0.160 sec)
18-06-05 02:26-INFO->> Step 457470 run_train: loss = 5.3704  (0.164 sec)
18-06-05 02:26-INFO->> Step 457480 run_train: loss = 5.4303  (0.155 sec)
18-06-05 02:26-INFO->> Step 457490 run_train: loss = 5.3962  (0.164 sec)
18-06-05 02:26-INFO->> Step 457500 run_train: loss = 5.4238  (0.141 sec)
18-06-05 02:26-INFO->> Step 457510 run_train: loss = 5.3689  (0.155 sec)
18-06-05 02:26-INFO->> Step 457520 run_train: loss = 5.4393  (0.184 sec)
18-06-05 02:26-INFO->> Step 457530 run_train: loss = 5.3609  (0.140 sec)
18-06-05 02:26-INFO->> Step 457540 run_train: loss = 5.3961  (0.123 sec)
18-06-05 02:26-INFO->> Step 457550 run_train: loss = 5.4256  (0.146 sec)
18-06-05 02:26-INFO->> Step 457560 run_train: loss = 5.3866  (0.204 sec)
18-06-05 02:26-INFO->> Step 457570 run_train: loss = 5.4213  (0.184 sec)
18-06-05 02:26-INFO->> Step 457580 run_train: loss = 5.4537  (0.165 sec)
18-06-05 02:26-INFO->> Step 457590 run_train: loss = 5.3860  (0.120 sec)
18-06-05 02:26-INFO->> Step 457600 run_train: loss = 5.3854  (0.163 sec)
18-06-05 02:26-INFO->> Step 457610 run_train: loss = 5.4885  (0.133 sec)
18-06-05 02:26-INFO->> Step 457620 run_train: loss = 5.3500  (0.158 sec)
18-06-05 02:26-INFO->> Step 457630 run_train: loss = 5.3707  (0.170 sec)
18-06-05 02:26-INFO->> Step 457640 run_train: loss = 5.4474  (0.180 sec)
18-06-05 02:26-INFO->> Step 457650 run_train: loss = 5.3923  (0.173 sec)
18-06-05 02:27-INFO->> Step 457660 run_train: loss = 5.4424  (0.170 sec)
18-06-05 02:27-INFO->> Step 457670 run_train: loss = 5.4771  (0.145 sec)
18-06-05 02:27-INFO->> Step 457680 run_train: loss = 5.3922  (0.170 sec)
18-06-05 02:27-INFO->> Step 457690 run_train: loss = 5.4603  (0.150 sec)
18-06-05 02:27-INFO->> Step 457700 run_train: loss = 5.4510  (0.152 sec)
18-06-05 02:27-INFO->> Step 457710 run_train: loss = 5.3576  (0.194 sec)
18-06-05 02:27-INFO->> Step 457720 run_train: loss = 5.3986  (0.155 sec)
18-06-05 02:27-INFO->> Step 457730 run_train: loss = 5.4345  (0.155 sec)
18-06-05 02:27-INFO->> Step 457740 run_train: loss = 5.4131  (0.183 sec)
18-06-05 02:27-INFO->> Step 457750 run_train: loss = 5.3749  (0.190 sec)
18-06-05 02:27-INFO->> Step 457760 run_train: loss = 5.4115  (0.187 sec)
18-06-05 02:27-INFO->> Step 457770 run_train: loss = 5.4510  (0.164 sec)
18-06-05 02:27-INFO->> Step 457780 run_train: loss = 5.4312  (0.158 sec)
18-06-05 02:27-INFO->> Step 457790 run_train: loss = 5.4051  (0.140 sec)
18-06-05 02:27-INFO->> Step 457800 run_train: loss = 5.3941  (0.136 sec)
18-06-05 02:27-INFO->> Step 457810 run_train: loss = 5.4531  (0.135 sec)
18-06-05 02:27-INFO->> Step 457820 run_train: loss = 5.4164  (0.155 sec)
18-06-05 02:27-INFO->> Step 457830 run_train: loss = 5.3543  (0.187 sec)
18-06-05 02:27-INFO->> Step 457840 run_train: loss = 5.3978  (0.155 sec)
18-06-05 02:27-INFO->> Step 457850 run_train: loss = 5.4111  (0.196 sec)
18-06-05 02:27-INFO->> Step 457860 run_train: loss = 5.3894  (0.166 sec)
18-06-05 02:27-INFO->> Step 457870 run_train: loss = 5.4081  (0.149 sec)
18-06-05 02:27-INFO->> Step 457880 run_train: loss = 5.4174  (0.190 sec)
18-06-05 02:27-INFO->> Step 457890 run_train: loss = 5.3470  (0.157 sec)
18-06-05 02:27-INFO->> Step 457900 run_train: loss = 5.4688  (0.181 sec)
18-06-05 02:27-INFO->> Step 457910 run_train: loss = 5.4417  (0.147 sec)
18-06-05 02:27-INFO->> Step 457920 run_train: loss = 5.3818  (0.136 sec)
18-06-05 02:27-INFO->> Step 457930 run_train: loss = 5.4029  (0.128 sec)
18-06-05 02:27-INFO->> Step 457940 run_train: loss = 5.3925  (0.149 sec)
18-06-05 02:27-INFO->> Step 457950 run_train: loss = 5.4669  (0.170 sec)
18-06-05 02:27-INFO->> Step 457960 run_train: loss = 5.4109  (0.158 sec)
18-06-05 02:27-INFO->> Step 457970 run_train: loss = 5.3765  (0.165 sec)
18-06-05 02:27-INFO->> Step 457980 run_train: loss = 5.4417  (0.172 sec)
18-06-05 02:27-INFO->> Step 457990 run_train: loss = 5.4066  (0.152 sec)
18-06-05 02:27-INFO->> Step 458000 run_train: loss = 5.4588  (0.159 sec)
18-06-05 02:27-INFO->> 2018-06-05 02:27:55.056231 Saving in ckpt
18-06-05 02:27-INFO-Test Data Eval:
18-06-05 02:28-INFO-fpr95 = 0.17386756110520724 and auc = 0.9689961279116723
18-06-05 02:28-INFO->> Step 458010 run_train: loss = 5.3829  (0.155 sec)
18-06-05 02:28-INFO->> Step 458020 run_train: loss = 5.4263  (0.155 sec)
18-06-05 02:28-INFO->> Step 458030 run_train: loss = 5.4408  (0.174 sec)
18-06-05 02:28-INFO->> Step 458040 run_train: loss = 5.3710  (0.158 sec)
18-06-05 02:28-INFO->> Step 458050 run_train: loss = 5.4395  (0.172 sec)
18-06-05 02:28-INFO->> Step 458060 run_train: loss = 5.4085  (0.135 sec)
18-06-05 02:28-INFO->> Step 458070 run_train: loss = 5.4033  (0.162 sec)
18-06-05 02:28-INFO->> Step 458080 run_train: loss = 5.4244  (0.163 sec)
18-06-05 02:28-INFO->> Step 458090 run_train: loss = 5.4458  (0.171 sec)
18-06-05 02:28-INFO->> Step 458100 run_train: loss = 5.3958  (0.151 sec)
18-06-05 02:28-INFO->> Step 458110 run_train: loss = 5.4323  (0.146 sec)
18-06-05 02:28-INFO->> Step 458120 run_train: loss = 5.3720  (0.118 sec)
18-06-05 02:28-INFO->> Step 458130 run_train: loss = 5.4071  (0.147 sec)
18-06-05 02:28-INFO->> Step 458140 run_train: loss = 5.4601  (0.167 sec)
18-06-05 02:28-INFO->> Step 458150 run_train: loss = 5.3747  (0.134 sec)
18-06-05 02:29-INFO->> Step 458160 run_train: loss = 5.4258  (0.165 sec)
18-06-05 02:29-INFO->> Step 458170 run_train: loss = 5.4003  (0.157 sec)
18-06-05 02:29-INFO->> Step 458180 run_train: loss = 5.4807  (0.155 sec)
18-06-05 02:29-INFO->> Step 458190 run_train: loss = 5.4193  (0.146 sec)
18-06-05 02:29-INFO->> Step 458200 run_train: loss = 5.3694  (0.168 sec)
18-06-05 02:29-INFO->> Step 458210 run_train: loss = 5.4461  (0.167 sec)
18-06-05 02:29-INFO->> Step 458220 run_train: loss = 5.4579  (0.173 sec)
18-06-05 02:29-INFO->> Step 458230 run_train: loss = 5.3605  (0.165 sec)
18-06-05 02:29-INFO->> Step 458240 run_train: loss = 5.4143  (0.159 sec)
18-06-05 02:29-INFO->> Step 458250 run_train: loss = 5.3796  (0.153 sec)
18-06-05 02:29-INFO->> Step 458260 run_train: loss = 5.4043  (0.160 sec)
18-06-05 02:29-INFO->> Step 458270 run_train: loss = 5.3634  (0.134 sec)
18-06-05 02:29-INFO->> Step 458280 run_train: loss = 5.4123  (0.178 sec)
18-06-05 02:29-INFO->> Step 458290 run_train: loss = 5.4039  (0.159 sec)
18-06-05 02:29-INFO->> Step 458300 run_train: loss = 5.3453  (0.159 sec)
18-06-05 02:29-INFO->> Step 458310 run_train: loss = 5.4410  (0.163 sec)
18-06-05 02:29-INFO->> Step 458320 run_train: loss = 5.4224  (0.154 sec)
18-06-05 02:29-INFO->> Step 458330 run_train: loss = 5.4370  (0.144 sec)
18-06-05 02:29-INFO->> Step 458340 run_train: loss = 5.3848  (0.180 sec)
18-06-05 02:29-INFO->> Step 458350 run_train: loss = 5.4325  (0.197 sec)
18-06-05 02:29-INFO->> Step 458360 run_train: loss = 5.4260  (0.152 sec)
18-06-05 02:29-INFO->> Step 458370 run_train: loss = 5.4387  (0.149 sec)
18-06-05 02:29-INFO->> Step 458380 run_train: loss = 5.3826  (0.183 sec)
18-06-05 02:29-INFO->> Step 458390 run_train: loss = 5.4681  (0.150 sec)
18-06-05 02:29-INFO->> Step 458400 run_train: loss = 5.4299  (0.171 sec)
18-06-05 02:29-INFO->> Step 458410 run_train: loss = 5.3702  (0.196 sec)
18-06-05 02:29-INFO->> Step 458420 run_train: loss = 5.4019  (0.182 sec)
18-06-05 02:29-INFO->> Step 458430 run_train: loss = 5.3696  (0.185 sec)
18-06-05 02:29-INFO->> Step 458440 run_train: loss = 5.4873  (0.168 sec)
18-06-05 02:29-INFO->> Step 458450 run_train: loss = 5.4169  (0.157 sec)
18-06-05 02:29-INFO->> Step 458460 run_train: loss = 5.4512  (0.141 sec)
18-06-05 02:29-INFO->> Step 458470 run_train: loss = 5.4396  (0.176 sec)
18-06-05 02:29-INFO->> Step 458480 run_train: loss = 5.4149  (0.176 sec)
18-06-05 02:29-INFO->> Step 458490 run_train: loss = 5.4154  (0.190 sec)
18-06-05 02:29-INFO->> Step 458500 run_train: loss = 5.3522  (0.159 sec)
18-06-05 02:29-INFO->> Step 458510 run_train: loss = 5.3512  (0.143 sec)
18-06-05 02:29-INFO->> Step 458520 run_train: loss = 5.3393  (0.154 sec)
18-06-05 02:29-INFO->> Step 458530 run_train: loss = 5.5146  (0.154 sec)
18-06-05 02:30-INFO->> Step 458540 run_train: loss = 5.4212  (0.156 sec)
18-06-05 02:30-INFO->> Step 458550 run_train: loss = 5.4353  (0.166 sec)
18-06-05 02:30-INFO->> Step 458560 run_train: loss = 5.4241  (0.165 sec)
18-06-05 02:30-INFO->> Step 458570 run_train: loss = 5.3868  (0.161 sec)
18-06-05 02:30-INFO->> Step 458580 run_train: loss = 5.4008  (0.192 sec)
18-06-05 02:30-INFO->> Step 458590 run_train: loss = 5.4336  (0.162 sec)
18-06-05 02:30-INFO->> Step 458600 run_train: loss = 5.4498  (0.202 sec)
18-06-05 02:30-INFO->> Step 458610 run_train: loss = 5.4089  (0.189 sec)
18-06-05 02:30-INFO->> Step 458620 run_train: loss = 5.4238  (0.138 sec)
18-06-05 02:30-INFO->> Step 458630 run_train: loss = 5.4113  (0.144 sec)
18-06-05 02:30-INFO->> Step 458640 run_train: loss = 5.3728  (0.138 sec)
18-06-05 02:30-INFO->> Step 458650 run_train: loss = 5.4256  (0.156 sec)
18-06-05 02:30-INFO->> Step 458660 run_train: loss = 5.3629  (0.165 sec)
18-06-05 02:30-INFO->> Step 458670 run_train: loss = 5.4543  (0.147 sec)
18-06-05 02:30-INFO->> Step 458680 run_train: loss = 5.4159  (0.160 sec)
18-06-05 02:30-INFO->> Step 458690 run_train: loss = 5.3318  (0.173 sec)
18-06-05 02:30-INFO->> Step 458700 run_train: loss = 5.3532  (0.164 sec)
18-06-05 02:30-INFO->> Step 458710 run_train: loss = 5.4240  (0.157 sec)
18-06-05 02:30-INFO->> Step 458720 run_train: loss = 5.4441  (0.158 sec)
18-06-05 02:30-INFO->> Step 458730 run_train: loss = 5.3724  (0.157 sec)
18-06-05 02:30-INFO->> Step 458740 run_train: loss = 5.3577  (0.146 sec)
18-06-05 02:30-INFO->> Step 458750 run_train: loss = 5.4523  (0.152 sec)
18-06-05 02:30-INFO->> Step 458760 run_train: loss = 5.4037  (0.143 sec)
18-06-05 02:30-INFO->> Step 458770 run_train: loss = 5.4112  (0.147 sec)
18-06-05 02:30-INFO->> Step 458780 run_train: loss = 5.4131  (0.187 sec)
18-06-05 02:30-INFO->> Step 458790 run_train: loss = 5.3889  (0.177 sec)
18-06-05 02:30-INFO->> Step 458800 run_train: loss = 5.4233  (0.172 sec)
18-06-05 02:30-INFO->> Step 458810 run_train: loss = 5.3996  (0.160 sec)
18-06-05 02:30-INFO->> Step 458820 run_train: loss = 5.4331  (0.147 sec)
18-06-05 02:30-INFO->> Step 458830 run_train: loss = 5.4336  (0.145 sec)
18-06-05 02:30-INFO->> Step 458840 run_train: loss = 5.3952  (0.159 sec)
18-06-05 02:30-INFO->> Step 458850 run_train: loss = 5.4212  (0.156 sec)
18-06-05 02:30-INFO->> Step 458860 run_train: loss = 5.4414  (0.164 sec)
18-06-05 02:30-INFO->> Step 458870 run_train: loss = 5.3581  (0.153 sec)
18-06-05 02:30-INFO->> Step 458880 run_train: loss = 5.4066  (0.162 sec)
18-06-05 02:30-INFO->> Step 458890 run_train: loss = 5.4115  (0.176 sec)
18-06-05 02:30-INFO->> Step 458900 run_train: loss = 5.4092  (0.162 sec)
18-06-05 02:30-INFO->> Step 458910 run_train: loss = 5.4163  (0.171 sec)
18-06-05 02:31-INFO->> Step 458920 run_train: loss = 5.3096  (0.205 sec)
18-06-05 02:31-INFO->> Step 458930 run_train: loss = 5.3626  (0.149 sec)
18-06-05 02:31-INFO->> Step 458940 run_train: loss = 5.4087  (0.121 sec)
18-06-05 02:31-INFO->> Step 458950 run_train: loss = 5.3474  (0.135 sec)
18-06-05 02:31-INFO->> Step 458960 run_train: loss = 5.3774  (0.147 sec)
18-06-05 02:31-INFO->> Step 458970 run_train: loss = 5.3741  (0.166 sec)
18-06-05 02:31-INFO->> Step 458980 run_train: loss = 5.4451  (0.155 sec)
18-06-05 02:31-INFO->> Step 458990 run_train: loss = 5.3749  (0.150 sec)
18-06-05 02:31-INFO->> Step 459000 run_train: loss = 5.4448  (0.149 sec)
18-06-05 02:31-INFO->> 2018-06-05 02:31:13.641453 Saving in ckpt
18-06-05 02:31-INFO-Test Data Eval:
18-06-05 02:31-INFO-fpr95 = 0.1740087008501594 and auc = 0.9687600148054242
18-06-05 02:31-INFO->> Step 459010 run_train: loss = 5.4622  (0.175 sec)
18-06-05 02:31-INFO->> Step 459020 run_train: loss = 5.3397  (0.160 sec)
18-06-05 02:31-INFO->> Step 459030 run_train: loss = 5.3949  (0.160 sec)
18-06-05 02:32-INFO->> Step 459040 run_train: loss = 5.4294  (0.147 sec)
18-06-05 02:32-INFO->> Step 459050 run_train: loss = 5.4400  (0.189 sec)
18-06-05 02:32-INFO->> Step 459060 run_train: loss = 5.4412  (0.152 sec)
18-06-05 02:32-INFO->> Step 459070 run_train: loss = 5.3932  (0.135 sec)
18-06-05 02:32-INFO->> Step 459080 run_train: loss = 5.4109  (0.196 sec)
18-06-05 02:32-INFO->> Step 459090 run_train: loss = 5.4136  (0.184 sec)
18-06-05 02:32-INFO->> Step 459100 run_train: loss = 5.3911  (0.155 sec)
18-06-05 02:32-INFO->> Step 459110 run_train: loss = 5.3309  (0.184 sec)
18-06-05 02:32-INFO->> Step 459120 run_train: loss = 5.4094  (0.154 sec)
18-06-05 02:32-INFO->> Step 459130 run_train: loss = 5.3954  (0.163 sec)
18-06-05 02:32-INFO->> Step 459140 run_train: loss = 5.4193  (0.121 sec)
18-06-05 02:32-INFO->> Step 459150 run_train: loss = 5.3671  (0.143 sec)
18-06-05 02:32-INFO->> Step 459160 run_train: loss = 5.3698  (0.157 sec)
18-06-05 02:32-INFO->> Step 459170 run_train: loss = 5.3734  (0.204 sec)
18-06-05 02:32-INFO->> Step 459180 run_train: loss = 5.3967  (0.143 sec)
18-06-05 02:32-INFO->> Step 459190 run_train: loss = 5.3812  (0.172 sec)
18-06-05 02:32-INFO->> Step 459200 run_train: loss = 5.4356  (0.181 sec)
18-06-05 02:32-INFO->> Step 459210 run_train: loss = 5.3633  (0.159 sec)
18-06-05 02:32-INFO->> Step 459220 run_train: loss = 5.4147  (0.165 sec)
18-06-05 02:32-INFO->> Step 459230 run_train: loss = 5.4257  (0.162 sec)
18-06-05 02:32-INFO->> Step 459240 run_train: loss = 5.4261  (0.188 sec)
18-06-05 02:32-INFO->> Step 459250 run_train: loss = 5.4603  (0.146 sec)
18-06-05 02:32-INFO->> Step 459260 run_train: loss = 5.3493  (0.194 sec)
18-06-05 02:32-INFO->> Step 459270 run_train: loss = 5.4267  (0.147 sec)
18-06-05 02:32-INFO->> Step 459280 run_train: loss = 5.3971  (0.172 sec)
18-06-05 02:32-INFO->> Step 459290 run_train: loss = 5.3995  (0.166 sec)
18-06-05 02:32-INFO->> Step 459300 run_train: loss = 5.3279  (0.191 sec)
18-06-05 02:32-INFO->> Step 459310 run_train: loss = 5.4483  (0.162 sec)
18-06-05 02:32-INFO->> Step 459320 run_train: loss = 5.4187  (0.159 sec)
18-06-05 02:32-INFO->> Step 459330 run_train: loss = 5.3973  (0.176 sec)
18-06-05 02:32-INFO->> Step 459340 run_train: loss = 5.3753  (0.149 sec)
18-06-05 02:32-INFO->> Step 459350 run_train: loss = 5.4048  (0.152 sec)
18-06-05 02:32-INFO->> Step 459360 run_train: loss = 5.4859  (0.157 sec)
18-06-05 02:32-INFO->> Step 459370 run_train: loss = 5.4161  (0.143 sec)
18-06-05 02:32-INFO->> Step 459380 run_train: loss = 5.4302  (0.172 sec)
18-06-05 02:32-INFO->> Step 459390 run_train: loss = 5.4048  (0.211 sec)
18-06-05 02:32-INFO->> Step 459400 run_train: loss = 5.4130  (0.131 sec)
18-06-05 02:32-INFO->> Step 459410 run_train: loss = 5.3705  (0.160 sec)
18-06-05 02:33-INFO->> Step 459420 run_train: loss = 5.4014  (0.170 sec)
18-06-05 02:33-INFO->> Step 459430 run_train: loss = 5.3648  (0.145 sec)
18-06-05 02:33-INFO->> Step 459440 run_train: loss = 5.4215  (0.192 sec)
18-06-05 02:33-INFO->> Step 459450 run_train: loss = 5.3865  (0.162 sec)
18-06-05 02:33-INFO->> Step 459460 run_train: loss = 5.4273  (0.138 sec)
18-06-05 02:33-INFO->> Step 459470 run_train: loss = 5.4497  (0.118 sec)
18-06-05 02:33-INFO->> Step 459480 run_train: loss = 5.4584  (0.198 sec)
18-06-05 02:33-INFO->> Step 459490 run_train: loss = 5.4257  (0.149 sec)
18-06-05 02:33-INFO->> Step 459500 run_train: loss = 5.4585  (0.192 sec)
18-06-05 02:33-INFO->> Step 459510 run_train: loss = 5.4061  (0.151 sec)
18-06-05 02:33-INFO->> Step 459520 run_train: loss = 5.3808  (0.156 sec)
18-06-05 02:33-INFO->> Step 459530 run_train: loss = 5.4144  (0.162 sec)
18-06-05 02:33-INFO->> Step 459540 run_train: loss = 5.4042  (0.177 sec)
18-06-05 02:33-INFO->> Step 459550 run_train: loss = 5.4054  (0.161 sec)
18-06-05 02:33-INFO->> Step 459560 run_train: loss = 5.4431  (0.158 sec)
18-06-05 02:33-INFO->> Step 459570 run_train: loss = 5.4388  (0.167 sec)
18-06-05 02:33-INFO->> Step 459580 run_train: loss = 5.3824  (0.143 sec)
18-06-05 02:33-INFO->> Step 459590 run_train: loss = 5.4551  (0.160 sec)
18-06-05 02:33-INFO->> Step 459600 run_train: loss = 5.3969  (0.207 sec)
18-06-05 02:33-INFO->> Step 459610 run_train: loss = 5.4159  (0.148 sec)
18-06-05 02:33-INFO->> Step 459620 run_train: loss = 5.4596  (0.158 sec)
18-06-05 02:33-INFO->> Step 459630 run_train: loss = 5.3957  (0.171 sec)
18-06-05 02:33-INFO->> Step 459640 run_train: loss = 5.3946  (0.144 sec)
18-06-05 02:33-INFO->> Step 459650 run_train: loss = 5.3855  (0.146 sec)
18-06-05 02:33-INFO->> Step 459660 run_train: loss = 5.3428  (0.150 sec)
18-06-05 02:33-INFO->> Step 459670 run_train: loss = 5.3596  (0.194 sec)
18-06-05 02:33-INFO->> Step 459680 run_train: loss = 5.4418  (0.162 sec)
18-06-05 02:33-INFO->> Step 459690 run_train: loss = 5.4348  (0.187 sec)
18-06-05 02:33-INFO->> Step 459700 run_train: loss = 5.3868  (0.141 sec)
18-06-05 02:33-INFO->> Step 459710 run_train: loss = 5.4090  (0.183 sec)
18-06-05 02:33-INFO->> Step 459720 run_train: loss = 5.4562  (0.180 sec)
18-06-05 02:33-INFO->> Step 459730 run_train: loss = 5.4402  (0.141 sec)
18-06-05 02:33-INFO->> Step 459740 run_train: loss = 5.4144  (0.165 sec)
18-06-05 02:33-INFO->> Step 459750 run_train: loss = 5.4465  (0.183 sec)
18-06-05 02:33-INFO->> Step 459760 run_train: loss = 5.4446  (0.202 sec)
18-06-05 02:33-INFO->> Step 459770 run_train: loss = 5.3922  (0.123 sec)
18-06-05 02:33-INFO->> Step 459780 run_train: loss = 5.3468  (0.156 sec)
18-06-05 02:33-INFO->> Step 459790 run_train: loss = 5.4432  (0.175 sec)
18-06-05 02:34-INFO->> Step 459800 run_train: loss = 5.4108  (0.163 sec)
18-06-05 02:34-INFO->> Step 459810 run_train: loss = 5.3981  (0.145 sec)
18-06-05 02:34-INFO->> Step 459820 run_train: loss = 5.4175  (0.164 sec)
18-06-05 02:34-INFO->> Step 459830 run_train: loss = 5.4368  (0.158 sec)
18-06-05 02:34-INFO->> Step 459840 run_train: loss = 5.3767  (0.149 sec)
18-06-05 02:34-INFO->> Step 459850 run_train: loss = 5.4246  (0.194 sec)
18-06-05 02:34-INFO->> Step 459860 run_train: loss = 5.3991  (0.168 sec)
18-06-05 02:34-INFO->> Step 459870 run_train: loss = 5.3652  (0.185 sec)
18-06-05 02:34-INFO->> Step 459880 run_train: loss = 5.4127  (0.153 sec)
18-06-05 02:34-INFO->> Step 459890 run_train: loss = 5.3821  (0.154 sec)
18-06-05 02:34-INFO->> Step 459900 run_train: loss = 5.4169  (0.168 sec)
18-06-05 02:34-INFO->> Step 459910 run_train: loss = 5.3815  (0.174 sec)
18-06-05 02:34-INFO->> Step 459920 run_train: loss = 5.4531  (0.143 sec)
18-06-05 02:34-INFO->> Step 459930 run_train: loss = 5.4362  (0.196 sec)
18-06-05 02:34-INFO->> Step 459940 run_train: loss = 5.4544  (0.164 sec)
18-06-05 02:34-INFO->> Step 459950 run_train: loss = 5.4112  (0.171 sec)
18-06-05 02:34-INFO->> Step 459960 run_train: loss = 5.4084  (0.142 sec)
18-06-05 02:34-INFO->> Step 459970 run_train: loss = 5.4380  (0.146 sec)
18-06-05 02:34-INFO->> Step 459980 run_train: loss = 5.4037  (0.156 sec)
18-06-05 02:34-INFO->> Step 459990 run_train: loss = 5.3909  (0.168 sec)
18-06-05 02:34-INFO->> Step 460000 run_train: loss = 5.4216  (0.146 sec)
18-06-05 02:34-INFO->> 2018-06-05 02:34:33.086937 Saving in ckpt
18-06-05 02:34-INFO-Test Data Eval:
18-06-05 02:35-INFO-fpr95 = 0.1727384431455898 and auc = 0.9691786299775735
18-06-05 02:35-INFO->> Step 460010 run_train: loss = 5.4109  (0.151 sec)
18-06-05 02:35-INFO->> Step 460020 run_train: loss = 5.3898  (0.161 sec)
18-06-05 02:35-INFO->> Step 460030 run_train: loss = 5.3911  (0.149 sec)
18-06-05 02:35-INFO->> Step 460040 run_train: loss = 5.4186  (0.115 sec)
18-06-05 02:35-INFO->> Step 460050 run_train: loss = 5.3916  (0.176 sec)
18-06-05 02:35-INFO->> Step 460060 run_train: loss = 5.4296  (0.191 sec)
18-06-05 02:35-INFO->> Step 460070 run_train: loss = 5.4478  (0.187 sec)
18-06-05 02:35-INFO->> Step 460080 run_train: loss = 5.3905  (0.163 sec)
18-06-05 02:35-INFO->> Step 460090 run_train: loss = 5.4253  (0.185 sec)
18-06-05 02:35-INFO->> Step 460100 run_train: loss = 5.4505  (0.161 sec)
18-06-05 02:35-INFO->> Step 460110 run_train: loss = 5.4043  (0.155 sec)
18-06-05 02:35-INFO->> Step 460120 run_train: loss = 5.4094  (0.169 sec)
18-06-05 02:35-INFO->> Step 460130 run_train: loss = 5.4012  (0.146 sec)
18-06-05 02:35-INFO->> Step 460140 run_train: loss = 5.4147  (0.131 sec)
18-06-05 02:35-INFO->> Step 460150 run_train: loss = 5.4466  (0.172 sec)
18-06-05 02:35-INFO->> Step 460160 run_train: loss = 5.4122  (0.205 sec)
18-06-05 02:35-INFO->> Step 460170 run_train: loss = 5.4342  (0.128 sec)
18-06-05 02:35-INFO->> Step 460180 run_train: loss = 5.3738  (0.169 sec)
18-06-05 02:35-INFO->> Step 460190 run_train: loss = 5.4022  (0.160 sec)
18-06-05 02:35-INFO->> Step 460200 run_train: loss = 5.4813  (0.140 sec)
18-06-05 02:35-INFO->> Step 460210 run_train: loss = 5.4465  (0.172 sec)
18-06-05 02:35-INFO->> Step 460220 run_train: loss = 5.4531  (0.159 sec)
18-06-05 02:35-INFO->> Step 460230 run_train: loss = 5.4524  (0.157 sec)
18-06-05 02:35-INFO->> Step 460240 run_train: loss = 5.4258  (0.147 sec)
18-06-05 02:35-INFO->> Step 460250 run_train: loss = 5.4134  (0.184 sec)
18-06-05 02:35-INFO->> Step 460260 run_train: loss = 5.3906  (0.149 sec)
18-06-05 02:35-INFO->> Step 460270 run_train: loss = 5.4015  (0.148 sec)
18-06-05 02:35-INFO->> Step 460280 run_train: loss = 5.4320  (0.165 sec)
18-06-05 02:35-INFO->> Step 460290 run_train: loss = 5.3534  (0.156 sec)
18-06-05 02:36-INFO->> Step 460300 run_train: loss = 5.4011  (0.162 sec)
18-06-05 02:36-INFO->> Step 460310 run_train: loss = 5.3723  (0.152 sec)
18-06-05 02:36-INFO->> Step 460320 run_train: loss = 5.4006  (0.145 sec)
18-06-05 02:36-INFO->> Step 460330 run_train: loss = 5.3832  (0.143 sec)
18-06-05 02:36-INFO->> Step 460340 run_train: loss = 5.3713  (0.142 sec)
18-06-05 02:36-INFO->> Step 460350 run_train: loss = 5.4393  (0.176 sec)
18-06-05 02:36-INFO->> Step 460360 run_train: loss = 5.4547  (0.158 sec)
18-06-05 02:36-INFO->> Step 460370 run_train: loss = 5.4164  (0.153 sec)
18-06-05 02:36-INFO->> Step 460380 run_train: loss = 5.4689  (0.157 sec)
18-06-05 02:36-INFO->> Step 460390 run_train: loss = 5.3855  (0.161 sec)
18-06-05 02:36-INFO->> Step 460400 run_train: loss = 5.3531  (0.172 sec)
18-06-05 02:36-INFO->> Step 460410 run_train: loss = 5.4128  (0.156 sec)
18-06-05 02:36-INFO->> Step 460420 run_train: loss = 5.3724  (0.152 sec)
18-06-05 02:36-INFO->> Step 460430 run_train: loss = 5.3800  (0.169 sec)
18-06-05 02:36-INFO->> Step 460440 run_train: loss = 5.4676  (0.130 sec)
18-06-05 02:36-INFO->> Step 460450 run_train: loss = 5.3593  (0.174 sec)
18-06-05 02:36-INFO->> Step 460460 run_train: loss = 5.4275  (0.162 sec)
18-06-05 02:36-INFO->> Step 460470 run_train: loss = 5.3894  (0.128 sec)
18-06-05 02:36-INFO->> Step 460480 run_train: loss = 5.3920  (0.156 sec)
18-06-05 02:36-INFO->> Step 460490 run_train: loss = 5.4778  (0.151 sec)
18-06-05 02:36-INFO->> Step 460500 run_train: loss = 5.3916  (0.165 sec)
18-06-05 02:36-INFO->> Step 460510 run_train: loss = 5.4365  (0.151 sec)
18-06-05 02:36-INFO->> Step 460520 run_train: loss = 5.4622  (0.150 sec)
18-06-05 02:36-INFO->> Step 460530 run_train: loss = 5.3299  (0.152 sec)
18-06-05 02:36-INFO->> Step 460540 run_train: loss = 5.4336  (0.132 sec)
18-06-05 02:36-INFO->> Step 460550 run_train: loss = 5.4270  (0.116 sec)
18-06-05 02:36-INFO->> Step 460560 run_train: loss = 5.3951  (0.150 sec)
18-06-05 02:36-INFO->> Step 460570 run_train: loss = 5.3478  (0.166 sec)
18-06-05 02:36-INFO->> Step 460580 run_train: loss = 5.4121  (0.157 sec)
18-06-05 02:36-INFO->> Step 460590 run_train: loss = 5.4340  (0.187 sec)
18-06-05 02:36-INFO->> Step 460600 run_train: loss = 5.4184  (0.157 sec)
18-06-05 02:36-INFO->> Step 460610 run_train: loss = 5.4191  (0.157 sec)
18-06-05 02:36-INFO->> Step 460620 run_train: loss = 5.4197  (0.184 sec)
18-06-05 02:36-INFO->> Step 460630 run_train: loss = 5.3643  (0.139 sec)
18-06-05 02:36-INFO->> Step 460640 run_train: loss = 5.3553  (0.157 sec)
18-06-05 02:36-INFO->> Step 460650 run_train: loss = 5.4412  (0.198 sec)
18-06-05 02:36-INFO->> Step 460660 run_train: loss = 5.4441  (0.157 sec)
18-06-05 02:36-INFO->> Step 460670 run_train: loss = 5.3743  (0.167 sec)
18-06-05 02:37-INFO->> Step 460680 run_train: loss = 5.4080  (0.147 sec)
18-06-05 02:37-INFO->> Step 460690 run_train: loss = 5.5142  (0.156 sec)
18-06-05 02:37-INFO->> Step 460700 run_train: loss = 5.4162  (0.158 sec)
18-06-05 02:37-INFO->> Step 460710 run_train: loss = 5.3569  (0.147 sec)
18-06-05 02:37-INFO->> Step 460720 run_train: loss = 5.3911  (0.170 sec)
18-06-05 02:37-INFO->> Step 460730 run_train: loss = 5.3653  (0.113 sec)
18-06-05 02:37-INFO->> Step 460740 run_train: loss = 5.3917  (0.150 sec)
18-06-05 02:37-INFO->> Step 460750 run_train: loss = 5.3932  (0.141 sec)
18-06-05 02:37-INFO->> Step 460760 run_train: loss = 5.4365  (0.162 sec)
18-06-05 02:37-INFO->> Step 460770 run_train: loss = 5.3747  (0.165 sec)
18-06-05 02:37-INFO->> Step 460780 run_train: loss = 5.4383  (0.172 sec)
18-06-05 02:37-INFO->> Step 460790 run_train: loss = 5.3482  (0.167 sec)
18-06-05 02:37-INFO->> Step 460800 run_train: loss = 5.3779  (0.150 sec)
18-06-05 02:37-INFO->> Step 460810 run_train: loss = 5.4343  (0.162 sec)
18-06-05 02:37-INFO->> Step 460820 run_train: loss = 5.4597  (0.155 sec)
18-06-05 02:37-INFO->> Step 460830 run_train: loss = 5.4120  (0.140 sec)
18-06-05 02:37-INFO->> Step 460840 run_train: loss = 5.4446  (0.172 sec)
18-06-05 02:37-INFO->> Step 460850 run_train: loss = 5.4127  (0.150 sec)
18-06-05 02:37-INFO->> Step 460860 run_train: loss = 5.3983  (0.172 sec)
18-06-05 02:37-INFO->> Step 460870 run_train: loss = 5.3860  (0.148 sec)
18-06-05 02:37-INFO->> Step 460880 run_train: loss = 5.3912  (0.152 sec)
18-06-05 02:37-INFO->> Step 460890 run_train: loss = 5.3630  (0.173 sec)
18-06-05 02:37-INFO->> Step 460900 run_train: loss = 5.3638  (0.153 sec)
18-06-05 02:37-INFO->> Step 460910 run_train: loss = 5.4174  (0.153 sec)
18-06-05 02:37-INFO->> Step 460920 run_train: loss = 5.4346  (0.158 sec)
18-06-05 02:37-INFO->> Step 460930 run_train: loss = 5.3119  (0.168 sec)
18-06-05 02:37-INFO->> Step 460940 run_train: loss = 5.4150  (0.163 sec)
18-06-05 02:37-INFO->> Step 460950 run_train: loss = 5.4189  (0.148 sec)
18-06-05 02:37-INFO->> Step 460960 run_train: loss = 5.4390  (0.159 sec)
18-06-05 02:37-INFO->> Step 460970 run_train: loss = 5.3463  (0.142 sec)
18-06-05 02:37-INFO->> Step 460980 run_train: loss = 5.3813  (0.167 sec)
18-06-05 02:37-INFO->> Step 460990 run_train: loss = 5.3503  (0.121 sec)
18-06-05 02:37-INFO->> Step 461000 run_train: loss = 5.4084  (0.182 sec)
18-06-05 02:37-INFO->> 2018-06-05 02:37:51.850526 Saving in ckpt
18-06-05 02:37-INFO-Test Data Eval:
18-06-05 02:38-INFO-fpr95 = 0.17473100425079702 and auc = 0.969135651448229
18-06-05 02:38-INFO->> Step 461010 run_train: loss = 5.3949  (0.135 sec)
18-06-05 02:38-INFO->> Step 461020 run_train: loss = 5.4492  (0.177 sec)
18-06-05 02:38-INFO->> Step 461030 run_train: loss = 5.3946  (0.153 sec)
18-06-05 02:38-INFO->> Step 461040 run_train: loss = 5.3942  (0.159 sec)
18-06-05 02:38-INFO->> Step 461050 run_train: loss = 5.3888  (0.152 sec)
18-06-05 02:38-INFO->> Step 461060 run_train: loss = 5.4225  (0.158 sec)
18-06-05 02:38-INFO->> Step 461070 run_train: loss = 5.4181  (0.198 sec)
18-06-05 02:38-INFO->> Step 461080 run_train: loss = 5.4013  (0.163 sec)
18-06-05 02:38-INFO->> Step 461090 run_train: loss = 5.4601  (0.157 sec)
18-06-05 02:38-INFO->> Step 461100 run_train: loss = 5.4313  (0.173 sec)
18-06-05 02:38-INFO->> Step 461110 run_train: loss = 5.4422  (0.150 sec)
18-06-05 02:38-INFO->> Step 461120 run_train: loss = 5.4634  (0.157 sec)
18-06-05 02:38-INFO->> Step 461130 run_train: loss = 5.3834  (0.155 sec)
18-06-05 02:38-INFO->> Step 461140 run_train: loss = 5.4696  (0.146 sec)
18-06-05 02:38-INFO->> Step 461150 run_train: loss = 5.4125  (0.154 sec)
18-06-05 02:38-INFO->> Step 461160 run_train: loss = 5.4539  (0.140 sec)
18-06-05 02:38-INFO->> Step 461170 run_train: loss = 5.4452  (0.182 sec)
18-06-05 02:39-INFO->> Step 461180 run_train: loss = 5.4246  (0.148 sec)
18-06-05 02:39-INFO->> Step 461190 run_train: loss = 5.3957  (0.137 sec)
18-06-05 02:39-INFO->> Step 461200 run_train: loss = 5.3887  (0.180 sec)
18-06-05 02:39-INFO->> Step 461210 run_train: loss = 5.5126  (0.169 sec)
18-06-05 02:39-INFO->> Step 461220 run_train: loss = 5.3728  (0.151 sec)
18-06-05 02:39-INFO->> Step 461230 run_train: loss = 5.3914  (0.146 sec)
18-06-05 02:39-INFO->> Step 461240 run_train: loss = 5.4294  (0.156 sec)
18-06-05 02:39-INFO->> Step 461250 run_train: loss = 5.4238  (0.145 sec)
18-06-05 02:39-INFO->> Step 461260 run_train: loss = 5.3594  (0.170 sec)
18-06-05 02:39-INFO->> Step 461270 run_train: loss = 5.3688  (0.164 sec)
18-06-05 02:39-INFO->> Step 461280 run_train: loss = 5.3987  (0.159 sec)
18-06-05 02:39-INFO->> Step 461290 run_train: loss = 5.4053  (0.180 sec)
18-06-05 02:39-INFO->> Step 461300 run_train: loss = 5.3757  (0.203 sec)
18-06-05 02:39-INFO->> Step 461310 run_train: loss = 5.4316  (0.191 sec)
18-06-05 02:39-INFO->> Step 461320 run_train: loss = 5.4237  (0.158 sec)
18-06-05 02:39-INFO->> Step 461330 run_train: loss = 5.4086  (0.163 sec)
18-06-05 02:39-INFO->> Step 461340 run_train: loss = 5.4323  (0.168 sec)
18-06-05 02:39-INFO->> Step 461350 run_train: loss = 5.4333  (0.143 sec)
18-06-05 02:39-INFO->> Step 461360 run_train: loss = 5.3806  (0.159 sec)
18-06-05 02:39-INFO->> Step 461370 run_train: loss = 5.3891  (0.138 sec)
18-06-05 02:39-INFO->> Step 461380 run_train: loss = 5.3942  (0.190 sec)
18-06-05 02:39-INFO->> Step 461390 run_train: loss = 5.3968  (0.168 sec)
18-06-05 02:39-INFO->> Step 461400 run_train: loss = 5.4080  (0.156 sec)
18-06-05 02:39-INFO->> Step 461410 run_train: loss = 5.4272  (0.167 sec)
18-06-05 02:39-INFO->> Step 461420 run_train: loss = 5.4112  (0.175 sec)
18-06-05 02:39-INFO->> Step 461430 run_train: loss = 5.4088  (0.117 sec)
18-06-05 02:39-INFO->> Step 461440 run_train: loss = 5.4077  (0.157 sec)
18-06-05 02:39-INFO->> Step 461450 run_train: loss = 5.3919  (0.142 sec)
18-06-05 02:39-INFO->> Step 461460 run_train: loss = 5.4177  (0.121 sec)
18-06-05 02:39-INFO->> Step 461470 run_train: loss = 5.3503  (0.150 sec)
18-06-05 02:39-INFO->> Step 461480 run_train: loss = 5.4067  (0.157 sec)
18-06-05 02:39-INFO->> Step 461490 run_train: loss = 5.4214  (0.164 sec)
18-06-05 02:39-INFO->> Step 461500 run_train: loss = 5.3720  (0.180 sec)
18-06-05 02:39-INFO->> Step 461510 run_train: loss = 5.4120  (0.140 sec)
18-06-05 02:39-INFO->> Step 461520 run_train: loss = 5.4213  (0.150 sec)
18-06-05 02:39-INFO->> Step 461530 run_train: loss = 5.4005  (0.144 sec)
18-06-05 02:39-INFO->> Step 461540 run_train: loss = 5.4592  (0.179 sec)
18-06-05 02:39-INFO->> Step 461550 run_train: loss = 5.3735  (0.182 sec)
18-06-05 02:40-INFO->> Step 461560 run_train: loss = 5.3972  (0.136 sec)
18-06-05 02:40-INFO->> Step 461570 run_train: loss = 5.4179  (0.135 sec)
18-06-05 02:40-INFO->> Step 461580 run_train: loss = 5.2981  (0.168 sec)
18-06-05 02:40-INFO->> Step 461590 run_train: loss = 5.4514  (0.161 sec)
18-06-05 02:40-INFO->> Step 461600 run_train: loss = 5.3723  (0.144 sec)
18-06-05 02:40-INFO->> Step 461610 run_train: loss = 5.3887  (0.156 sec)
18-06-05 02:40-INFO->> Step 461620 run_train: loss = 5.4723  (0.150 sec)
18-06-05 02:40-INFO->> Step 461630 run_train: loss = 5.4126  (0.156 sec)
18-06-05 02:40-INFO->> Step 461640 run_train: loss = 5.3737  (0.140 sec)
18-06-05 02:40-INFO->> Step 461650 run_train: loss = 5.3607  (0.126 sec)
18-06-05 02:40-INFO->> Step 461660 run_train: loss = 5.4162  (0.172 sec)
18-06-05 02:40-INFO->> Step 461670 run_train: loss = 5.4004  (0.184 sec)
18-06-05 02:40-INFO->> Step 461680 run_train: loss = 5.4027  (0.166 sec)
18-06-05 02:40-INFO->> Step 461690 run_train: loss = 5.4294  (0.146 sec)
18-06-05 02:40-INFO->> Step 461700 run_train: loss = 5.4162  (0.162 sec)
18-06-05 02:40-INFO->> Step 461710 run_train: loss = 5.4037  (0.160 sec)
18-06-05 02:40-INFO->> Step 461720 run_train: loss = 5.4263  (0.153 sec)
18-06-05 02:40-INFO->> Step 461730 run_train: loss = 5.4792  (0.181 sec)
18-06-05 02:40-INFO->> Step 461740 run_train: loss = 5.4072  (0.135 sec)
18-06-05 02:40-INFO->> Step 461750 run_train: loss = 5.4114  (0.173 sec)
18-06-05 02:40-INFO->> Step 461760 run_train: loss = 5.4245  (0.134 sec)
18-06-05 02:40-INFO->> Step 461770 run_train: loss = 5.4432  (0.158 sec)
18-06-05 02:40-INFO->> Step 461780 run_train: loss = 5.3657  (0.134 sec)
18-06-05 02:40-INFO->> Step 461790 run_train: loss = 5.4064  (0.146 sec)
18-06-05 02:40-INFO->> Step 461800 run_train: loss = 5.4019  (0.181 sec)
18-06-05 02:40-INFO->> Step 461810 run_train: loss = 5.3793  (0.161 sec)
18-06-05 02:40-INFO->> Step 461820 run_train: loss = 5.4735  (0.162 sec)
18-06-05 02:40-INFO->> Step 461830 run_train: loss = 5.4040  (0.196 sec)
18-06-05 02:40-INFO->> Step 461840 run_train: loss = 5.4109  (0.157 sec)
18-06-05 02:40-INFO->> Step 461850 run_train: loss = 5.3739  (0.164 sec)
18-06-05 02:40-INFO->> Step 461860 run_train: loss = 5.3593  (0.185 sec)
18-06-05 02:40-INFO->> Step 461870 run_train: loss = 5.3682  (0.158 sec)
18-06-05 02:40-INFO->> Step 461880 run_train: loss = 5.4273  (0.155 sec)
18-06-05 02:40-INFO->> Step 461890 run_train: loss = 5.4493  (0.167 sec)
18-06-05 02:40-INFO->> Step 461900 run_train: loss = 5.4860  (0.152 sec)
18-06-05 02:40-INFO->> Step 461910 run_train: loss = 5.4616  (0.163 sec)
18-06-05 02:40-INFO->> Step 461920 run_train: loss = 5.3715  (0.171 sec)
18-06-05 02:40-INFO->> Step 461930 run_train: loss = 5.3840  (0.137 sec)
18-06-05 02:41-INFO->> Step 461940 run_train: loss = 5.4489  (0.165 sec)
18-06-05 02:41-INFO->> Step 461950 run_train: loss = 5.4067  (0.144 sec)
18-06-05 02:41-INFO->> Step 461960 run_train: loss = 5.4034  (0.167 sec)
18-06-05 02:41-INFO->> Step 461970 run_train: loss = 5.4130  (0.155 sec)
18-06-05 02:41-INFO->> Step 461980 run_train: loss = 5.3796  (0.154 sec)
18-06-05 02:41-INFO->> Step 461990 run_train: loss = 5.3763  (0.157 sec)
18-06-05 02:41-INFO->> Step 462000 run_train: loss = 5.3903  (0.151 sec)
18-06-05 02:41-INFO->> 2018-06-05 02:41:10.333377 Saving in ckpt
18-06-05 02:41-INFO-Test Data Eval:
18-06-05 02:41-INFO-fpr95 = 0.17456495749202974 and auc = 0.9690487973844027
18-06-05 02:41-INFO->> Step 462010 run_train: loss = 5.4122  (0.172 sec)
18-06-05 02:41-INFO->> Step 462020 run_train: loss = 5.3859  (0.130 sec)
18-06-05 02:41-INFO->> Step 462030 run_train: loss = 5.3755  (0.174 sec)
18-06-05 02:41-INFO->> Step 462040 run_train: loss = 5.3700  (0.174 sec)
18-06-05 02:41-INFO->> Step 462050 run_train: loss = 5.3757  (0.185 sec)
18-06-05 02:41-INFO->> Step 462060 run_train: loss = 5.3315  (0.140 sec)
18-06-05 02:42-INFO->> Step 462070 run_train: loss = 5.3841  (0.172 sec)
18-06-05 02:42-INFO->> Step 462080 run_train: loss = 5.3833  (0.152 sec)
18-06-05 02:42-INFO->> Step 462090 run_train: loss = 5.4310  (0.182 sec)
18-06-05 02:42-INFO->> Step 462100 run_train: loss = 5.3793  (0.163 sec)
18-06-05 02:42-INFO->> Step 462110 run_train: loss = 5.3940  (0.144 sec)
18-06-05 02:42-INFO->> Step 462120 run_train: loss = 5.3871  (0.139 sec)
18-06-05 02:42-INFO->> Step 462130 run_train: loss = 5.4034  (0.174 sec)
18-06-05 02:42-INFO->> Step 462140 run_train: loss = 5.4456  (0.200 sec)
18-06-05 02:42-INFO->> Step 462150 run_train: loss = 5.4149  (0.159 sec)
18-06-05 02:42-INFO->> Step 462160 run_train: loss = 5.3735  (0.153 sec)
18-06-05 02:42-INFO->> Step 462170 run_train: loss = 5.4270  (0.153 sec)
18-06-05 02:42-INFO->> Step 462180 run_train: loss = 5.3399  (0.168 sec)
18-06-05 02:42-INFO->> Step 462190 run_train: loss = 5.3851  (0.146 sec)
18-06-05 02:42-INFO->> Step 462200 run_train: loss = 5.3738  (0.164 sec)
18-06-05 02:42-INFO->> Step 462210 run_train: loss = 5.4281  (0.176 sec)
18-06-05 02:42-INFO->> Step 462220 run_train: loss = 5.3881  (0.173 sec)
18-06-05 02:42-INFO->> Step 462230 run_train: loss = 5.3783  (0.173 sec)
18-06-05 02:42-INFO->> Step 462240 run_train: loss = 5.4502  (0.171 sec)
18-06-05 02:42-INFO->> Step 462250 run_train: loss = 5.4278  (0.166 sec)
18-06-05 02:42-INFO->> Step 462260 run_train: loss = 5.4180  (0.157 sec)
18-06-05 02:42-INFO->> Step 462270 run_train: loss = 5.4407  (0.145 sec)
18-06-05 02:42-INFO->> Step 462280 run_train: loss = 5.4232  (0.148 sec)
18-06-05 02:42-INFO->> Step 462290 run_train: loss = 5.3513  (0.161 sec)
18-06-05 02:42-INFO->> Step 462300 run_train: loss = 5.3780  (0.171 sec)
18-06-05 02:42-INFO->> Step 462310 run_train: loss = 5.4373  (0.126 sec)
18-06-05 02:42-INFO->> Step 462320 run_train: loss = 5.4190  (0.187 sec)
18-06-05 02:42-INFO->> Step 462330 run_train: loss = 5.4161  (0.156 sec)
18-06-05 02:42-INFO->> Step 462340 run_train: loss = 5.4725  (0.172 sec)
18-06-05 02:42-INFO->> Step 462350 run_train: loss = 5.4315  (0.145 sec)
18-06-05 02:42-INFO->> Step 462360 run_train: loss = 5.4787  (0.168 sec)
18-06-05 02:42-INFO->> Step 462370 run_train: loss = 5.4530  (0.151 sec)
18-06-05 02:42-INFO->> Step 462380 run_train: loss = 5.3836  (0.148 sec)
18-06-05 02:42-INFO->> Step 462390 run_train: loss = 5.4700  (0.151 sec)
18-06-05 02:42-INFO->> Step 462400 run_train: loss = 5.3719  (0.175 sec)
18-06-05 02:42-INFO->> Step 462410 run_train: loss = 5.4057  (0.159 sec)
18-06-05 02:42-INFO->> Step 462420 run_train: loss = 5.3975  (0.139 sec)
18-06-05 02:42-INFO->> Step 462430 run_train: loss = 5.4492  (0.183 sec)
18-06-05 02:43-INFO->> Step 462440 run_train: loss = 5.4228  (0.158 sec)
18-06-05 02:43-INFO->> Step 462450 run_train: loss = 5.3924  (0.147 sec)
18-06-05 02:43-INFO->> Step 462460 run_train: loss = 5.4613  (0.170 sec)
18-06-05 02:43-INFO->> Step 462470 run_train: loss = 5.4719  (0.156 sec)
18-06-05 02:43-INFO->> Step 462480 run_train: loss = 5.3901  (0.165 sec)
18-06-05 02:43-INFO->> Step 462490 run_train: loss = 5.4164  (0.190 sec)
18-06-05 02:43-INFO->> Step 462500 run_train: loss = 5.4117  (0.164 sec)
18-06-05 02:43-INFO->> Step 462510 run_train: loss = 5.3818  (0.156 sec)
18-06-05 02:43-INFO->> Step 462520 run_train: loss = 5.4334  (0.143 sec)
18-06-05 02:43-INFO->> Step 462530 run_train: loss = 5.4120  (0.166 sec)
18-06-05 02:43-INFO->> Step 462540 run_train: loss = 5.3760  (0.149 sec)
18-06-05 02:43-INFO->> Step 462550 run_train: loss = 5.4249  (0.149 sec)
18-06-05 02:43-INFO->> Step 462560 run_train: loss = 5.4076  (0.166 sec)
18-06-05 02:43-INFO->> Step 462570 run_train: loss = 5.4662  (0.175 sec)
18-06-05 02:43-INFO->> Step 462580 run_train: loss = 5.3443  (0.164 sec)
18-06-05 02:43-INFO->> Step 462590 run_train: loss = 5.4291  (0.157 sec)
18-06-05 02:43-INFO->> Step 462600 run_train: loss = 5.4039  (0.159 sec)
18-06-05 02:43-INFO->> Step 462610 run_train: loss = 5.4260  (0.161 sec)
18-06-05 02:43-INFO->> Step 462620 run_train: loss = 5.4170  (0.160 sec)
18-06-05 02:43-INFO->> Step 462630 run_train: loss = 5.4605  (0.147 sec)
18-06-05 02:43-INFO->> Step 462640 run_train: loss = 5.4017  (0.145 sec)
18-06-05 02:43-INFO->> Step 462650 run_train: loss = 5.4252  (0.161 sec)
18-06-05 02:43-INFO->> Step 462660 run_train: loss = 5.3584  (0.173 sec)
18-06-05 02:43-INFO->> Step 462670 run_train: loss = 5.3788  (0.170 sec)
18-06-05 02:43-INFO->> Step 462680 run_train: loss = 5.4078  (0.187 sec)
18-06-05 02:43-INFO->> Step 462690 run_train: loss = 5.4019  (0.127 sec)
18-06-05 02:43-INFO->> Step 462700 run_train: loss = 5.3883  (0.143 sec)
18-06-05 02:43-INFO->> Step 462710 run_train: loss = 5.3948  (0.126 sec)
18-06-05 02:43-INFO->> Step 462720 run_train: loss = 5.3669  (0.134 sec)
18-06-05 02:43-INFO->> Step 462730 run_train: loss = 5.4191  (0.126 sec)
18-06-05 02:43-INFO->> Step 462740 run_train: loss = 5.3866  (0.173 sec)
18-06-05 02:43-INFO->> Step 462750 run_train: loss = 5.4220  (0.152 sec)
18-06-05 02:43-INFO->> Step 462760 run_train: loss = 5.3628  (0.159 sec)
18-06-05 02:43-INFO->> Step 462770 run_train: loss = 5.4140  (0.166 sec)
18-06-05 02:43-INFO->> Step 462780 run_train: loss = 5.4457  (0.127 sec)
18-06-05 02:43-INFO->> Step 462790 run_train: loss = 5.3477  (0.154 sec)
18-06-05 02:43-INFO->> Step 462800 run_train: loss = 5.3491  (0.205 sec)
18-06-05 02:43-INFO->> Step 462810 run_train: loss = 5.4085  (0.140 sec)
18-06-05 02:44-INFO->> Step 462820 run_train: loss = 5.3807  (0.153 sec)
18-06-05 02:44-INFO->> Step 462830 run_train: loss = 5.4170  (0.172 sec)
18-06-05 02:44-INFO->> Step 462840 run_train: loss = 5.4438  (0.148 sec)
18-06-05 02:44-INFO->> Step 462850 run_train: loss = 5.3510  (0.183 sec)
18-06-05 02:44-INFO->> Step 462860 run_train: loss = 5.3788  (0.177 sec)
18-06-05 02:44-INFO->> Step 462870 run_train: loss = 5.4081  (0.167 sec)
18-06-05 02:44-INFO->> Step 462880 run_train: loss = 5.4407  (0.175 sec)
18-06-05 02:44-INFO->> Step 462890 run_train: loss = 5.4332  (0.128 sec)
18-06-05 02:44-INFO->> Step 462900 run_train: loss = 5.4046  (0.166 sec)
18-06-05 02:44-INFO->> Step 462910 run_train: loss = 5.4404  (0.171 sec)
18-06-05 02:44-INFO->> Step 462920 run_train: loss = 5.4455  (0.188 sec)
18-06-05 02:44-INFO->> Step 462930 run_train: loss = 5.3780  (0.121 sec)
18-06-05 02:44-INFO->> Step 462940 run_train: loss = 5.3839  (0.169 sec)
18-06-05 02:44-INFO->> Step 462950 run_train: loss = 5.4290  (0.190 sec)
18-06-05 02:44-INFO->> Step 462960 run_train: loss = 5.4357  (0.159 sec)
18-06-05 02:44-INFO->> Step 462970 run_train: loss = 5.3950  (0.176 sec)
18-06-05 02:44-INFO->> Step 462980 run_train: loss = 5.3994  (0.153 sec)
18-06-05 02:44-INFO->> Step 462990 run_train: loss = 5.3967  (0.156 sec)
18-06-05 02:44-INFO->> Step 463000 run_train: loss = 5.3624  (0.139 sec)
18-06-05 02:44-INFO->> 2018-06-05 02:44:28.885278 Saving in ckpt
18-06-05 02:44-INFO-Test Data Eval:
18-06-05 02:45-INFO-fpr95 = 0.17747077577045697 and auc = 0.968967029233209
18-06-05 02:45-INFO->> Step 463010 run_train: loss = 5.3827  (0.132 sec)
18-06-05 02:45-INFO->> Step 463020 run_train: loss = 5.4114  (0.149 sec)
18-06-05 02:45-INFO->> Step 463030 run_train: loss = 5.4118  (0.134 sec)
18-06-05 02:45-INFO->> Step 463040 run_train: loss = 5.4867  (0.121 sec)
18-06-05 02:45-INFO->> Step 463050 run_train: loss = 5.4541  (0.131 sec)
18-06-05 02:45-INFO->> Step 463060 run_train: loss = 5.4882  (0.180 sec)
18-06-05 02:45-INFO->> Step 463070 run_train: loss = 5.4075  (0.156 sec)
18-06-05 02:45-INFO->> Step 463080 run_train: loss = 5.4778  (0.158 sec)
18-06-05 02:45-INFO->> Step 463090 run_train: loss = 5.4210  (0.122 sec)
18-06-05 02:45-INFO->> Step 463100 run_train: loss = 5.4549  (0.184 sec)
18-06-05 02:45-INFO->> Step 463110 run_train: loss = 5.4684  (0.168 sec)
18-06-05 02:45-INFO->> Step 463120 run_train: loss = 5.3862  (0.128 sec)
18-06-05 02:45-INFO->> Step 463130 run_train: loss = 5.3055  (0.128 sec)
18-06-05 02:45-INFO->> Step 463140 run_train: loss = 5.3896  (0.172 sec)
18-06-05 02:45-INFO->> Step 463150 run_train: loss = 5.3639  (0.165 sec)
18-06-05 02:45-INFO->> Step 463160 run_train: loss = 5.4189  (0.173 sec)
18-06-05 02:45-INFO->> Step 463170 run_train: loss = 5.4247  (0.123 sec)
18-06-05 02:45-INFO->> Step 463180 run_train: loss = 5.4154  (0.157 sec)
18-06-05 02:45-INFO->> Step 463190 run_train: loss = 5.4609  (0.174 sec)
18-06-05 02:45-INFO->> Step 463200 run_train: loss = 5.3728  (0.154 sec)
18-06-05 02:45-INFO->> Step 463210 run_train: loss = 5.3971  (0.161 sec)
18-06-05 02:45-INFO->> Step 463220 run_train: loss = 5.4347  (0.152 sec)
18-06-05 02:45-INFO->> Step 463230 run_train: loss = 5.4468  (0.129 sec)
18-06-05 02:45-INFO->> Step 463240 run_train: loss = 5.3855  (0.152 sec)
18-06-05 02:45-INFO->> Step 463250 run_train: loss = 5.3833  (0.163 sec)
18-06-05 02:45-INFO->> Step 463260 run_train: loss = 5.3675  (0.157 sec)
18-06-05 02:45-INFO->> Step 463270 run_train: loss = 5.4179  (0.186 sec)
18-06-05 02:45-INFO->> Step 463280 run_train: loss = 5.3829  (0.176 sec)
18-06-05 02:45-INFO->> Step 463290 run_train: loss = 5.5465  (0.157 sec)
18-06-05 02:45-INFO->> Step 463300 run_train: loss = 5.4016  (0.150 sec)
18-06-05 02:45-INFO->> Step 463310 run_train: loss = 5.3767  (0.173 sec)
18-06-05 02:45-INFO->> Step 463320 run_train: loss = 5.4101  (0.167 sec)
18-06-05 02:46-INFO->> Step 463330 run_train: loss = 5.3803  (0.152 sec)
18-06-05 02:46-INFO->> Step 463340 run_train: loss = 5.3647  (0.220 sec)
18-06-05 02:46-INFO->> Step 463350 run_train: loss = 5.4344  (0.145 sec)
18-06-05 02:46-INFO->> Step 463360 run_train: loss = 5.4068  (0.154 sec)
18-06-05 02:46-INFO->> Step 463370 run_train: loss = 5.4212  (0.125 sec)
18-06-05 02:46-INFO->> Step 463380 run_train: loss = 5.3827  (0.173 sec)
18-06-05 02:46-INFO->> Step 463390 run_train: loss = 5.4321  (0.185 sec)
18-06-05 02:46-INFO->> Step 463400 run_train: loss = 5.3697  (0.164 sec)
18-06-05 02:46-INFO->> Step 463410 run_train: loss = 5.3846  (0.146 sec)
18-06-05 02:46-INFO->> Step 463420 run_train: loss = 5.3956  (0.179 sec)
18-06-05 02:46-INFO->> Step 463430 run_train: loss = 5.4554  (0.161 sec)
18-06-05 02:46-INFO->> Step 463440 run_train: loss = 5.3855  (0.144 sec)
18-06-05 02:46-INFO->> Step 463450 run_train: loss = 5.3729  (0.167 sec)
18-06-05 02:46-INFO->> Step 463460 run_train: loss = 5.3679  (0.165 sec)
18-06-05 02:46-INFO->> Step 463470 run_train: loss = 5.3790  (0.184 sec)
18-06-05 02:46-INFO->> Step 463480 run_train: loss = 5.3741  (0.147 sec)
18-06-05 02:46-INFO->> Step 463490 run_train: loss = 5.4258  (0.123 sec)
18-06-05 02:46-INFO->> Step 463500 run_train: loss = 5.3712  (0.135 sec)
18-06-05 02:46-INFO->> Step 463510 run_train: loss = 5.4112  (0.142 sec)
18-06-05 02:46-INFO->> Step 463520 run_train: loss = 5.3980  (0.141 sec)
18-06-05 02:46-INFO->> Step 463530 run_train: loss = 5.3919  (0.153 sec)
18-06-05 02:46-INFO->> Step 463540 run_train: loss = 5.4418  (0.195 sec)
18-06-05 02:46-INFO->> Step 463550 run_train: loss = 5.3711  (0.178 sec)
18-06-05 02:46-INFO->> Step 463560 run_train: loss = 5.3967  (0.209 sec)
18-06-05 02:46-INFO->> Step 463570 run_train: loss = 5.4847  (0.162 sec)
18-06-05 02:46-INFO->> Step 463580 run_train: loss = 5.4381  (0.144 sec)
18-06-05 02:46-INFO->> Step 463590 run_train: loss = 5.4658  (0.160 sec)
18-06-05 02:46-INFO->> Step 463600 run_train: loss = 5.4974  (0.161 sec)
18-06-05 02:46-INFO->> Step 463610 run_train: loss = 5.3602  (0.167 sec)
18-06-05 02:46-INFO->> Step 463620 run_train: loss = 5.3749  (0.188 sec)
18-06-05 02:46-INFO->> Step 463630 run_train: loss = 5.4031  (0.157 sec)
18-06-05 02:46-INFO->> Step 463640 run_train: loss = 5.4457  (0.137 sec)
18-06-05 02:46-INFO->> Step 463650 run_train: loss = 5.4190  (0.193 sec)
18-06-05 02:46-INFO->> Step 463660 run_train: loss = 5.3754  (0.129 sec)
18-06-05 02:46-INFO->> Step 463670 run_train: loss = 5.4274  (0.162 sec)
18-06-05 02:46-INFO->> Step 463680 run_train: loss = 5.3950  (0.151 sec)
18-06-05 02:46-INFO->> Step 463690 run_train: loss = 5.4262  (0.169 sec)
18-06-05 02:47-INFO->> Step 463700 run_train: loss = 5.4124  (0.136 sec)
18-06-05 02:47-INFO->> Step 463710 run_train: loss = 5.3879  (0.144 sec)
18-06-05 02:47-INFO->> Step 463720 run_train: loss = 5.4278  (0.206 sec)
18-06-05 02:47-INFO->> Step 463730 run_train: loss = 5.3751  (0.155 sec)
18-06-05 02:47-INFO->> Step 463740 run_train: loss = 5.3542  (0.188 sec)
18-06-05 02:47-INFO->> Step 463750 run_train: loss = 5.4119  (0.155 sec)
18-06-05 02:47-INFO->> Step 463760 run_train: loss = 5.4438  (0.181 sec)
18-06-05 02:47-INFO->> Step 463770 run_train: loss = 5.4041  (0.160 sec)
18-06-05 02:47-INFO->> Step 463780 run_train: loss = 5.4509  (0.172 sec)
18-06-05 02:47-INFO->> Step 463790 run_train: loss = 5.3994  (0.144 sec)
18-06-05 02:47-INFO->> Step 463800 run_train: loss = 5.3885  (0.137 sec)
18-06-05 02:47-INFO->> Step 463810 run_train: loss = 5.4058  (0.191 sec)
18-06-05 02:47-INFO->> Step 463820 run_train: loss = 5.3445  (0.174 sec)
18-06-05 02:47-INFO->> Step 463830 run_train: loss = 5.4750  (0.121 sec)
18-06-05 02:47-INFO->> Step 463840 run_train: loss = 5.4812  (0.147 sec)
18-06-05 02:47-INFO->> Step 463850 run_train: loss = 5.3991  (0.171 sec)
18-06-05 02:47-INFO->> Step 463860 run_train: loss = 5.4062  (0.174 sec)
18-06-05 02:47-INFO->> Step 463870 run_train: loss = 5.4284  (0.164 sec)
18-06-05 02:47-INFO->> Step 463880 run_train: loss = 5.4739  (0.165 sec)
18-06-05 02:47-INFO->> Step 463890 run_train: loss = 5.3946  (0.170 sec)
18-06-05 02:47-INFO->> Step 463900 run_train: loss = 5.3850  (0.142 sec)
18-06-05 02:47-INFO->> Step 463910 run_train: loss = 5.4450  (0.192 sec)
18-06-05 02:47-INFO->> Step 463920 run_train: loss = 5.4902  (0.166 sec)
18-06-05 02:47-INFO->> Step 463930 run_train: loss = 5.3474  (0.183 sec)
18-06-05 02:47-INFO->> Step 463940 run_train: loss = 5.3774  (0.180 sec)
18-06-05 02:47-INFO->> Step 463950 run_train: loss = 5.4011  (0.124 sec)
18-06-05 02:47-INFO->> Step 463960 run_train: loss = 5.4659  (0.151 sec)
18-06-05 02:47-INFO->> Step 463970 run_train: loss = 5.4800  (0.171 sec)
18-06-05 02:47-INFO->> Step 463980 run_train: loss = 5.4169  (0.173 sec)
18-06-05 02:47-INFO->> Step 463990 run_train: loss = 5.3916  (0.151 sec)
18-06-05 02:47-INFO->> Step 464000 run_train: loss = 5.3089  (0.160 sec)
18-06-05 02:47-INFO->> 2018-06-05 02:47:47.787209 Saving in ckpt
18-06-05 02:47-INFO-Test Data Eval:
18-06-05 02:48-INFO-fpr95 = 0.17680658873538788 and auc = 0.968946468594768
18-06-05 02:48-INFO->> Step 464010 run_train: loss = 5.4387  (0.148 sec)
18-06-05 02:48-INFO->> Step 464020 run_train: loss = 5.3977  (0.158 sec)
18-06-05 02:48-INFO->> Step 464030 run_train: loss = 5.4315  (0.164 sec)
18-06-05 02:48-INFO->> Step 464040 run_train: loss = 5.4027  (0.161 sec)
18-06-05 02:48-INFO->> Step 464050 run_train: loss = 5.4412  (0.178 sec)
18-06-05 02:48-INFO->> Step 464060 run_train: loss = 5.4416  (0.180 sec)
18-06-05 02:48-INFO->> Step 464070 run_train: loss = 5.3976  (0.164 sec)
18-06-05 02:48-INFO->> Step 464080 run_train: loss = 5.3817  (0.185 sec)
18-06-05 02:48-INFO->> Step 464090 run_train: loss = 5.4490  (0.130 sec)
18-06-05 02:48-INFO->> Step 464100 run_train: loss = 5.3869  (0.147 sec)
18-06-05 02:48-INFO->> Step 464110 run_train: loss = 5.4468  (0.118 sec)
18-06-05 02:48-INFO->> Step 464120 run_train: loss = 5.4324  (0.148 sec)
18-06-05 02:48-INFO->> Step 464130 run_train: loss = 5.3507  (0.166 sec)
18-06-05 02:48-INFO->> Step 464140 run_train: loss = 5.3394  (0.138 sec)
18-06-05 02:48-INFO->> Step 464150 run_train: loss = 5.4246  (0.151 sec)
18-06-05 02:48-INFO->> Step 464160 run_train: loss = 5.4117  (0.144 sec)
18-06-05 02:48-INFO->> Step 464170 run_train: loss = 5.3967  (0.161 sec)
18-06-05 02:48-INFO->> Step 464180 run_train: loss = 5.4239  (0.170 sec)
18-06-05 02:48-INFO->> Step 464190 run_train: loss = 5.4364  (0.139 sec)
18-06-05 02:48-INFO->> Step 464200 run_train: loss = 5.4533  (0.164 sec)
18-06-05 02:49-INFO->> Step 464210 run_train: loss = 5.3970  (0.126 sec)
18-06-05 02:49-INFO->> Step 464220 run_train: loss = 5.4127  (0.129 sec)
18-06-05 02:49-INFO->> Step 464230 run_train: loss = 5.4032  (0.145 sec)
18-06-05 02:49-INFO->> Step 464240 run_train: loss = 5.3689  (0.132 sec)
18-06-05 02:49-INFO->> Step 464250 run_train: loss = 5.4108  (0.174 sec)
18-06-05 02:49-INFO->> Step 464260 run_train: loss = 5.4087  (0.164 sec)
18-06-05 02:49-INFO->> Step 464270 run_train: loss = 5.4258  (0.143 sec)
18-06-05 02:49-INFO->> Step 464280 run_train: loss = 5.3679  (0.138 sec)
18-06-05 02:49-INFO->> Step 464290 run_train: loss = 5.4814  (0.151 sec)
18-06-05 02:49-INFO->> Step 464300 run_train: loss = 5.4176  (0.136 sec)
18-06-05 02:49-INFO->> Step 464310 run_train: loss = 5.3986  (0.163 sec)
18-06-05 02:49-INFO->> Step 464320 run_train: loss = 5.3605  (0.153 sec)
18-06-05 02:49-INFO->> Step 464330 run_train: loss = 5.3913  (0.180 sec)
18-06-05 02:49-INFO->> Step 464340 run_train: loss = 5.4174  (0.147 sec)
18-06-05 02:49-INFO->> Step 464350 run_train: loss = 5.3392  (0.164 sec)
18-06-05 02:49-INFO->> Step 464360 run_train: loss = 5.3769  (0.132 sec)
18-06-05 02:49-INFO->> Step 464370 run_train: loss = 5.4798  (0.138 sec)
18-06-05 02:49-INFO->> Step 464380 run_train: loss = 5.3548  (0.164 sec)
18-06-05 02:49-INFO->> Step 464390 run_train: loss = 5.3932  (0.142 sec)
18-06-05 02:49-INFO->> Step 464400 run_train: loss = 5.3986  (0.165 sec)
18-06-05 02:49-INFO->> Step 464410 run_train: loss = 5.3696  (0.168 sec)
18-06-05 02:49-INFO->> Step 464420 run_train: loss = 5.4295  (0.113 sec)
18-06-05 02:49-INFO->> Step 464430 run_train: loss = 5.3737  (0.172 sec)
18-06-05 02:49-INFO->> Step 464440 run_train: loss = 5.4828  (0.187 sec)
18-06-05 02:49-INFO->> Step 464450 run_train: loss = 5.3628  (0.149 sec)
18-06-05 02:49-INFO->> Step 464460 run_train: loss = 5.4470  (0.149 sec)
18-06-05 02:49-INFO->> Step 464470 run_train: loss = 5.3738  (0.132 sec)
18-06-05 02:49-INFO->> Step 464480 run_train: loss = 5.3576  (0.169 sec)
18-06-05 02:49-INFO->> Step 464490 run_train: loss = 5.3845  (0.173 sec)
18-06-05 02:49-INFO->> Step 464500 run_train: loss = 5.3641  (0.141 sec)
18-06-05 02:49-INFO->> Step 464510 run_train: loss = 5.4016  (0.206 sec)
18-06-05 02:49-INFO->> Step 464520 run_train: loss = 5.4374  (0.183 sec)
18-06-05 02:49-INFO->> Step 464530 run_train: loss = 5.4279  (0.187 sec)
18-06-05 02:49-INFO->> Step 464540 run_train: loss = 5.3526  (0.106 sec)
18-06-05 02:49-INFO->> Step 464550 run_train: loss = 5.3860  (0.158 sec)
18-06-05 02:49-INFO->> Step 464560 run_train: loss = 5.3948  (0.181 sec)
18-06-05 02:49-INFO->> Step 464570 run_train: loss = 5.3742  (0.131 sec)
18-06-05 02:50-INFO->> Step 464580 run_train: loss = 5.3392  (0.175 sec)
18-06-05 02:50-INFO->> Step 464590 run_train: loss = 5.4144  (0.159 sec)
18-06-05 02:50-INFO->> Step 464600 run_train: loss = 5.4047  (0.150 sec)
18-06-05 02:50-INFO->> Step 464610 run_train: loss = 5.4148  (0.170 sec)
18-06-05 02:50-INFO->> Step 464620 run_train: loss = 5.4719  (0.140 sec)
18-06-05 02:50-INFO->> Step 464630 run_train: loss = 5.4341  (0.122 sec)
18-06-05 02:50-INFO->> Step 464640 run_train: loss = 5.5023  (0.157 sec)
18-06-05 02:50-INFO->> Step 464650 run_train: loss = 5.4018  (0.183 sec)
18-06-05 02:50-INFO->> Step 464660 run_train: loss = 5.3770  (0.158 sec)
18-06-05 02:50-INFO->> Step 464670 run_train: loss = 5.4599  (0.194 sec)
18-06-05 02:50-INFO->> Step 464680 run_train: loss = 5.4224  (0.164 sec)
18-06-05 02:50-INFO->> Step 464690 run_train: loss = 5.3503  (0.170 sec)
18-06-05 02:50-INFO->> Step 464700 run_train: loss = 5.3290  (0.166 sec)
18-06-05 02:50-INFO->> Step 464710 run_train: loss = 5.4428  (0.153 sec)
18-06-05 02:50-INFO->> Step 464720 run_train: loss = 5.4453  (0.131 sec)
18-06-05 02:50-INFO->> Step 464730 run_train: loss = 5.4507  (0.149 sec)
18-06-05 02:50-INFO->> Step 464740 run_train: loss = 5.4077  (0.184 sec)
18-06-05 02:50-INFO->> Step 464750 run_train: loss = 5.4066  (0.192 sec)
18-06-05 02:50-INFO->> Step 464760 run_train: loss = 5.3622  (0.163 sec)
18-06-05 02:50-INFO->> Step 464770 run_train: loss = 5.3632  (0.142 sec)
18-06-05 02:50-INFO->> Step 464780 run_train: loss = 5.4164  (0.150 sec)
18-06-05 02:50-INFO->> Step 464790 run_train: loss = 5.4097  (0.122 sec)
18-06-05 02:50-INFO->> Step 464800 run_train: loss = 5.4479  (0.163 sec)
18-06-05 02:50-INFO->> Step 464810 run_train: loss = 5.4054  (0.147 sec)
18-06-05 02:50-INFO->> Step 464820 run_train: loss = 5.4094  (0.150 sec)
18-06-05 02:50-INFO->> Step 464830 run_train: loss = 5.4126  (0.152 sec)
18-06-05 02:50-INFO->> Step 464840 run_train: loss = 5.4094  (0.152 sec)
18-06-05 02:50-INFO->> Step 464850 run_train: loss = 5.3867  (0.194 sec)
18-06-05 02:50-INFO->> Step 464860 run_train: loss = 5.4009  (0.163 sec)
18-06-05 02:50-INFO->> Step 464870 run_train: loss = 5.3676  (0.153 sec)
18-06-05 02:50-INFO->> Step 464880 run_train: loss = 5.3191  (0.145 sec)
18-06-05 02:50-INFO->> Step 464890 run_train: loss = 5.3566  (0.131 sec)
18-06-05 02:50-INFO->> Step 464900 run_train: loss = 5.4092  (0.157 sec)
18-06-05 02:50-INFO->> Step 464910 run_train: loss = 5.3928  (0.164 sec)
18-06-05 02:50-INFO->> Step 464920 run_train: loss = 5.4436  (0.163 sec)
18-06-05 02:50-INFO->> Step 464930 run_train: loss = 5.4071  (0.141 sec)
18-06-05 02:50-INFO->> Step 464940 run_train: loss = 5.3780  (0.171 sec)
18-06-05 02:50-INFO->> Step 464950 run_train: loss = 5.3926  (0.153 sec)
18-06-05 02:51-INFO->> Step 464960 run_train: loss = 5.4527  (0.182 sec)
18-06-05 02:51-INFO->> Step 464970 run_train: loss = 5.4629  (0.148 sec)
18-06-05 02:51-INFO->> Step 464980 run_train: loss = 5.4399  (0.131 sec)
18-06-05 02:51-INFO->> Step 464990 run_train: loss = 5.3660  (0.160 sec)
18-06-05 02:51-INFO->> Step 465000 run_train: loss = 5.3801  (0.160 sec)
18-06-05 02:51-INFO->> 2018-06-05 02:51:06.890015 Saving in ckpt
18-06-05 02:51-INFO-Test Data Eval:
18-06-05 02:51-INFO-fpr95 = 0.17309544367693944 and auc = 0.9690958426449179
18-06-05 02:51-INFO->> Step 465010 run_train: loss = 5.3990  (0.154 sec)
18-06-05 02:51-INFO->> Step 465020 run_train: loss = 5.3902  (0.147 sec)
18-06-05 02:51-INFO->> Step 465030 run_train: loss = 5.3981  (0.155 sec)
18-06-05 02:51-INFO->> Step 465040 run_train: loss = 5.4003  (0.157 sec)
18-06-05 02:51-INFO->> Step 465050 run_train: loss = 5.3625  (0.143 sec)
18-06-05 02:51-INFO->> Step 465060 run_train: loss = 5.4499  (0.155 sec)
18-06-05 02:51-INFO->> Step 465070 run_train: loss = 5.4175  (0.149 sec)
18-06-05 02:51-INFO->> Step 465080 run_train: loss = 5.3787  (0.154 sec)
18-06-05 02:52-INFO->> Step 465090 run_train: loss = 5.4615  (0.181 sec)
18-06-05 02:52-INFO->> Step 465100 run_train: loss = 5.4113  (0.164 sec)
18-06-05 02:52-INFO->> Step 465110 run_train: loss = 5.3890  (0.162 sec)
18-06-05 02:52-INFO->> Step 465120 run_train: loss = 5.4059  (0.165 sec)
18-06-05 02:52-INFO->> Step 465130 run_train: loss = 5.3630  (0.161 sec)
18-06-05 02:52-INFO->> Step 465140 run_train: loss = 5.4350  (0.139 sec)
18-06-05 02:52-INFO->> Step 465150 run_train: loss = 5.3683  (0.116 sec)
18-06-05 02:52-INFO->> Step 465160 run_train: loss = 5.4547  (0.139 sec)
18-06-05 02:52-INFO->> Step 465170 run_train: loss = 5.4410  (0.158 sec)
18-06-05 02:52-INFO->> Step 465180 run_train: loss = 5.4284  (0.151 sec)
18-06-05 02:52-INFO->> Step 465190 run_train: loss = 5.3935  (0.169 sec)
18-06-05 02:52-INFO->> Step 465200 run_train: loss = 5.4551  (0.163 sec)
18-06-05 02:52-INFO->> Step 465210 run_train: loss = 5.4303  (0.180 sec)
18-06-05 02:52-INFO->> Step 465220 run_train: loss = 5.3722  (0.168 sec)
18-06-05 02:52-INFO->> Step 465230 run_train: loss = 5.4447  (0.193 sec)
18-06-05 02:52-INFO->> Step 465240 run_train: loss = 5.4283  (0.152 sec)
18-06-05 02:52-INFO->> Step 465250 run_train: loss = 5.4625  (0.146 sec)
18-06-05 02:52-INFO->> Step 465260 run_train: loss = 5.3624  (0.204 sec)
18-06-05 02:52-INFO->> Step 465270 run_train: loss = 5.3837  (0.156 sec)
18-06-05 02:52-INFO->> Step 465280 run_train: loss = 5.4315  (0.157 sec)
18-06-05 02:52-INFO->> Step 465290 run_train: loss = 5.4196  (0.159 sec)
18-06-05 02:52-INFO->> Step 465300 run_train: loss = 5.3775  (0.149 sec)
18-06-05 02:52-INFO->> Step 465310 run_train: loss = 5.4285  (0.134 sec)
18-06-05 02:52-INFO->> Step 465320 run_train: loss = 5.3921  (0.174 sec)
18-06-05 02:52-INFO->> Step 465330 run_train: loss = 5.3992  (0.157 sec)
18-06-05 02:52-INFO->> Step 465340 run_train: loss = 5.3688  (0.186 sec)
18-06-05 02:52-INFO->> Step 465350 run_train: loss = 5.4173  (0.151 sec)
18-06-05 02:52-INFO->> Step 465360 run_train: loss = 5.4019  (0.148 sec)
18-06-05 02:52-INFO->> Step 465370 run_train: loss = 5.3541  (0.167 sec)
18-06-05 02:52-INFO->> Step 465380 run_train: loss = 5.3997  (0.156 sec)
18-06-05 02:52-INFO->> Step 465390 run_train: loss = 5.3774  (0.165 sec)
18-06-05 02:52-INFO->> Step 465400 run_train: loss = 5.3826  (0.146 sec)
18-06-05 02:52-INFO->> Step 465410 run_train: loss = 5.4609  (0.174 sec)
18-06-05 02:52-INFO->> Step 465420 run_train: loss = 5.3128  (0.142 sec)
18-06-05 02:52-INFO->> Step 465430 run_train: loss = 5.3966  (0.143 sec)
18-06-05 02:52-INFO->> Step 465440 run_train: loss = 5.3920  (0.174 sec)
18-06-05 02:52-INFO->> Step 465450 run_train: loss = 5.3396  (0.163 sec)
18-06-05 02:52-INFO->> Step 465460 run_train: loss = 5.3768  (0.148 sec)
18-06-05 02:53-INFO->> Step 465470 run_train: loss = 5.3913  (0.182 sec)
18-06-05 02:53-INFO->> Step 465480 run_train: loss = 5.4540  (0.188 sec)
18-06-05 02:53-INFO->> Step 465490 run_train: loss = 5.4648  (0.134 sec)
18-06-05 02:53-INFO->> Step 465500 run_train: loss = 5.3920  (0.153 sec)
18-06-05 02:53-INFO->> Step 465510 run_train: loss = 5.4074  (0.133 sec)
18-06-05 02:53-INFO->> Step 465520 run_train: loss = 5.3622  (0.167 sec)
18-06-05 02:53-INFO->> Step 465530 run_train: loss = 5.4437  (0.153 sec)
18-06-05 02:53-INFO->> Step 465540 run_train: loss = 5.3921  (0.167 sec)
18-06-05 02:53-INFO->> Step 465550 run_train: loss = 5.3960  (0.182 sec)
18-06-05 02:53-INFO->> Step 465560 run_train: loss = 5.4358  (0.199 sec)
18-06-05 02:53-INFO->> Step 465570 run_train: loss = 5.4579  (0.140 sec)
18-06-05 02:53-INFO->> Step 465580 run_train: loss = 5.4325  (0.171 sec)
18-06-05 02:53-INFO->> Step 465590 run_train: loss = 5.4464  (0.166 sec)
18-06-05 02:53-INFO->> Step 465600 run_train: loss = 5.4167  (0.152 sec)
18-06-05 02:53-INFO->> Step 465610 run_train: loss = 5.4114  (0.154 sec)
18-06-05 02:53-INFO->> Step 465620 run_train: loss = 5.3745  (0.187 sec)
18-06-05 02:53-INFO->> Step 465630 run_train: loss = 5.4304  (0.185 sec)
18-06-05 02:53-INFO->> Step 465640 run_train: loss = 5.3825  (0.137 sec)
18-06-05 02:53-INFO->> Step 465650 run_train: loss = 5.4129  (0.160 sec)
18-06-05 02:53-INFO->> Step 465660 run_train: loss = 5.4097  (0.127 sec)
18-06-05 02:53-INFO->> Step 465670 run_train: loss = 5.4827  (0.137 sec)
18-06-05 02:53-INFO->> Step 465680 run_train: loss = 5.3944  (0.188 sec)
18-06-05 02:53-INFO->> Step 465690 run_train: loss = 5.3836  (0.168 sec)
18-06-05 02:53-INFO->> Step 465700 run_train: loss = 5.3793  (0.171 sec)
18-06-05 02:53-INFO->> Step 465710 run_train: loss = 5.3601  (0.161 sec)
18-06-05 02:53-INFO->> Step 465720 run_train: loss = 5.4015  (0.151 sec)
18-06-05 02:53-INFO->> Step 465730 run_train: loss = 5.3577  (0.129 sec)
18-06-05 02:53-INFO->> Step 465740 run_train: loss = 5.4137  (0.146 sec)
18-06-05 02:53-INFO->> Step 465750 run_train: loss = 5.4050  (0.130 sec)
18-06-05 02:53-INFO->> Step 465760 run_train: loss = 5.3904  (0.167 sec)
18-06-05 02:53-INFO->> Step 465770 run_train: loss = 5.4352  (0.178 sec)
18-06-05 02:53-INFO->> Step 465780 run_train: loss = 5.3877  (0.220 sec)
18-06-05 02:53-INFO->> Step 465790 run_train: loss = 5.3662  (0.152 sec)
18-06-05 02:53-INFO->> Step 465800 run_train: loss = 5.4096  (0.130 sec)
18-06-05 02:53-INFO->> Step 465810 run_train: loss = 5.4375  (0.143 sec)
18-06-05 02:53-INFO->> Step 465820 run_train: loss = 5.4245  (0.194 sec)
18-06-05 02:53-INFO->> Step 465830 run_train: loss = 5.3867  (0.166 sec)
18-06-05 02:54-INFO->> Step 465840 run_train: loss = 5.3706  (0.148 sec)
18-06-05 02:54-INFO->> Step 465850 run_train: loss = 5.3846  (0.151 sec)
18-06-05 02:54-INFO->> Step 465860 run_train: loss = 5.4698  (0.179 sec)
18-06-05 02:54-INFO->> Step 465870 run_train: loss = 5.5085  (0.138 sec)
18-06-05 02:54-INFO->> Step 465880 run_train: loss = 5.3646  (0.143 sec)
18-06-05 02:54-INFO->> Step 465890 run_train: loss = 5.4899  (0.145 sec)
18-06-05 02:54-INFO->> Step 465900 run_train: loss = 5.3882  (0.178 sec)
18-06-05 02:54-INFO->> Step 465910 run_train: loss = 5.3724  (0.183 sec)
18-06-05 02:54-INFO->> Step 465920 run_train: loss = 5.3598  (0.189 sec)
18-06-05 02:54-INFO->> Step 465930 run_train: loss = 5.4117  (0.163 sec)
18-06-05 02:54-INFO->> Step 465940 run_train: loss = 5.4611  (0.161 sec)
18-06-05 02:54-INFO->> Step 465950 run_train: loss = 5.4767  (0.138 sec)
18-06-05 02:54-INFO->> Step 465960 run_train: loss = 5.3672  (0.151 sec)
18-06-05 02:54-INFO->> Step 465970 run_train: loss = 5.3989  (0.154 sec)
18-06-05 02:54-INFO->> Step 465980 run_train: loss = 5.3847  (0.187 sec)
18-06-05 02:54-INFO->> Step 465990 run_train: loss = 5.4059  (0.149 sec)
18-06-05 02:54-INFO->> Step 466000 run_train: loss = 5.4996  (0.183 sec)
18-06-05 02:54-INFO->> 2018-06-05 02:54:25.765009 Saving in ckpt
18-06-05 02:54-INFO-Test Data Eval:
18-06-05 02:55-INFO-fpr95 = 0.17624202975557918 and auc = 0.9689463510711379
18-06-05 02:55-INFO->> Step 466010 run_train: loss = 5.3660  (0.159 sec)
18-06-05 02:55-INFO->> Step 466020 run_train: loss = 5.3489  (0.177 sec)
18-06-05 02:55-INFO->> Step 466030 run_train: loss = 5.4023  (0.146 sec)
18-06-05 02:55-INFO->> Step 466040 run_train: loss = 5.4270  (0.163 sec)
18-06-05 02:55-INFO->> Step 466050 run_train: loss = 5.3860  (0.161 sec)
18-06-05 02:55-INFO->> Step 466060 run_train: loss = 5.4016  (0.131 sec)
18-06-05 02:55-INFO->> Step 466070 run_train: loss = 5.4368  (0.187 sec)
18-06-05 02:55-INFO->> Step 466080 run_train: loss = 5.4163  (0.135 sec)
18-06-05 02:55-INFO->> Step 466090 run_train: loss = 5.4439  (0.167 sec)
18-06-05 02:55-INFO->> Step 466100 run_train: loss = 5.4038  (0.135 sec)
18-06-05 02:55-INFO->> Step 466110 run_train: loss = 5.4271  (0.173 sec)
18-06-05 02:55-INFO->> Step 466120 run_train: loss = 5.3491  (0.150 sec)
18-06-05 02:55-INFO->> Step 466130 run_train: loss = 5.3894  (0.147 sec)
18-06-05 02:55-INFO->> Step 466140 run_train: loss = 5.4415  (0.148 sec)
18-06-05 02:55-INFO->> Step 466150 run_train: loss = 5.3721  (0.131 sec)
18-06-05 02:55-INFO->> Step 466160 run_train: loss = 5.4094  (0.186 sec)
18-06-05 02:55-INFO->> Step 466170 run_train: loss = 5.4093  (0.142 sec)
18-06-05 02:55-INFO->> Step 466180 run_train: loss = 5.3853  (0.159 sec)
18-06-05 02:55-INFO->> Step 466190 run_train: loss = 5.3919  (0.185 sec)
18-06-05 02:55-INFO->> Step 466200 run_train: loss = 5.3886  (0.133 sec)
18-06-05 02:55-INFO->> Step 466210 run_train: loss = 5.4446  (0.145 sec)
18-06-05 02:55-INFO->> Step 466220 run_train: loss = 5.4868  (0.198 sec)
18-06-05 02:55-INFO->> Step 466230 run_train: loss = 5.4044  (0.167 sec)
18-06-05 02:55-INFO->> Step 466240 run_train: loss = 5.3816  (0.140 sec)
18-06-05 02:55-INFO->> Step 466250 run_train: loss = 5.4668  (0.162 sec)
18-06-05 02:55-INFO->> Step 466260 run_train: loss = 5.3644  (0.138 sec)
18-06-05 02:55-INFO->> Step 466270 run_train: loss = 5.3459  (0.152 sec)
18-06-05 02:55-INFO->> Step 466280 run_train: loss = 5.4113  (0.160 sec)
18-06-05 02:55-INFO->> Step 466290 run_train: loss = 5.4349  (0.173 sec)
18-06-05 02:55-INFO->> Step 466300 run_train: loss = 5.4687  (0.144 sec)
18-06-05 02:55-INFO->> Step 466310 run_train: loss = 5.3741  (0.147 sec)
18-06-05 02:55-INFO->> Step 466320 run_train: loss = 5.3846  (0.176 sec)
18-06-05 02:55-INFO->> Step 466330 run_train: loss = 5.4490  (0.152 sec)
18-06-05 02:55-INFO->> Step 466340 run_train: loss = 5.4740  (0.149 sec)
18-06-05 02:56-INFO->> Step 466350 run_train: loss = 5.4368  (0.164 sec)
18-06-05 02:56-INFO->> Step 466360 run_train: loss = 5.4866  (0.121 sec)
18-06-05 02:56-INFO->> Step 466370 run_train: loss = 5.4760  (0.154 sec)
18-06-05 02:56-INFO->> Step 466380 run_train: loss = 5.4185  (0.196 sec)
18-06-05 02:56-INFO->> Step 466390 run_train: loss = 5.4116  (0.141 sec)
18-06-05 02:56-INFO->> Step 466400 run_train: loss = 5.4281  (0.160 sec)
18-06-05 02:56-INFO->> Step 466410 run_train: loss = 5.4059  (0.165 sec)
18-06-05 02:56-INFO->> Step 466420 run_train: loss = 5.3787  (0.129 sec)
18-06-05 02:56-INFO->> Step 466430 run_train: loss = 5.3433  (0.111 sec)
18-06-05 02:56-INFO->> Step 466440 run_train: loss = 5.4471  (0.166 sec)
18-06-05 02:56-INFO->> Step 466450 run_train: loss = 5.3961  (0.155 sec)
18-06-05 02:56-INFO->> Step 466460 run_train: loss = 5.4040  (0.171 sec)
18-06-05 02:56-INFO->> Step 466470 run_train: loss = 5.4587  (0.154 sec)
18-06-05 02:56-INFO->> Step 466480 run_train: loss = 5.4231  (0.161 sec)
18-06-05 02:56-INFO->> Step 466490 run_train: loss = 5.4484  (0.194 sec)
18-06-05 02:56-INFO->> Step 466500 run_train: loss = 5.4497  (0.163 sec)
18-06-05 02:56-INFO->> Step 466510 run_train: loss = 5.4477  (0.184 sec)
18-06-05 02:56-INFO->> Step 466520 run_train: loss = 5.4546  (0.139 sec)
18-06-05 02:56-INFO->> Step 466530 run_train: loss = 5.4073  (0.161 sec)
18-06-05 02:56-INFO->> Step 466540 run_train: loss = 5.4153  (0.161 sec)
18-06-05 02:56-INFO->> Step 466550 run_train: loss = 5.3358  (0.179 sec)
18-06-05 02:56-INFO->> Step 466560 run_train: loss = 5.4406  (0.158 sec)
18-06-05 02:56-INFO->> Step 466570 run_train: loss = 5.4068  (0.170 sec)
18-06-05 02:56-INFO->> Step 466580 run_train: loss = 5.3654  (0.145 sec)
18-06-05 02:56-INFO->> Step 466590 run_train: loss = 5.4045  (0.159 sec)
18-06-05 02:56-INFO->> Step 466600 run_train: loss = 5.4554  (0.107 sec)
18-06-05 02:56-INFO->> Step 466610 run_train: loss = 5.4326  (0.176 sec)
18-06-05 02:56-INFO->> Step 466620 run_train: loss = 5.4315  (0.161 sec)
18-06-05 02:56-INFO->> Step 466630 run_train: loss = 5.3995  (0.158 sec)
18-06-05 02:56-INFO->> Step 466640 run_train: loss = 5.3696  (0.158 sec)
18-06-05 02:56-INFO->> Step 466650 run_train: loss = 5.4467  (0.131 sec)
18-06-05 02:56-INFO->> Step 466660 run_train: loss = 5.3351  (0.155 sec)
18-06-05 02:56-INFO->> Step 466670 run_train: loss = 5.3942  (0.159 sec)
18-06-05 02:56-INFO->> Step 466680 run_train: loss = 5.4280  (0.179 sec)
18-06-05 02:56-INFO->> Step 466690 run_train: loss = 5.3833  (0.191 sec)
18-06-05 02:56-INFO->> Step 466700 run_train: loss = 5.4346  (0.158 sec)
18-06-05 02:56-INFO->> Step 466710 run_train: loss = 5.4079  (0.169 sec)
18-06-05 02:57-INFO->> Step 466720 run_train: loss = 5.3396  (0.177 sec)
18-06-05 02:57-INFO->> Step 466730 run_train: loss = 5.4296  (0.146 sec)
18-06-05 02:57-INFO->> Step 466740 run_train: loss = 5.4295  (0.109 sec)
18-06-05 02:57-INFO->> Step 466750 run_train: loss = 5.4962  (0.183 sec)
18-06-05 02:57-INFO->> Step 466760 run_train: loss = 5.3869  (0.183 sec)
18-06-05 02:57-INFO->> Step 466770 run_train: loss = 5.3807  (0.138 sec)
18-06-05 02:57-INFO->> Step 466780 run_train: loss = 5.3865  (0.106 sec)
18-06-05 02:57-INFO->> Step 466790 run_train: loss = 5.4147  (0.151 sec)
18-06-05 02:57-INFO->> Step 466800 run_train: loss = 5.4521  (0.128 sec)
18-06-05 02:57-INFO->> Step 466810 run_train: loss = 5.3629  (0.144 sec)
18-06-05 02:57-INFO->> Step 466820 run_train: loss = 5.3935  (0.143 sec)
18-06-05 02:57-INFO->> Step 466830 run_train: loss = 5.4290  (0.169 sec)
18-06-05 02:57-INFO->> Step 466840 run_train: loss = 5.3820  (0.147 sec)
18-06-05 02:57-INFO->> Step 466850 run_train: loss = 5.3517  (0.162 sec)
18-06-05 02:57-INFO->> Step 466860 run_train: loss = 5.4196  (0.208 sec)
18-06-05 02:57-INFO->> Step 466870 run_train: loss = 5.3295  (0.134 sec)
18-06-05 02:57-INFO->> Step 466880 run_train: loss = 5.4198  (0.150 sec)
18-06-05 02:57-INFO->> Step 466890 run_train: loss = 5.3488  (0.166 sec)
18-06-05 02:57-INFO->> Step 466900 run_train: loss = 5.4589  (0.148 sec)
18-06-05 02:57-INFO->> Step 466910 run_train: loss = 5.3694  (0.153 sec)
18-06-05 02:57-INFO->> Step 466920 run_train: loss = 5.4193  (0.161 sec)
18-06-05 02:57-INFO->> Step 466930 run_train: loss = 5.3833  (0.159 sec)
18-06-05 02:57-INFO->> Step 466940 run_train: loss = 5.4430  (0.171 sec)
18-06-05 02:57-INFO->> Step 466950 run_train: loss = 5.3764  (0.164 sec)
18-06-05 02:57-INFO->> Step 466960 run_train: loss = 5.3868  (0.166 sec)
18-06-05 02:57-INFO->> Step 466970 run_train: loss = 5.4113  (0.184 sec)
18-06-05 02:57-INFO->> Step 466980 run_train: loss = 5.3686  (0.121 sec)
18-06-05 02:57-INFO->> Step 466990 run_train: loss = 5.4131  (0.169 sec)
18-06-05 02:57-INFO->> Step 467000 run_train: loss = 5.4152  (0.134 sec)
18-06-05 02:57-INFO->> 2018-06-05 02:57:44.509257 Saving in ckpt
18-06-05 02:57-INFO-Test Data Eval:
18-06-05 02:58-INFO-fpr95 = 0.17352716524973433 and auc = 0.968934106211754
18-06-05 02:58-INFO->> Step 467010 run_train: loss = 5.3575  (0.147 sec)
18-06-05 02:58-INFO->> Step 467020 run_train: loss = 5.3895  (0.159 sec)
18-06-05 02:58-INFO->> Step 467030 run_train: loss = 5.3945  (0.182 sec)
18-06-05 02:58-INFO->> Step 467040 run_train: loss = 5.3795  (0.151 sec)
18-06-05 02:58-INFO->> Step 467050 run_train: loss = 5.3779  (0.133 sec)
18-06-05 02:58-INFO->> Step 467060 run_train: loss = 5.4109  (0.148 sec)
18-06-05 02:58-INFO->> Step 467070 run_train: loss = 5.4297  (0.155 sec)
18-06-05 02:58-INFO->> Step 467080 run_train: loss = 5.4400  (0.137 sec)
18-06-05 02:58-INFO->> Step 467090 run_train: loss = 5.3581  (0.128 sec)
18-06-05 02:58-INFO->> Step 467100 run_train: loss = 5.4131  (0.164 sec)
18-06-05 02:58-INFO->> Step 467110 run_train: loss = 5.3904  (0.156 sec)
18-06-05 02:58-INFO->> Step 467120 run_train: loss = 5.4519  (0.155 sec)
18-06-05 02:58-INFO->> Step 467130 run_train: loss = 5.4048  (0.144 sec)
18-06-05 02:58-INFO->> Step 467140 run_train: loss = 5.3919  (0.143 sec)
18-06-05 02:58-INFO->> Step 467150 run_train: loss = 5.3993  (0.145 sec)
18-06-05 02:58-INFO->> Step 467160 run_train: loss = 5.4466  (0.129 sec)
18-06-05 02:58-INFO->> Step 467170 run_train: loss = 5.4277  (0.137 sec)
18-06-05 02:58-INFO->> Step 467180 run_train: loss = 5.4072  (0.162 sec)
18-06-05 02:58-INFO->> Step 467190 run_train: loss = 5.3758  (0.170 sec)
18-06-05 02:58-INFO->> Step 467200 run_train: loss = 5.3594  (0.151 sec)
18-06-05 02:58-INFO->> Step 467210 run_train: loss = 5.4550  (0.163 sec)
18-06-05 02:58-INFO->> Step 467220 run_train: loss = 5.4632  (0.152 sec)
18-06-05 02:59-INFO->> Step 467230 run_train: loss = 5.3709  (0.137 sec)
18-06-05 02:59-INFO->> Step 467240 run_train: loss = 5.3786  (0.193 sec)
18-06-05 02:59-INFO->> Step 467250 run_train: loss = 5.4180  (0.158 sec)
18-06-05 02:59-INFO->> Step 467260 run_train: loss = 5.3916  (0.179 sec)
18-06-05 02:59-INFO->> Step 467270 run_train: loss = 5.4367  (0.166 sec)
18-06-05 02:59-INFO->> Step 467280 run_train: loss = 5.4343  (0.177 sec)
18-06-05 02:59-INFO->> Step 467290 run_train: loss = 5.4171  (0.168 sec)
18-06-05 02:59-INFO->> Step 467300 run_train: loss = 5.4507  (0.158 sec)
18-06-05 02:59-INFO->> Step 467310 run_train: loss = 5.4468  (0.171 sec)
18-06-05 02:59-INFO->> Step 467320 run_train: loss = 5.4530  (0.156 sec)
18-06-05 02:59-INFO->> Step 467330 run_train: loss = 5.4146  (0.171 sec)
18-06-05 02:59-INFO->> Step 467340 run_train: loss = 5.4096  (0.130 sec)
18-06-05 02:59-INFO->> Step 467350 run_train: loss = 5.3527  (0.198 sec)
18-06-05 02:59-INFO->> Step 467360 run_train: loss = 5.4104  (0.187 sec)
18-06-05 02:59-INFO->> Step 467370 run_train: loss = 5.4235  (0.163 sec)
18-06-05 02:59-INFO->> Step 467380 run_train: loss = 5.4210  (0.190 sec)
18-06-05 02:59-INFO->> Step 467390 run_train: loss = 5.5115  (0.136 sec)
18-06-05 02:59-INFO->> Step 467400 run_train: loss = 5.4700  (0.158 sec)
18-06-05 02:59-INFO->> Step 467410 run_train: loss = 5.4452  (0.166 sec)
18-06-05 02:59-INFO->> Step 467420 run_train: loss = 5.4286  (0.173 sec)
18-06-05 02:59-INFO->> Step 467430 run_train: loss = 5.3912  (0.186 sec)
18-06-05 02:59-INFO->> Step 467440 run_train: loss = 5.3967  (0.169 sec)
18-06-05 02:59-INFO->> Step 467450 run_train: loss = 5.3788  (0.168 sec)
18-06-05 02:59-INFO->> Step 467460 run_train: loss = 5.3911  (0.155 sec)
18-06-05 02:59-INFO->> Step 467470 run_train: loss = 5.3669  (0.148 sec)
18-06-05 02:59-INFO->> Step 467480 run_train: loss = 5.4734  (0.141 sec)
18-06-05 02:59-INFO->> Step 467490 run_train: loss = 5.3939  (0.140 sec)
18-06-05 02:59-INFO->> Step 467500 run_train: loss = 5.4693  (0.133 sec)
18-06-05 02:59-INFO->> Step 467510 run_train: loss = 5.4060  (0.165 sec)
18-06-05 02:59-INFO->> Step 467520 run_train: loss = 5.4281  (0.157 sec)
18-06-05 02:59-INFO->> Step 467530 run_train: loss = 5.3682  (0.173 sec)
18-06-05 02:59-INFO->> Step 467540 run_train: loss = 5.4126  (0.152 sec)
18-06-05 02:59-INFO->> Step 467550 run_train: loss = 5.4571  (0.161 sec)
18-06-05 02:59-INFO->> Step 467560 run_train: loss = 5.3737  (0.145 sec)
18-06-05 02:59-INFO->> Step 467570 run_train: loss = 5.4164  (0.155 sec)
18-06-05 02:59-INFO->> Step 467580 run_train: loss = 5.4411  (0.161 sec)
18-06-05 02:59-INFO->> Step 467590 run_train: loss = 5.4257  (0.148 sec)
18-06-05 02:59-INFO->> Step 467600 run_train: loss = 5.3596  (0.180 sec)
18-06-05 03:00-INFO->> Step 467610 run_train: loss = 5.4816  (0.141 sec)
18-06-05 03:00-INFO->> Step 467620 run_train: loss = 5.4113  (0.160 sec)
18-06-05 03:00-INFO->> Step 467630 run_train: loss = 5.4751  (0.195 sec)
18-06-05 03:00-INFO->> Step 467640 run_train: loss = 5.4475  (0.124 sec)
18-06-05 03:00-INFO->> Step 467650 run_train: loss = 5.3648  (0.132 sec)
18-06-05 03:00-INFO->> Step 467660 run_train: loss = 5.4011  (0.158 sec)
18-06-05 03:00-INFO->> Step 467670 run_train: loss = 5.3964  (0.198 sec)
18-06-05 03:00-INFO->> Step 467680 run_train: loss = 5.4110  (0.153 sec)
18-06-05 03:00-INFO->> Step 467690 run_train: loss = 5.3761  (0.190 sec)
18-06-05 03:00-INFO->> Step 467700 run_train: loss = 5.3580  (0.160 sec)
18-06-05 03:00-INFO->> Step 467710 run_train: loss = 5.4046  (0.145 sec)
18-06-05 03:00-INFO->> Step 467720 run_train: loss = 5.4795  (0.151 sec)
18-06-05 03:00-INFO->> Step 467730 run_train: loss = 5.3717  (0.133 sec)
18-06-05 03:00-INFO->> Step 467740 run_train: loss = 5.3726  (0.121 sec)
18-06-05 03:00-INFO->> Step 467750 run_train: loss = 5.3780  (0.177 sec)
18-06-05 03:00-INFO->> Step 467760 run_train: loss = 5.3790  (0.163 sec)
18-06-05 03:00-INFO->> Step 467770 run_train: loss = 5.4136  (0.155 sec)
18-06-05 03:00-INFO->> Step 467780 run_train: loss = 5.4079  (0.166 sec)
18-06-05 03:00-INFO->> Step 467790 run_train: loss = 5.3407  (0.173 sec)
18-06-05 03:00-INFO->> Step 467800 run_train: loss = 5.3781  (0.156 sec)
18-06-05 03:00-INFO->> Step 467810 run_train: loss = 5.3220  (0.159 sec)
18-06-05 03:00-INFO->> Step 467820 run_train: loss = 5.4136  (0.145 sec)
18-06-05 03:00-INFO->> Step 467830 run_train: loss = 5.3681  (0.157 sec)
18-06-05 03:00-INFO->> Step 467840 run_train: loss = 5.4026  (0.153 sec)
18-06-05 03:00-INFO->> Step 467850 run_train: loss = 5.4299  (0.150 sec)
18-06-05 03:00-INFO->> Step 467860 run_train: loss = 5.3055  (0.166 sec)
18-06-05 03:00-INFO->> Step 467870 run_train: loss = 5.4516  (0.135 sec)
18-06-05 03:00-INFO->> Step 467880 run_train: loss = 5.3831  (0.132 sec)
18-06-05 03:00-INFO->> Step 467890 run_train: loss = 5.4506  (0.151 sec)
18-06-05 03:00-INFO->> Step 467900 run_train: loss = 5.3490  (0.172 sec)
18-06-05 03:00-INFO->> Step 467910 run_train: loss = 5.3557  (0.156 sec)
18-06-05 03:00-INFO->> Step 467920 run_train: loss = 5.3785  (0.131 sec)
18-06-05 03:00-INFO->> Step 467930 run_train: loss = 5.4251  (0.151 sec)
18-06-05 03:00-INFO->> Step 467940 run_train: loss = 5.3952  (0.173 sec)
18-06-05 03:00-INFO->> Step 467950 run_train: loss = 5.4044  (0.141 sec)
18-06-05 03:00-INFO->> Step 467960 run_train: loss = 5.4480  (0.129 sec)
18-06-05 03:00-INFO->> Step 467970 run_train: loss = 5.4406  (0.139 sec)
18-06-05 03:01-INFO->> Step 467980 run_train: loss = 5.4057  (0.151 sec)
18-06-05 03:01-INFO->> Step 467990 run_train: loss = 5.3915  (0.152 sec)
18-06-05 03:01-INFO->> Step 468000 run_train: loss = 5.4726  (0.170 sec)
18-06-05 03:01-INFO->> 2018-06-05 03:01:03.415388 Saving in ckpt
18-06-05 03:01-INFO-Test Data Eval:
18-06-05 03:01-INFO-fpr95 = 0.1722153958554729 and auc = 0.9691024359962754
18-06-05 03:01-INFO->> Step 468010 run_train: loss = 5.4034  (0.162 sec)
18-06-05 03:01-INFO->> Step 468020 run_train: loss = 5.3545  (0.177 sec)
18-06-05 03:01-INFO->> Step 468030 run_train: loss = 5.3981  (0.157 sec)
18-06-05 03:01-INFO->> Step 468040 run_train: loss = 5.4472  (0.181 sec)
18-06-05 03:01-INFO->> Step 468050 run_train: loss = 5.4072  (0.136 sec)
18-06-05 03:01-INFO->> Step 468060 run_train: loss = 5.3927  (0.188 sec)
18-06-05 03:01-INFO->> Step 468070 run_train: loss = 5.3830  (0.180 sec)
18-06-05 03:01-INFO->> Step 468080 run_train: loss = 5.3676  (0.163 sec)
18-06-05 03:01-INFO->> Step 468090 run_train: loss = 5.3454  (0.170 sec)
18-06-05 03:01-INFO->> Step 468100 run_train: loss = 5.4308  (0.171 sec)
18-06-05 03:02-INFO->> Step 468110 run_train: loss = 5.4225  (0.188 sec)
18-06-05 03:02-INFO->> Step 468120 run_train: loss = 5.3324  (0.158 sec)
18-06-05 03:02-INFO->> Step 468130 run_train: loss = 5.4185  (0.174 sec)
18-06-05 03:02-INFO->> Step 468140 run_train: loss = 5.4163  (0.160 sec)
18-06-05 03:02-INFO->> Step 468150 run_train: loss = 5.4165  (0.181 sec)
18-06-05 03:02-INFO->> Step 468160 run_train: loss = 5.4080  (0.156 sec)
18-06-05 03:02-INFO->> Step 468170 run_train: loss = 5.4111  (0.129 sec)
18-06-05 03:02-INFO->> Step 468180 run_train: loss = 5.3881  (0.174 sec)
18-06-05 03:02-INFO->> Step 468190 run_train: loss = 5.3831  (0.174 sec)
18-06-05 03:02-INFO->> Step 468200 run_train: loss = 5.4043  (0.149 sec)
18-06-05 03:02-INFO->> Step 468210 run_train: loss = 5.4171  (0.151 sec)
18-06-05 03:02-INFO->> Step 468220 run_train: loss = 5.4740  (0.189 sec)
18-06-05 03:02-INFO->> Step 468230 run_train: loss = 5.4113  (0.157 sec)
18-06-05 03:02-INFO->> Step 468240 run_train: loss = 5.4380  (0.183 sec)
18-06-05 03:02-INFO->> Step 468250 run_train: loss = 5.4700  (0.165 sec)
18-06-05 03:02-INFO->> Step 468260 run_train: loss = 5.4958  (0.158 sec)
18-06-05 03:02-INFO->> Step 468270 run_train: loss = 5.3769  (0.126 sec)
18-06-05 03:02-INFO->> Step 468280 run_train: loss = 5.4365  (0.188 sec)
18-06-05 03:02-INFO->> Step 468290 run_train: loss = 5.4052  (0.186 sec)
18-06-05 03:02-INFO->> Step 468300 run_train: loss = 5.4303  (0.134 sec)
18-06-05 03:02-INFO->> Step 468310 run_train: loss = 5.4380  (0.205 sec)
18-06-05 03:02-INFO->> Step 468320 run_train: loss = 5.4303  (0.157 sec)
18-06-05 03:02-INFO->> Step 468330 run_train: loss = 5.3819  (0.162 sec)
18-06-05 03:02-INFO->> Step 468340 run_train: loss = 5.3649  (0.160 sec)
18-06-05 03:02-INFO->> Step 468350 run_train: loss = 5.3689  (0.145 sec)
18-06-05 03:02-INFO->> Step 468360 run_train: loss = 5.3871  (0.169 sec)
18-06-05 03:02-INFO->> Step 468370 run_train: loss = 5.3821  (0.144 sec)
18-06-05 03:02-INFO->> Step 468380 run_train: loss = 5.3569  (0.210 sec)
18-06-05 03:02-INFO->> Step 468390 run_train: loss = 5.4479  (0.151 sec)
18-06-05 03:02-INFO->> Step 468400 run_train: loss = 5.4084  (0.137 sec)
18-06-05 03:02-INFO->> Step 468410 run_train: loss = 5.3643  (0.177 sec)
18-06-05 03:02-INFO->> Step 468420 run_train: loss = 5.3491  (0.132 sec)
18-06-05 03:02-INFO->> Step 468430 run_train: loss = 5.4319  (0.153 sec)
18-06-05 03:02-INFO->> Step 468440 run_train: loss = 5.4187  (0.176 sec)
18-06-05 03:02-INFO->> Step 468450 run_train: loss = 5.3905  (0.174 sec)
18-06-05 03:02-INFO->> Step 468460 run_train: loss = 5.4347  (0.103 sec)
18-06-05 03:02-INFO->> Step 468470 run_train: loss = 5.4406  (0.179 sec)
18-06-05 03:02-INFO->> Step 468480 run_train: loss = 5.3428  (0.176 sec)
18-06-05 03:03-INFO->> Step 468490 run_train: loss = 5.3824  (0.141 sec)
18-06-05 03:03-INFO->> Step 468500 run_train: loss = 5.4422  (0.174 sec)
18-06-05 03:03-INFO->> Step 468510 run_train: loss = 5.4326  (0.152 sec)
18-06-05 03:03-INFO->> Step 468520 run_train: loss = 5.3591  (0.160 sec)
18-06-05 03:03-INFO->> Step 468530 run_train: loss = 5.3355  (0.153 sec)
18-06-05 03:03-INFO->> Step 468540 run_train: loss = 5.4305  (0.160 sec)
18-06-05 03:03-INFO->> Step 468550 run_train: loss = 5.3662  (0.172 sec)
18-06-05 03:03-INFO->> Step 468560 run_train: loss = 5.4379  (0.161 sec)
18-06-05 03:03-INFO->> Step 468570 run_train: loss = 5.4278  (0.136 sec)
18-06-05 03:03-INFO->> Step 468580 run_train: loss = 5.3715  (0.194 sec)
18-06-05 03:03-INFO->> Step 468590 run_train: loss = 5.3880  (0.150 sec)
18-06-05 03:03-INFO->> Step 468600 run_train: loss = 5.4325  (0.197 sec)
18-06-05 03:03-INFO->> Step 468610 run_train: loss = 5.4299  (0.154 sec)
18-06-05 03:03-INFO->> Step 468620 run_train: loss = 5.4420  (0.145 sec)
18-06-05 03:03-INFO->> Step 468630 run_train: loss = 5.4530  (0.139 sec)
18-06-05 03:03-INFO->> Step 468640 run_train: loss = 5.4170  (0.188 sec)
18-06-05 03:03-INFO->> Step 468650 run_train: loss = 5.4833  (0.162 sec)
18-06-05 03:03-INFO->> Step 468660 run_train: loss = 5.4086  (0.125 sec)
18-06-05 03:03-INFO->> Step 468670 run_train: loss = 5.3951  (0.157 sec)
18-06-05 03:03-INFO->> Step 468680 run_train: loss = 5.3888  (0.172 sec)
18-06-05 03:03-INFO->> Step 468690 run_train: loss = 5.4267  (0.175 sec)
18-06-05 03:03-INFO->> Step 468700 run_train: loss = 5.3890  (0.213 sec)
18-06-05 03:03-INFO->> Step 468710 run_train: loss = 5.4559  (0.148 sec)
18-06-05 03:03-INFO->> Step 468720 run_train: loss = 5.4212  (0.145 sec)
18-06-05 03:03-INFO->> Step 468730 run_train: loss = 5.3370  (0.160 sec)
18-06-05 03:03-INFO->> Step 468740 run_train: loss = 5.3581  (0.147 sec)
18-06-05 03:03-INFO->> Step 468750 run_train: loss = 5.3205  (0.159 sec)
18-06-05 03:03-INFO->> Step 468760 run_train: loss = 5.4386  (0.143 sec)
18-06-05 03:03-INFO->> Step 468770 run_train: loss = 5.3916  (0.202 sec)
18-06-05 03:03-INFO->> Step 468780 run_train: loss = 5.3703  (0.135 sec)
18-06-05 03:03-INFO->> Step 468790 run_train: loss = 5.3841  (0.151 sec)
18-06-05 03:03-INFO->> Step 468800 run_train: loss = 5.4294  (0.169 sec)
18-06-05 03:03-INFO->> Step 468810 run_train: loss = 5.4114  (0.156 sec)
18-06-05 03:03-INFO->> Step 468820 run_train: loss = 5.4643  (0.195 sec)
18-06-05 03:03-INFO->> Step 468830 run_train: loss = 5.3669  (0.143 sec)
18-06-05 03:03-INFO->> Step 468840 run_train: loss = 5.4217  (0.145 sec)
18-06-05 03:03-INFO->> Step 468850 run_train: loss = 5.4797  (0.132 sec)
18-06-05 03:04-INFO->> Step 468860 run_train: loss = 5.3644  (0.168 sec)
18-06-05 03:04-INFO->> Step 468870 run_train: loss = 5.4484  (0.129 sec)
18-06-05 03:04-INFO->> Step 468880 run_train: loss = 5.3768  (0.136 sec)
18-06-05 03:04-INFO->> Step 468890 run_train: loss = 5.3979  (0.113 sec)
18-06-05 03:04-INFO->> Step 468900 run_train: loss = 5.3412  (0.156 sec)
18-06-05 03:04-INFO->> Step 468910 run_train: loss = 5.5036  (0.161 sec)
18-06-05 03:04-INFO->> Step 468920 run_train: loss = 5.4059  (0.187 sec)
18-06-05 03:04-INFO->> Step 468930 run_train: loss = 5.4462  (0.142 sec)
18-06-05 03:04-INFO->> Step 468940 run_train: loss = 5.3354  (0.159 sec)
18-06-05 03:04-INFO->> Step 468950 run_train: loss = 5.4310  (0.161 sec)
18-06-05 03:04-INFO->> Step 468960 run_train: loss = 5.3669  (0.168 sec)
18-06-05 03:04-INFO->> Step 468970 run_train: loss = 5.4377  (0.179 sec)
18-06-05 03:04-INFO->> Step 468980 run_train: loss = 5.3635  (0.172 sec)
18-06-05 03:04-INFO->> Step 468990 run_train: loss = 5.4371  (0.158 sec)
18-06-05 03:04-INFO->> Step 469000 run_train: loss = 5.4236  (0.162 sec)
18-06-05 03:04-INFO->> 2018-06-05 03:04:22.609882 Saving in ckpt
18-06-05 03:04-INFO-Test Data Eval:
18-06-05 03:05-INFO-fpr95 = 0.17416644527098832 and auc = 0.9689973115572877
18-06-05 03:05-INFO->> Step 469010 run_train: loss = 5.4372  (0.159 sec)
18-06-05 03:05-INFO->> Step 469020 run_train: loss = 5.3972  (0.141 sec)
18-06-05 03:05-INFO->> Step 469030 run_train: loss = 5.4160  (0.143 sec)
18-06-05 03:05-INFO->> Step 469040 run_train: loss = 5.3697  (0.159 sec)
18-06-05 03:05-INFO->> Step 469050 run_train: loss = 5.3685  (0.161 sec)
18-06-05 03:05-INFO->> Step 469060 run_train: loss = 5.3868  (0.155 sec)
18-06-05 03:05-INFO->> Step 469070 run_train: loss = 5.4291  (0.145 sec)
18-06-05 03:05-INFO->> Step 469080 run_train: loss = 5.4434  (0.164 sec)
18-06-05 03:05-INFO->> Step 469090 run_train: loss = 5.3880  (0.165 sec)
18-06-05 03:05-INFO->> Step 469100 run_train: loss = 5.4526  (0.172 sec)
18-06-05 03:05-INFO->> Step 469110 run_train: loss = 5.3847  (0.166 sec)
18-06-05 03:05-INFO->> Step 469120 run_train: loss = 5.3711  (0.174 sec)
18-06-05 03:05-INFO->> Step 469130 run_train: loss = 5.4225  (0.186 sec)
18-06-05 03:05-INFO->> Step 469140 run_train: loss = 5.4012  (0.166 sec)
18-06-05 03:05-INFO->> Step 469150 run_train: loss = 5.4209  (0.155 sec)
18-06-05 03:05-INFO->> Step 469160 run_train: loss = 5.3892  (0.132 sec)
18-06-05 03:05-INFO->> Step 469170 run_train: loss = 5.4114  (0.189 sec)
18-06-05 03:05-INFO->> Step 469180 run_train: loss = 5.3936  (0.126 sec)
18-06-05 03:05-INFO->> Step 469190 run_train: loss = 5.4465  (0.129 sec)
18-06-05 03:05-INFO->> Step 469200 run_train: loss = 5.3464  (0.126 sec)
18-06-05 03:05-INFO->> Step 469210 run_train: loss = 5.3767  (0.145 sec)
18-06-05 03:05-INFO->> Step 469220 run_train: loss = 5.4066  (0.156 sec)
18-06-05 03:05-INFO->> Step 469230 run_train: loss = 5.4044  (0.162 sec)
18-06-05 03:05-INFO->> Step 469240 run_train: loss = 5.3785  (0.158 sec)
18-06-05 03:05-INFO->> Step 469250 run_train: loss = 5.3874  (0.153 sec)
18-06-05 03:05-INFO->> Step 469260 run_train: loss = 5.4492  (0.183 sec)
18-06-05 03:05-INFO->> Step 469270 run_train: loss = 5.3055  (0.169 sec)
18-06-05 03:05-INFO->> Step 469280 run_train: loss = 5.3405  (0.155 sec)
18-06-05 03:05-INFO->> Step 469290 run_train: loss = 5.4119  (0.145 sec)
18-06-05 03:05-INFO->> Step 469300 run_train: loss = 5.4037  (0.210 sec)
18-06-05 03:05-INFO->> Step 469310 run_train: loss = 5.3787  (0.160 sec)
18-06-05 03:05-INFO->> Step 469320 run_train: loss = 5.3596  (0.157 sec)
18-06-05 03:05-INFO->> Step 469330 run_train: loss = 5.3887  (0.179 sec)
18-06-05 03:05-INFO->> Step 469340 run_train: loss = 5.3968  (0.172 sec)
18-06-05 03:05-INFO->> Step 469350 run_train: loss = 5.4200  (0.181 sec)
18-06-05 03:05-INFO->> Step 469360 run_train: loss = 5.4340  (0.140 sec)
18-06-05 03:06-INFO->> Step 469370 run_train: loss = 5.3769  (0.138 sec)
18-06-05 03:06-INFO->> Step 469380 run_train: loss = 5.3874  (0.145 sec)
18-06-05 03:06-INFO->> Step 469390 run_train: loss = 5.4076  (0.140 sec)
18-06-05 03:06-INFO->> Step 469400 run_train: loss = 5.4440  (0.156 sec)
18-06-05 03:06-INFO->> Step 469410 run_train: loss = 5.3806  (0.134 sec)
18-06-05 03:06-INFO->> Step 469420 run_train: loss = 5.3976  (0.132 sec)
18-06-05 03:06-INFO->> Step 469430 run_train: loss = 5.4187  (0.139 sec)
18-06-05 03:06-INFO->> Step 469440 run_train: loss = 5.4103  (0.160 sec)
18-06-05 03:06-INFO->> Step 469450 run_train: loss = 5.3425  (0.177 sec)
18-06-05 03:06-INFO->> Step 469460 run_train: loss = 5.4099  (0.181 sec)
18-06-05 03:06-INFO->> Step 469470 run_train: loss = 5.3986  (0.177 sec)
18-06-05 03:06-INFO->> Step 469480 run_train: loss = 5.3760  (0.172 sec)
18-06-05 03:06-INFO->> Step 469490 run_train: loss = 5.3688  (0.146 sec)
18-06-05 03:06-INFO->> Step 469500 run_train: loss = 5.4344  (0.129 sec)
18-06-05 03:06-INFO->> Step 469510 run_train: loss = 5.4085  (0.167 sec)
18-06-05 03:06-INFO->> Step 469520 run_train: loss = 5.3990  (0.150 sec)
18-06-05 03:06-INFO->> Step 469530 run_train: loss = 5.4029  (0.160 sec)
18-06-05 03:06-INFO->> Step 469540 run_train: loss = 5.3936  (0.156 sec)
18-06-05 03:06-INFO->> Step 469550 run_train: loss = 5.4744  (0.172 sec)
18-06-05 03:06-INFO->> Step 469560 run_train: loss = 5.4072  (0.149 sec)
18-06-05 03:06-INFO->> Step 469570 run_train: loss = 5.4011  (0.159 sec)
18-06-05 03:06-INFO->> Step 469580 run_train: loss = 5.3939  (0.161 sec)
18-06-05 03:06-INFO->> Step 469590 run_train: loss = 5.3559  (0.127 sec)
18-06-05 03:06-INFO->> Step 469600 run_train: loss = 5.3927  (0.161 sec)
18-06-05 03:06-INFO->> Step 469610 run_train: loss = 5.3804  (0.184 sec)
18-06-05 03:06-INFO->> Step 469620 run_train: loss = 5.4397  (0.149 sec)
18-06-05 03:06-INFO->> Step 469630 run_train: loss = 5.3815  (0.154 sec)
18-06-05 03:06-INFO->> Step 469640 run_train: loss = 5.3914  (0.114 sec)
18-06-05 03:06-INFO->> Step 469650 run_train: loss = 5.4317  (0.174 sec)
18-06-05 03:06-INFO->> Step 469660 run_train: loss = 5.4130  (0.185 sec)
18-06-05 03:06-INFO->> Step 469670 run_train: loss = 5.4853  (0.151 sec)
18-06-05 03:06-INFO->> Step 469680 run_train: loss = 5.3615  (0.140 sec)
18-06-05 03:06-INFO->> Step 469690 run_train: loss = 5.3876  (0.148 sec)
18-06-05 03:06-INFO->> Step 469700 run_train: loss = 5.3950  (0.159 sec)
18-06-05 03:06-INFO->> Step 469710 run_train: loss = 5.3893  (0.143 sec)
18-06-05 03:06-INFO->> Step 469720 run_train: loss = 5.4262  (0.164 sec)
18-06-05 03:06-INFO->> Step 469730 run_train: loss = 5.3376  (0.154 sec)
18-06-05 03:06-INFO->> Step 469740 run_train: loss = 5.4085  (0.143 sec)
18-06-05 03:07-INFO->> Step 469750 run_train: loss = 5.4089  (0.148 sec)
18-06-05 03:07-INFO->> Step 469760 run_train: loss = 5.4264  (0.122 sec)
18-06-05 03:07-INFO->> Step 469770 run_train: loss = 5.4001  (0.154 sec)
18-06-05 03:07-INFO->> Step 469780 run_train: loss = 5.3866  (0.162 sec)
18-06-05 03:07-INFO->> Step 469790 run_train: loss = 5.3921  (0.153 sec)
18-06-05 03:07-INFO->> Step 469800 run_train: loss = 5.4054  (0.144 sec)
18-06-05 03:07-INFO->> Step 469810 run_train: loss = 5.4482  (0.165 sec)
18-06-05 03:07-INFO->> Step 469820 run_train: loss = 5.3649  (0.168 sec)
18-06-05 03:07-INFO->> Step 469830 run_train: loss = 5.3882  (0.135 sec)
18-06-05 03:07-INFO->> Step 469840 run_train: loss = 5.3785  (0.164 sec)
18-06-05 03:07-INFO->> Step 469850 run_train: loss = 5.4068  (0.156 sec)
18-06-05 03:07-INFO->> Step 469860 run_train: loss = 5.3734  (0.161 sec)
18-06-05 03:07-INFO->> Step 469870 run_train: loss = 5.4710  (0.157 sec)
18-06-05 03:07-INFO->> Step 469880 run_train: loss = 5.3072  (0.119 sec)
18-06-05 03:07-INFO->> Step 469890 run_train: loss = 5.3788  (0.151 sec)
18-06-05 03:07-INFO->> Step 469900 run_train: loss = 5.4501  (0.157 sec)
18-06-05 03:07-INFO->> Step 469910 run_train: loss = 5.4590  (0.166 sec)
18-06-05 03:07-INFO->> Step 469920 run_train: loss = 5.3844  (0.177 sec)
18-06-05 03:07-INFO->> Step 469930 run_train: loss = 5.3893  (0.157 sec)
18-06-05 03:07-INFO->> Step 469940 run_train: loss = 5.4887  (0.172 sec)
18-06-05 03:07-INFO->> Step 469950 run_train: loss = 5.4505  (0.160 sec)
18-06-05 03:07-INFO->> Step 469960 run_train: loss = 5.3985  (0.124 sec)
18-06-05 03:07-INFO->> Step 469970 run_train: loss = 5.3896  (0.122 sec)
18-06-05 03:07-INFO->> Step 469980 run_train: loss = 5.4226  (0.156 sec)
18-06-05 03:07-INFO->> Step 469990 run_train: loss = 5.4165  (0.164 sec)
18-06-05 03:07-INFO->> Step 470000 run_train: loss = 5.3895  (0.171 sec)
18-06-05 03:07-INFO->> 2018-06-05 03:07:41.322468 Saving in ckpt
18-06-05 03:07-INFO-Test Data Eval:
18-06-05 03:08-INFO-fpr95 = 0.1720825584484591 and auc = 0.9691427037276386
18-06-05 03:08-INFO->> Step 470010 run_train: loss = 5.3930  (0.170 sec)
18-06-05 03:08-INFO->> Step 470020 run_train: loss = 5.4499  (0.156 sec)
18-06-05 03:08-INFO->> Step 470030 run_train: loss = 5.3788  (0.146 sec)
18-06-05 03:08-INFO->> Step 470040 run_train: loss = 5.3944  (0.167 sec)
18-06-05 03:08-INFO->> Step 470050 run_train: loss = 5.3971  (0.157 sec)
18-06-05 03:08-INFO->> Step 470060 run_train: loss = 5.4227  (0.160 sec)
18-06-05 03:08-INFO->> Step 470070 run_train: loss = 5.4250  (0.172 sec)
18-06-05 03:08-INFO->> Step 470080 run_train: loss = 5.3869  (0.143 sec)
18-06-05 03:08-INFO->> Step 470090 run_train: loss = 5.3662  (0.164 sec)
18-06-05 03:08-INFO->> Step 470100 run_train: loss = 5.4628  (0.163 sec)
18-06-05 03:08-INFO->> Step 470110 run_train: loss = 5.4288  (0.187 sec)
18-06-05 03:08-INFO->> Step 470120 run_train: loss = 5.4469  (0.143 sec)
18-06-05 03:08-INFO->> Step 470130 run_train: loss = 5.3679  (0.170 sec)
18-06-05 03:08-INFO->> Step 470140 run_train: loss = 5.4520  (0.151 sec)
18-06-05 03:08-INFO->> Step 470150 run_train: loss = 5.4131  (0.129 sec)
18-06-05 03:08-INFO->> Step 470160 run_train: loss = 5.3934  (0.156 sec)
18-06-05 03:08-INFO->> Step 470170 run_train: loss = 5.3779  (0.126 sec)
18-06-05 03:08-INFO->> Step 470180 run_train: loss = 5.4098  (0.158 sec)
18-06-05 03:08-INFO->> Step 470190 run_train: loss = 5.3795  (0.164 sec)
18-06-05 03:08-INFO->> Step 470200 run_train: loss = 5.3837  (0.160 sec)
18-06-05 03:08-INFO->> Step 470210 run_train: loss = 5.4829  (0.158 sec)
18-06-05 03:08-INFO->> Step 470220 run_train: loss = 5.3852  (0.148 sec)
18-06-05 03:08-INFO->> Step 470230 run_train: loss = 5.4439  (0.152 sec)
18-06-05 03:08-INFO->> Step 470240 run_train: loss = 5.4003  (0.165 sec)
18-06-05 03:09-INFO->> Step 470250 run_train: loss = 5.4241  (0.174 sec)
18-06-05 03:09-INFO->> Step 470260 run_train: loss = 5.4236  (0.179 sec)
18-06-05 03:09-INFO->> Step 470270 run_train: loss = 5.3851  (0.170 sec)
18-06-05 03:09-INFO->> Step 470280 run_train: loss = 5.3869  (0.134 sec)
18-06-05 03:09-INFO->> Step 470290 run_train: loss = 5.3890  (0.148 sec)
18-06-05 03:09-INFO->> Step 470300 run_train: loss = 5.3957  (0.173 sec)
18-06-05 03:09-INFO->> Step 470310 run_train: loss = 5.4877  (0.159 sec)
18-06-05 03:09-INFO->> Step 470320 run_train: loss = 5.4394  (0.168 sec)
18-06-05 03:09-INFO->> Step 470330 run_train: loss = 5.4139  (0.183 sec)
18-06-05 03:09-INFO->> Step 470340 run_train: loss = 5.4243  (0.157 sec)
18-06-05 03:09-INFO->> Step 470350 run_train: loss = 5.4055  (0.127 sec)
18-06-05 03:09-INFO->> Step 470360 run_train: loss = 5.4267  (0.146 sec)
18-06-05 03:09-INFO->> Step 470370 run_train: loss = 5.3487  (0.140 sec)
18-06-05 03:09-INFO->> Step 470380 run_train: loss = 5.3846  (0.188 sec)
18-06-05 03:09-INFO->> Step 470390 run_train: loss = 5.3667  (0.150 sec)
18-06-05 03:09-INFO->> Step 470400 run_train: loss = 5.4546  (0.161 sec)
18-06-05 03:09-INFO->> Step 470410 run_train: loss = 5.4090  (0.147 sec)
18-06-05 03:09-INFO->> Step 470420 run_train: loss = 5.4550  (0.141 sec)
18-06-05 03:09-INFO->> Step 470430 run_train: loss = 5.3849  (0.194 sec)
18-06-05 03:09-INFO->> Step 470440 run_train: loss = 5.3920  (0.148 sec)
18-06-05 03:09-INFO->> Step 470450 run_train: loss = 5.3963  (0.161 sec)
18-06-05 03:09-INFO->> Step 470460 run_train: loss = 5.4162  (0.152 sec)
18-06-05 03:09-INFO->> Step 470470 run_train: loss = 5.4746  (0.194 sec)
18-06-05 03:09-INFO->> Step 470480 run_train: loss = 5.4735  (0.160 sec)
18-06-05 03:09-INFO->> Step 470490 run_train: loss = 5.3742  (0.154 sec)
18-06-05 03:09-INFO->> Step 470500 run_train: loss = 5.4809  (0.130 sec)
18-06-05 03:09-INFO->> Step 470510 run_train: loss = 5.4535  (0.179 sec)
18-06-05 03:09-INFO->> Step 470520 run_train: loss = 5.4118  (0.155 sec)
18-06-05 03:09-INFO->> Step 470530 run_train: loss = 5.4662  (0.181 sec)
18-06-05 03:09-INFO->> Step 470540 run_train: loss = 5.4149  (0.172 sec)
18-06-05 03:09-INFO->> Step 470550 run_train: loss = 5.4134  (0.157 sec)
18-06-05 03:09-INFO->> Step 470560 run_train: loss = 5.3898  (0.152 sec)
18-06-05 03:09-INFO->> Step 470570 run_train: loss = 5.3970  (0.159 sec)
18-06-05 03:09-INFO->> Step 470580 run_train: loss = 5.4426  (0.172 sec)
18-06-05 03:09-INFO->> Step 470590 run_train: loss = 5.4232  (0.175 sec)
18-06-05 03:09-INFO->> Step 470600 run_train: loss = 5.4258  (0.157 sec)
18-06-05 03:09-INFO->> Step 470610 run_train: loss = 5.3721  (0.152 sec)
18-06-05 03:09-INFO->> Step 470620 run_train: loss = 5.4089  (0.166 sec)
18-06-05 03:10-INFO->> Step 470630 run_train: loss = 5.3839  (0.141 sec)
18-06-05 03:10-INFO->> Step 470640 run_train: loss = 5.5073  (0.170 sec)
18-06-05 03:10-INFO->> Step 470650 run_train: loss = 5.4262  (0.144 sec)
18-06-05 03:10-INFO->> Step 470660 run_train: loss = 5.4355  (0.167 sec)
18-06-05 03:10-INFO->> Step 470670 run_train: loss = 5.4298  (0.224 sec)
18-06-05 03:10-INFO->> Step 470680 run_train: loss = 5.4673  (0.143 sec)
18-06-05 03:10-INFO->> Step 470690 run_train: loss = 5.4049  (0.149 sec)
18-06-05 03:10-INFO->> Step 470700 run_train: loss = 5.3644  (0.144 sec)
18-06-05 03:10-INFO->> Step 470710 run_train: loss = 5.3709  (0.171 sec)
18-06-05 03:10-INFO->> Step 470720 run_train: loss = 5.3882  (0.157 sec)
18-06-05 03:10-INFO->> Step 470730 run_train: loss = 5.4042  (0.156 sec)
18-06-05 03:10-INFO->> Step 470740 run_train: loss = 5.3721  (0.154 sec)
18-06-05 03:10-INFO->> Step 470750 run_train: loss = 5.4183  (0.169 sec)
18-06-05 03:10-INFO->> Step 470760 run_train: loss = 5.3985  (0.166 sec)
18-06-05 03:10-INFO->> Step 470770 run_train: loss = 5.4394  (0.150 sec)
18-06-05 03:10-INFO->> Step 470780 run_train: loss = 5.4535  (0.153 sec)
18-06-05 03:10-INFO->> Step 470790 run_train: loss = 5.3575  (0.167 sec)
18-06-05 03:10-INFO->> Step 470800 run_train: loss = 5.4105  (0.162 sec)
18-06-05 03:10-INFO->> Step 470810 run_train: loss = 5.5056  (0.131 sec)
18-06-05 03:10-INFO->> Step 470820 run_train: loss = 5.4005  (0.146 sec)
18-06-05 03:10-INFO->> Step 470830 run_train: loss = 5.4801  (0.155 sec)
18-06-05 03:10-INFO->> Step 470840 run_train: loss = 5.4203  (0.165 sec)
18-06-05 03:10-INFO->> Step 470850 run_train: loss = 5.4652  (0.138 sec)
18-06-05 03:10-INFO->> Step 470860 run_train: loss = 5.4925  (0.154 sec)
18-06-05 03:10-INFO->> Step 470870 run_train: loss = 5.3989  (0.153 sec)
18-06-05 03:10-INFO->> Step 470880 run_train: loss = 5.4487  (0.157 sec)
18-06-05 03:10-INFO->> Step 470890 run_train: loss = 5.3996  (0.133 sec)
18-06-05 03:10-INFO->> Step 470900 run_train: loss = 5.4451  (0.144 sec)
18-06-05 03:10-INFO->> Step 470910 run_train: loss = 5.4368  (0.167 sec)
18-06-05 03:10-INFO->> Step 470920 run_train: loss = 5.3826  (0.160 sec)
18-06-05 03:10-INFO->> Step 470930 run_train: loss = 5.4700  (0.140 sec)
18-06-05 03:10-INFO->> Step 470940 run_train: loss = 5.4074  (0.158 sec)
18-06-05 03:10-INFO->> Step 470950 run_train: loss = 5.4209  (0.138 sec)
18-06-05 03:10-INFO->> Step 470960 run_train: loss = 5.4237  (0.174 sec)
18-06-05 03:10-INFO->> Step 470970 run_train: loss = 5.3818  (0.144 sec)
18-06-05 03:10-INFO->> Step 470980 run_train: loss = 5.3892  (0.146 sec)
18-06-05 03:10-INFO->> Step 470990 run_train: loss = 5.3743  (0.157 sec)
18-06-05 03:10-INFO->> Step 471000 run_train: loss = 5.3685  (0.137 sec)
18-06-05 03:10-INFO->> 2018-06-05 03:10:59.670345 Saving in ckpt
18-06-05 03:11-INFO-Test Data Eval:
18-06-05 03:11-INFO-fpr95 = 0.17240634962805526 and auc = 0.9692201396683298
18-06-05 03:11-INFO->> Step 471010 run_train: loss = 5.4048  (0.159 sec)
18-06-05 03:11-INFO->> Step 471020 run_train: loss = 5.3664  (0.154 sec)
18-06-05 03:11-INFO->> Step 471030 run_train: loss = 5.3882  (0.170 sec)
18-06-05 03:11-INFO->> Step 471040 run_train: loss = 5.5152  (0.127 sec)
18-06-05 03:11-INFO->> Step 471050 run_train: loss = 5.4371  (0.180 sec)
18-06-05 03:11-INFO->> Step 471060 run_train: loss = 5.4282  (0.185 sec)
18-06-05 03:11-INFO->> Step 471070 run_train: loss = 5.4740  (0.167 sec)
18-06-05 03:11-INFO->> Step 471080 run_train: loss = 5.3860  (0.166 sec)
18-06-05 03:11-INFO->> Step 471090 run_train: loss = 5.4202  (0.182 sec)
18-06-05 03:11-INFO->> Step 471100 run_train: loss = 5.3685  (0.175 sec)
18-06-05 03:11-INFO->> Step 471110 run_train: loss = 5.4438  (0.191 sec)
18-06-05 03:11-INFO->> Step 471120 run_train: loss = 5.3878  (0.193 sec)
18-06-05 03:12-INFO->> Step 471130 run_train: loss = 5.4007  (0.172 sec)
18-06-05 03:12-INFO->> Step 471140 run_train: loss = 5.3989  (0.153 sec)
18-06-05 03:12-INFO->> Step 471150 run_train: loss = 5.3774  (0.183 sec)
18-06-05 03:12-INFO->> Step 471160 run_train: loss = 5.3861  (0.166 sec)
18-06-05 03:12-INFO->> Step 471170 run_train: loss = 5.4111  (0.164 sec)
18-06-05 03:12-INFO->> Step 471180 run_train: loss = 5.3749  (0.166 sec)
18-06-05 03:12-INFO->> Step 471190 run_train: loss = 5.4012  (0.161 sec)
18-06-05 03:12-INFO->> Step 471200 run_train: loss = 5.4599  (0.144 sec)
18-06-05 03:12-INFO->> Step 471210 run_train: loss = 5.4859  (0.160 sec)
18-06-05 03:12-INFO->> Step 471220 run_train: loss = 5.4108  (0.142 sec)
18-06-05 03:12-INFO->> Step 471230 run_train: loss = 5.4318  (0.139 sec)
18-06-05 03:12-INFO->> Step 471240 run_train: loss = 5.3937  (0.161 sec)
18-06-05 03:12-INFO->> Step 471250 run_train: loss = 5.4647  (0.160 sec)
18-06-05 03:12-INFO->> Step 471260 run_train: loss = 5.3587  (0.150 sec)
18-06-05 03:12-INFO->> Step 471270 run_train: loss = 5.4266  (0.149 sec)
18-06-05 03:12-INFO->> Step 471280 run_train: loss = 5.3881  (0.162 sec)
18-06-05 03:12-INFO->> Step 471290 run_train: loss = 5.3506  (0.170 sec)
18-06-05 03:12-INFO->> Step 471300 run_train: loss = 5.3527  (0.147 sec)
18-06-05 03:12-INFO->> Step 471310 run_train: loss = 5.3873  (0.132 sec)
18-06-05 03:12-INFO->> Step 471320 run_train: loss = 5.3978  (0.239 sec)
18-06-05 03:12-INFO->> Step 471330 run_train: loss = 5.3803  (0.141 sec)
18-06-05 03:12-INFO->> Step 471340 run_train: loss = 5.4338  (0.144 sec)
18-06-05 03:12-INFO->> Step 471350 run_train: loss = 5.3663  (0.190 sec)
18-06-05 03:12-INFO->> Step 471360 run_train: loss = 5.4068  (0.149 sec)
18-06-05 03:12-INFO->> Step 471370 run_train: loss = 5.3579  (0.152 sec)
18-06-05 03:12-INFO->> Step 471380 run_train: loss = 5.3312  (0.144 sec)
18-06-05 03:12-INFO->> Step 471390 run_train: loss = 5.4762  (0.163 sec)
18-06-05 03:12-INFO->> Step 471400 run_train: loss = 5.4063  (0.160 sec)
18-06-05 03:12-INFO->> Step 471410 run_train: loss = 5.4105  (0.176 sec)
18-06-05 03:12-INFO->> Step 471420 run_train: loss = 5.4456  (0.119 sec)
18-06-05 03:12-INFO->> Step 471430 run_train: loss = 5.3497  (0.162 sec)
18-06-05 03:12-INFO->> Step 471440 run_train: loss = 5.3910  (0.179 sec)
18-06-05 03:12-INFO->> Step 471450 run_train: loss = 5.3585  (0.168 sec)
18-06-05 03:12-INFO->> Step 471460 run_train: loss = 5.3307  (0.157 sec)
18-06-05 03:12-INFO->> Step 471470 run_train: loss = 5.4858  (0.151 sec)
18-06-05 03:12-INFO->> Step 471480 run_train: loss = 5.4658  (0.147 sec)
18-06-05 03:12-INFO->> Step 471490 run_train: loss = 5.3581  (0.138 sec)
18-06-05 03:12-INFO->> Step 471500 run_train: loss = 5.4330  (0.147 sec)
18-06-05 03:13-INFO->> Step 471510 run_train: loss = 5.3835  (0.156 sec)
18-06-05 03:13-INFO->> Step 471520 run_train: loss = 5.3866  (0.151 sec)
18-06-05 03:13-INFO->> Step 471530 run_train: loss = 5.3961  (0.157 sec)
18-06-05 03:13-INFO->> Step 471540 run_train: loss = 5.3859  (0.125 sec)
18-06-05 03:13-INFO->> Step 471550 run_train: loss = 5.3890  (0.161 sec)
18-06-05 03:13-INFO->> Step 471560 run_train: loss = 5.3762  (0.126 sec)
18-06-05 03:13-INFO->> Step 471570 run_train: loss = 5.3958  (0.133 sec)
18-06-05 03:13-INFO->> Step 471580 run_train: loss = 5.4244  (0.140 sec)
18-06-05 03:13-INFO->> Step 471590 run_train: loss = 5.3830  (0.186 sec)
18-06-05 03:13-INFO->> Step 471600 run_train: loss = 5.4080  (0.144 sec)
18-06-05 03:13-INFO->> Step 471610 run_train: loss = 5.3782  (0.161 sec)
18-06-05 03:13-INFO->> Step 471620 run_train: loss = 5.3925  (0.114 sec)
18-06-05 03:13-INFO->> Step 471630 run_train: loss = 5.3633  (0.137 sec)
18-06-05 03:13-INFO->> Step 471640 run_train: loss = 5.4346  (0.152 sec)
18-06-05 03:13-INFO->> Step 471650 run_train: loss = 5.4359  (0.157 sec)
18-06-05 03:13-INFO->> Step 471660 run_train: loss = 5.3898  (0.145 sec)
18-06-05 03:13-INFO->> Step 471670 run_train: loss = 5.4631  (0.147 sec)
18-06-05 03:13-INFO->> Step 471680 run_train: loss = 5.4057  (0.151 sec)
18-06-05 03:13-INFO->> Step 471690 run_train: loss = 5.4157  (0.158 sec)
18-06-05 03:13-INFO->> Step 471700 run_train: loss = 5.4140  (0.176 sec)
18-06-05 03:13-INFO->> Step 471710 run_train: loss = 5.3696  (0.160 sec)
18-06-05 03:13-INFO->> Step 471720 run_train: loss = 5.3797  (0.129 sec)
18-06-05 03:13-INFO->> Step 471730 run_train: loss = 5.3996  (0.154 sec)
18-06-05 03:13-INFO->> Step 471740 run_train: loss = 5.3814  (0.153 sec)
18-06-05 03:13-INFO->> Step 471750 run_train: loss = 5.4526  (0.156 sec)
18-06-05 03:13-INFO->> Step 471760 run_train: loss = 5.3716  (0.201 sec)
18-06-05 03:13-INFO->> Step 471770 run_train: loss = 5.3737  (0.155 sec)
18-06-05 03:13-INFO->> Step 471780 run_train: loss = 5.4414  (0.128 sec)
18-06-05 03:13-INFO->> Step 471790 run_train: loss = 5.4061  (0.210 sec)
18-06-05 03:13-INFO->> Step 471800 run_train: loss = 5.4354  (0.124 sec)
18-06-05 03:13-INFO->> Step 471810 run_train: loss = 5.3590  (0.157 sec)
18-06-05 03:13-INFO->> Step 471820 run_train: loss = 5.4451  (0.161 sec)
18-06-05 03:13-INFO->> Step 471830 run_train: loss = 5.3465  (0.133 sec)
18-06-05 03:13-INFO->> Step 471840 run_train: loss = 5.4066  (0.164 sec)
18-06-05 03:13-INFO->> Step 471850 run_train: loss = 5.4275  (0.152 sec)
18-06-05 03:13-INFO->> Step 471860 run_train: loss = 5.5102  (0.166 sec)
18-06-05 03:13-INFO->> Step 471870 run_train: loss = 5.3474  (0.162 sec)
18-06-05 03:13-INFO->> Step 471880 run_train: loss = 5.4190  (0.180 sec)
18-06-05 03:14-INFO->> Step 471890 run_train: loss = 5.4401  (0.184 sec)
18-06-05 03:14-INFO->> Step 471900 run_train: loss = 5.3751  (0.131 sec)
18-06-05 03:14-INFO->> Step 471910 run_train: loss = 5.4237  (0.166 sec)
18-06-05 03:14-INFO->> Step 471920 run_train: loss = 5.4043  (0.156 sec)
18-06-05 03:14-INFO->> Step 471930 run_train: loss = 5.4287  (0.127 sec)
18-06-05 03:14-INFO->> Step 471940 run_train: loss = 5.4273  (0.148 sec)
18-06-05 03:14-INFO->> Step 471950 run_train: loss = 5.3991  (0.157 sec)
18-06-05 03:14-INFO->> Step 471960 run_train: loss = 5.4354  (0.150 sec)
18-06-05 03:14-INFO->> Step 471970 run_train: loss = 5.4261  (0.152 sec)
18-06-05 03:14-INFO->> Step 471980 run_train: loss = 5.4030  (0.153 sec)
18-06-05 03:14-INFO->> Step 471990 run_train: loss = 5.3799  (0.151 sec)
18-06-05 03:14-INFO->> Step 472000 run_train: loss = 5.3589  (0.135 sec)
18-06-05 03:14-INFO->> 2018-06-05 03:14:18.372282 Saving in ckpt
18-06-05 03:14-INFO-Test Data Eval:
18-06-05 03:14-INFO-fpr95 = 0.17590163390010627 and auc = 0.9690893139487888
18-06-05 03:14-INFO->> Step 472010 run_train: loss = 5.3425  (0.164 sec)
18-06-05 03:15-INFO->> Step 472020 run_train: loss = 5.4598  (0.168 sec)
18-06-05 03:15-INFO->> Step 472030 run_train: loss = 5.3776  (0.152 sec)
18-06-05 03:15-INFO->> Step 472040 run_train: loss = 5.3820  (0.163 sec)
18-06-05 03:15-INFO->> Step 472050 run_train: loss = 5.3440  (0.137 sec)
18-06-05 03:15-INFO->> Step 472060 run_train: loss = 5.4162  (0.151 sec)
18-06-05 03:15-INFO->> Step 472070 run_train: loss = 5.3836  (0.190 sec)
18-06-05 03:15-INFO->> Step 472080 run_train: loss = 5.3444  (0.158 sec)
18-06-05 03:15-INFO->> Step 472090 run_train: loss = 5.4491  (0.144 sec)
18-06-05 03:15-INFO->> Step 472100 run_train: loss = 5.3150  (0.152 sec)
18-06-05 03:15-INFO->> Step 472110 run_train: loss = 5.3511  (0.152 sec)
18-06-05 03:15-INFO->> Step 472120 run_train: loss = 5.4498  (0.164 sec)
18-06-05 03:15-INFO->> Step 472130 run_train: loss = 5.4067  (0.121 sec)
18-06-05 03:15-INFO->> Step 472140 run_train: loss = 5.4410  (0.211 sec)
18-06-05 03:15-INFO->> Step 472150 run_train: loss = 5.3742  (0.146 sec)
18-06-05 03:15-INFO->> Step 472160 run_train: loss = 5.4452  (0.159 sec)
18-06-05 03:15-INFO->> Step 472170 run_train: loss = 5.3969  (0.214 sec)
18-06-05 03:15-INFO->> Step 472180 run_train: loss = 5.3737  (0.136 sec)
18-06-05 03:15-INFO->> Step 472190 run_train: loss = 5.3816  (0.128 sec)
18-06-05 03:15-INFO->> Step 472200 run_train: loss = 5.4174  (0.173 sec)
18-06-05 03:15-INFO->> Step 472210 run_train: loss = 5.4025  (0.153 sec)
18-06-05 03:15-INFO->> Step 472220 run_train: loss = 5.4011  (0.188 sec)
18-06-05 03:15-INFO->> Step 472230 run_train: loss = 5.4177  (0.193 sec)
18-06-05 03:15-INFO->> Step 472240 run_train: loss = 5.4731  (0.160 sec)
18-06-05 03:15-INFO->> Step 472250 run_train: loss = 5.3965  (0.143 sec)
18-06-05 03:15-INFO->> Step 472260 run_train: loss = 5.4098  (0.187 sec)
18-06-05 03:15-INFO->> Step 472270 run_train: loss = 5.4236  (0.187 sec)
18-06-05 03:15-INFO->> Step 472280 run_train: loss = 5.3843  (0.148 sec)
18-06-05 03:15-INFO->> Step 472290 run_train: loss = 5.4417  (0.179 sec)
18-06-05 03:15-INFO->> Step 472300 run_train: loss = 5.3235  (0.144 sec)
18-06-05 03:15-INFO->> Step 472310 run_train: loss = 5.4239  (0.153 sec)
18-06-05 03:15-INFO->> Step 472320 run_train: loss = 5.3703  (0.158 sec)
18-06-05 03:15-INFO->> Step 472330 run_train: loss = 5.4466  (0.162 sec)
18-06-05 03:15-INFO->> Step 472340 run_train: loss = 5.4447  (0.149 sec)
18-06-05 03:15-INFO->> Step 472350 run_train: loss = 5.3872  (0.189 sec)
18-06-05 03:15-INFO->> Step 472360 run_train: loss = 5.3493  (0.164 sec)
18-06-05 03:15-INFO->> Step 472370 run_train: loss = 5.3679  (0.144 sec)
18-06-05 03:15-INFO->> Step 472380 run_train: loss = 5.4382  (0.153 sec)
18-06-05 03:16-INFO->> Step 472390 run_train: loss = 5.3528  (0.154 sec)
18-06-05 03:16-INFO->> Step 472400 run_train: loss = 5.3365  (0.160 sec)
18-06-05 03:16-INFO->> Step 472410 run_train: loss = 5.4251  (0.151 sec)
18-06-05 03:16-INFO->> Step 472420 run_train: loss = 5.4310  (0.158 sec)
18-06-05 03:16-INFO->> Step 472430 run_train: loss = 5.4235  (0.160 sec)
18-06-05 03:16-INFO->> Step 472440 run_train: loss = 5.4251  (0.148 sec)
18-06-05 03:16-INFO->> Step 472450 run_train: loss = 5.3665  (0.187 sec)
18-06-05 03:16-INFO->> Step 472460 run_train: loss = 5.4563  (0.149 sec)
18-06-05 03:16-INFO->> Step 472470 run_train: loss = 5.3581  (0.168 sec)
18-06-05 03:16-INFO->> Step 472480 run_train: loss = 5.4063  (0.171 sec)
18-06-05 03:16-INFO->> Step 472490 run_train: loss = 5.4415  (0.170 sec)
18-06-05 03:16-INFO->> Step 472500 run_train: loss = 5.4870  (0.150 sec)
18-06-05 03:16-INFO->> Step 472510 run_train: loss = 5.4126  (0.159 sec)
18-06-05 03:16-INFO->> Step 472520 run_train: loss = 5.3849  (0.170 sec)
18-06-05 03:16-INFO->> Step 472530 run_train: loss = 5.4051  (0.196 sec)
18-06-05 03:16-INFO->> Step 472540 run_train: loss = 5.3726  (0.142 sec)
18-06-05 03:16-INFO->> Step 472550 run_train: loss = 5.4204  (0.174 sec)
18-06-05 03:16-INFO->> Step 472560 run_train: loss = 5.4198  (0.120 sec)
18-06-05 03:16-INFO->> Step 472570 run_train: loss = 5.3991  (0.165 sec)
18-06-05 03:16-INFO->> Step 472580 run_train: loss = 5.3562  (0.169 sec)
18-06-05 03:16-INFO->> Step 472590 run_train: loss = 5.3733  (0.149 sec)
18-06-05 03:16-INFO->> Step 472600 run_train: loss = 5.3561  (0.172 sec)
18-06-05 03:16-INFO->> Step 472610 run_train: loss = 5.3432  (0.175 sec)
18-06-05 03:16-INFO->> Step 472620 run_train: loss = 5.4146  (0.155 sec)
18-06-05 03:16-INFO->> Step 472630 run_train: loss = 5.4478  (0.151 sec)
18-06-05 03:16-INFO->> Step 472640 run_train: loss = 5.3859  (0.154 sec)
18-06-05 03:16-INFO->> Step 472650 run_train: loss = 5.3766  (0.152 sec)
18-06-05 03:16-INFO->> Step 472660 run_train: loss = 5.3827  (0.159 sec)
18-06-05 03:16-INFO->> Step 472670 run_train: loss = 5.3654  (0.170 sec)
18-06-05 03:16-INFO->> Step 472680 run_train: loss = 5.4722  (0.123 sec)
18-06-05 03:16-INFO->> Step 472690 run_train: loss = 5.3298  (0.170 sec)
18-06-05 03:16-INFO->> Step 472700 run_train: loss = 5.4621  (0.150 sec)
18-06-05 03:16-INFO->> Step 472710 run_train: loss = 5.4707  (0.147 sec)
18-06-05 03:16-INFO->> Step 472720 run_train: loss = 5.4516  (0.194 sec)
18-06-05 03:16-INFO->> Step 472730 run_train: loss = 5.4074  (0.229 sec)
18-06-05 03:16-INFO->> Step 472740 run_train: loss = 5.3851  (0.159 sec)
18-06-05 03:16-INFO->> Step 472750 run_train: loss = 5.3822  (0.175 sec)
18-06-05 03:16-INFO->> Step 472760 run_train: loss = 5.4111  (0.173 sec)
18-06-05 03:17-INFO->> Step 472770 run_train: loss = 5.4593  (0.142 sec)
18-06-05 03:17-INFO->> Step 472780 run_train: loss = 5.4854  (0.177 sec)
18-06-05 03:17-INFO->> Step 472790 run_train: loss = 5.4264  (0.160 sec)
18-06-05 03:17-INFO->> Step 472800 run_train: loss = 5.3985  (0.176 sec)
18-06-05 03:17-INFO->> Step 472810 run_train: loss = 5.4453  (0.150 sec)
18-06-05 03:17-INFO->> Step 472820 run_train: loss = 5.3971  (0.167 sec)
18-06-05 03:17-INFO->> Step 472830 run_train: loss = 5.3864  (0.167 sec)
18-06-05 03:17-INFO->> Step 472840 run_train: loss = 5.4239  (0.167 sec)
18-06-05 03:17-INFO->> Step 472850 run_train: loss = 5.3543  (0.146 sec)
18-06-05 03:17-INFO->> Step 472860 run_train: loss = 5.3381  (0.146 sec)
18-06-05 03:17-INFO->> Step 472870 run_train: loss = 5.4482  (0.178 sec)
18-06-05 03:17-INFO->> Step 472880 run_train: loss = 5.4332  (0.161 sec)
18-06-05 03:17-INFO->> Step 472890 run_train: loss = 5.4498  (0.138 sec)
18-06-05 03:17-INFO->> Step 472900 run_train: loss = 5.3194  (0.166 sec)
18-06-05 03:17-INFO->> Step 472910 run_train: loss = 5.4015  (0.156 sec)
18-06-05 03:17-INFO->> Step 472920 run_train: loss = 5.4350  (0.154 sec)
18-06-05 03:17-INFO->> Step 472930 run_train: loss = 5.3633  (0.144 sec)
18-06-05 03:17-INFO->> Step 472940 run_train: loss = 5.4458  (0.162 sec)
18-06-05 03:17-INFO->> Step 472950 run_train: loss = 5.3718  (0.147 sec)
18-06-05 03:17-INFO->> Step 472960 run_train: loss = 5.4123  (0.158 sec)
18-06-05 03:17-INFO->> Step 472970 run_train: loss = 5.3820  (0.160 sec)
18-06-05 03:17-INFO->> Step 472980 run_train: loss = 5.4833  (0.158 sec)
18-06-05 03:17-INFO->> Step 472990 run_train: loss = 5.4197  (0.143 sec)
18-06-05 03:17-INFO->> Step 473000 run_train: loss = 5.3984  (0.149 sec)
18-06-05 03:17-INFO->> 2018-06-05 03:17:37.284717 Saving in ckpt
18-06-05 03:17-INFO-Test Data Eval:
18-06-05 03:18-INFO-fpr95 = 0.175552935706695 and auc = 0.9689110864689005
18-06-05 03:18-INFO->> Step 473010 run_train: loss = 5.4111  (0.157 sec)
18-06-05 03:18-INFO->> Step 473020 run_train: loss = 5.4423  (0.167 sec)
18-06-05 03:18-INFO->> Step 473030 run_train: loss = 5.3899  (0.145 sec)
18-06-05 03:18-INFO->> Step 473040 run_train: loss = 5.4064  (0.122 sec)
18-06-05 03:18-INFO->> Step 473050 run_train: loss = 5.3606  (0.141 sec)
18-06-05 03:18-INFO->> Step 473060 run_train: loss = 5.3926  (0.189 sec)
18-06-05 03:18-INFO->> Step 473070 run_train: loss = 5.4332  (0.158 sec)
18-06-05 03:18-INFO->> Step 473080 run_train: loss = 5.4115  (0.176 sec)
18-06-05 03:18-INFO->> Step 473090 run_train: loss = 5.4586  (0.167 sec)
18-06-05 03:18-INFO->> Step 473100 run_train: loss = 5.4102  (0.149 sec)
18-06-05 03:18-INFO->> Step 473110 run_train: loss = 5.3840  (0.172 sec)
18-06-05 03:18-INFO->> Step 473120 run_train: loss = 5.4352  (0.136 sec)
18-06-05 03:18-INFO->> Step 473130 run_train: loss = 5.4157  (0.148 sec)
18-06-05 03:18-INFO->> Step 473140 run_train: loss = 5.3413  (0.163 sec)
18-06-05 03:18-INFO->> Step 473150 run_train: loss = 5.4724  (0.163 sec)
18-06-05 03:18-INFO->> Step 473160 run_train: loss = 5.3748  (0.159 sec)
18-06-05 03:18-INFO->> Step 473170 run_train: loss = 5.3764  (0.162 sec)
18-06-05 03:18-INFO->> Step 473180 run_train: loss = 5.5151  (0.152 sec)
18-06-05 03:18-INFO->> Step 473190 run_train: loss = 5.4405  (0.162 sec)
18-06-05 03:18-INFO->> Step 473200 run_train: loss = 5.4341  (0.171 sec)
18-06-05 03:18-INFO->> Step 473210 run_train: loss = 5.4595  (0.152 sec)
18-06-05 03:18-INFO->> Step 473220 run_train: loss = 5.4352  (0.139 sec)
18-06-05 03:18-INFO->> Step 473230 run_train: loss = 5.4157  (0.135 sec)
18-06-05 03:18-INFO->> Step 473240 run_train: loss = 5.3529  (0.141 sec)
18-06-05 03:18-INFO->> Step 473250 run_train: loss = 5.4510  (0.151 sec)
18-06-05 03:18-INFO->> Step 473260 run_train: loss = 5.4430  (0.158 sec)
18-06-05 03:19-INFO->> Step 473270 run_train: loss = 5.4165  (0.163 sec)
18-06-05 03:19-INFO->> Step 473280 run_train: loss = 5.3579  (0.152 sec)
18-06-05 03:19-INFO->> Step 473290 run_train: loss = 5.3673  (0.167 sec)
18-06-05 03:19-INFO->> Step 473300 run_train: loss = 5.4481  (0.138 sec)
18-06-05 03:19-INFO->> Step 473310 run_train: loss = 5.3894  (0.147 sec)
18-06-05 03:19-INFO->> Step 473320 run_train: loss = 5.3995  (0.140 sec)
18-06-05 03:19-INFO->> Step 473330 run_train: loss = 5.3823  (0.128 sec)
18-06-05 03:19-INFO->> Step 473340 run_train: loss = 5.3934  (0.163 sec)
18-06-05 03:19-INFO->> Step 473350 run_train: loss = 5.4433  (0.151 sec)
18-06-05 03:19-INFO->> Step 473360 run_train: loss = 5.4945  (0.164 sec)
18-06-05 03:19-INFO->> Step 473370 run_train: loss = 5.4940  (0.194 sec)
18-06-05 03:19-INFO->> Step 473380 run_train: loss = 5.3494  (0.189 sec)
18-06-05 03:19-INFO->> Step 473390 run_train: loss = 5.3485  (0.177 sec)
18-06-05 03:19-INFO->> Step 473400 run_train: loss = 5.4420  (0.152 sec)
18-06-05 03:19-INFO->> Step 473410 run_train: loss = 5.4166  (0.166 sec)
18-06-05 03:19-INFO->> Step 473420 run_train: loss = 5.4047  (0.176 sec)
18-06-05 03:19-INFO->> Step 473430 run_train: loss = 5.3783  (0.165 sec)
18-06-05 03:19-INFO->> Step 473440 run_train: loss = 5.3888  (0.155 sec)
18-06-05 03:19-INFO->> Step 473450 run_train: loss = 5.4227  (0.169 sec)
18-06-05 03:19-INFO->> Step 473460 run_train: loss = 5.4093  (0.184 sec)
18-06-05 03:19-INFO->> Step 473470 run_train: loss = 5.3871  (0.122 sec)
18-06-05 03:19-INFO->> Step 473480 run_train: loss = 5.3974  (0.166 sec)
18-06-05 03:19-INFO->> Step 473490 run_train: loss = 5.3758  (0.144 sec)
18-06-05 03:19-INFO->> Step 473500 run_train: loss = 5.4321  (0.178 sec)
18-06-05 03:19-INFO->> Step 473510 run_train: loss = 5.4114  (0.166 sec)
18-06-05 03:19-INFO->> Step 473520 run_train: loss = 5.4578  (0.208 sec)
18-06-05 03:19-INFO->> Step 473530 run_train: loss = 5.3455  (0.145 sec)
18-06-05 03:19-INFO->> Step 473540 run_train: loss = 5.3957  (0.157 sec)
18-06-05 03:19-INFO->> Step 473550 run_train: loss = 5.3826  (0.174 sec)
18-06-05 03:19-INFO->> Step 473560 run_train: loss = 5.4328  (0.141 sec)
18-06-05 03:19-INFO->> Step 473570 run_train: loss = 5.4252  (0.152 sec)
18-06-05 03:19-INFO->> Step 473580 run_train: loss = 5.4565  (0.198 sec)
18-06-05 03:19-INFO->> Step 473590 run_train: loss = 5.3928  (0.147 sec)
18-06-05 03:19-INFO->> Step 473600 run_train: loss = 5.3772  (0.157 sec)
18-06-05 03:19-INFO->> Step 473610 run_train: loss = 5.4636  (0.182 sec)
18-06-05 03:19-INFO->> Step 473620 run_train: loss = 5.4069  (0.173 sec)
18-06-05 03:19-INFO->> Step 473630 run_train: loss = 5.4167  (0.159 sec)
18-06-05 03:19-INFO->> Step 473640 run_train: loss = 5.3668  (0.167 sec)
18-06-05 03:20-INFO->> Step 473650 run_train: loss = 5.4425  (0.150 sec)
18-06-05 03:20-INFO->> Step 473660 run_train: loss = 5.4089  (0.162 sec)
18-06-05 03:20-INFO->> Step 473670 run_train: loss = 5.4759  (0.165 sec)
18-06-05 03:20-INFO->> Step 473680 run_train: loss = 5.3599  (0.157 sec)
18-06-05 03:20-INFO->> Step 473690 run_train: loss = 5.4145  (0.152 sec)
18-06-05 03:20-INFO->> Step 473700 run_train: loss = 5.3923  (0.134 sec)
18-06-05 03:20-INFO->> Step 473710 run_train: loss = 5.4149  (0.158 sec)
18-06-05 03:20-INFO->> Step 473720 run_train: loss = 5.4356  (0.171 sec)
18-06-05 03:20-INFO->> Step 473730 run_train: loss = 5.4446  (0.168 sec)
18-06-05 03:20-INFO->> Step 473740 run_train: loss = 5.4016  (0.153 sec)
18-06-05 03:20-INFO->> Step 473750 run_train: loss = 5.4383  (0.165 sec)
18-06-05 03:20-INFO->> Step 473760 run_train: loss = 5.4472  (0.169 sec)
18-06-05 03:20-INFO->> Step 473770 run_train: loss = 5.3514  (0.143 sec)
18-06-05 03:20-INFO->> Step 473780 run_train: loss = 5.4220  (0.131 sec)
18-06-05 03:20-INFO->> Step 473790 run_train: loss = 5.4787  (0.173 sec)
18-06-05 03:20-INFO->> Step 473800 run_train: loss = 5.3621  (0.149 sec)
18-06-05 03:20-INFO->> Step 473810 run_train: loss = 5.4623  (0.155 sec)
18-06-05 03:20-INFO->> Step 473820 run_train: loss = 5.4472  (0.159 sec)
18-06-05 03:20-INFO->> Step 473830 run_train: loss = 5.3714  (0.163 sec)
18-06-05 03:20-INFO->> Step 473840 run_train: loss = 5.4113  (0.165 sec)
18-06-05 03:20-INFO->> Step 473850 run_train: loss = 5.3717  (0.140 sec)
18-06-05 03:20-INFO->> Step 473860 run_train: loss = 5.4120  (0.135 sec)
18-06-05 03:20-INFO->> Step 473870 run_train: loss = 5.4572  (0.206 sec)
18-06-05 03:20-INFO->> Step 473880 run_train: loss = 5.4055  (0.159 sec)
18-06-05 03:20-INFO->> Step 473890 run_train: loss = 5.4170  (0.149 sec)
18-06-05 03:20-INFO->> Step 473900 run_train: loss = 5.4188  (0.144 sec)
18-06-05 03:20-INFO->> Step 473910 run_train: loss = 5.4536  (0.159 sec)
18-06-05 03:20-INFO->> Step 473920 run_train: loss = 5.4572  (0.132 sec)
18-06-05 03:20-INFO->> Step 473930 run_train: loss = 5.3470  (0.152 sec)
18-06-05 03:20-INFO->> Step 473940 run_train: loss = 5.3774  (0.127 sec)
18-06-05 03:20-INFO->> Step 473950 run_train: loss = 5.3560  (0.158 sec)
18-06-05 03:20-INFO->> Step 473960 run_train: loss = 5.3896  (0.174 sec)
18-06-05 03:20-INFO->> Step 473970 run_train: loss = 5.4055  (0.166 sec)
18-06-05 03:20-INFO->> Step 473980 run_train: loss = 5.4337  (0.182 sec)
18-06-05 03:20-INFO->> Step 473990 run_train: loss = 5.4247  (0.140 sec)
18-06-05 03:20-INFO->> Step 474000 run_train: loss = 5.4113  (0.168 sec)
18-06-05 03:20-INFO->> 2018-06-05 03:20:55.904588 Saving in ckpt
18-06-05 03:20-INFO-Test Data Eval:
18-06-05 03:21-INFO-fpr95 = 0.17834252125398511 and auc = 0.968928511604465
18-06-05 03:21-INFO->> Step 474010 run_train: loss = 5.3920  (0.150 sec)
18-06-05 03:21-INFO->> Step 474020 run_train: loss = 5.3870  (0.160 sec)
18-06-05 03:21-INFO->> Step 474030 run_train: loss = 5.4115  (0.167 sec)
18-06-05 03:21-INFO->> Step 474040 run_train: loss = 5.3886  (0.180 sec)
18-06-05 03:21-INFO->> Step 474050 run_train: loss = 5.4071  (0.132 sec)
18-06-05 03:21-INFO->> Step 474060 run_train: loss = 5.3665  (0.120 sec)
18-06-05 03:21-INFO->> Step 474070 run_train: loss = 5.3825  (0.155 sec)
18-06-05 03:21-INFO->> Step 474080 run_train: loss = 5.4440  (0.167 sec)
18-06-05 03:21-INFO->> Step 474090 run_train: loss = 5.4097  (0.153 sec)
18-06-05 03:21-INFO->> Step 474100 run_train: loss = 5.4167  (0.173 sec)
18-06-05 03:21-INFO->> Step 474110 run_train: loss = 5.4306  (0.192 sec)
18-06-05 03:21-INFO->> Step 474120 run_train: loss = 5.4629  (0.162 sec)
18-06-05 03:21-INFO->> Step 474130 run_train: loss = 5.3850  (0.157 sec)
18-06-05 03:21-INFO->> Step 474140 run_train: loss = 5.4443  (0.149 sec)
18-06-05 03:21-INFO->> Step 474150 run_train: loss = 5.4013  (0.154 sec)
18-06-05 03:22-INFO->> Step 474160 run_train: loss = 5.3386  (0.156 sec)
18-06-05 03:22-INFO->> Step 474170 run_train: loss = 5.3542  (0.160 sec)
18-06-05 03:22-INFO->> Step 474180 run_train: loss = 5.4245  (0.193 sec)
18-06-05 03:22-INFO->> Step 474190 run_train: loss = 5.3966  (0.144 sec)
18-06-05 03:22-INFO->> Step 474200 run_train: loss = 5.3748  (0.150 sec)
18-06-05 03:22-INFO->> Step 474210 run_train: loss = 5.4343  (0.141 sec)
18-06-05 03:22-INFO->> Step 474220 run_train: loss = 5.4007  (0.137 sec)
18-06-05 03:22-INFO->> Step 474230 run_train: loss = 5.4057  (0.185 sec)
18-06-05 03:22-INFO->> Step 474240 run_train: loss = 5.3684  (0.132 sec)
18-06-05 03:22-INFO->> Step 474250 run_train: loss = 5.3708  (0.120 sec)
18-06-05 03:22-INFO->> Step 474260 run_train: loss = 5.4721  (0.164 sec)
18-06-05 03:22-INFO->> Step 474270 run_train: loss = 5.3706  (0.168 sec)
18-06-05 03:22-INFO->> Step 474280 run_train: loss = 5.3574  (0.142 sec)
18-06-05 03:22-INFO->> Step 474290 run_train: loss = 5.4277  (0.166 sec)
18-06-05 03:22-INFO->> Step 474300 run_train: loss = 5.3705  (0.120 sec)
18-06-05 03:22-INFO->> Step 474310 run_train: loss = 5.3688  (0.157 sec)
18-06-05 03:22-INFO->> Step 474320 run_train: loss = 5.4610  (0.182 sec)
18-06-05 03:22-INFO->> Step 474330 run_train: loss = 5.4612  (0.139 sec)
18-06-05 03:22-INFO->> Step 474340 run_train: loss = 5.4142  (0.138 sec)
18-06-05 03:22-INFO->> Step 474350 run_train: loss = 5.3940  (0.195 sec)
18-06-05 03:22-INFO->> Step 474360 run_train: loss = 5.3802  (0.193 sec)
18-06-05 03:22-INFO->> Step 474370 run_train: loss = 5.3998  (0.156 sec)
18-06-05 03:22-INFO->> Step 474380 run_train: loss = 5.3516  (0.189 sec)
18-06-05 03:22-INFO->> Step 474390 run_train: loss = 5.3608  (0.150 sec)
18-06-05 03:22-INFO->> Step 474400 run_train: loss = 5.3471  (0.185 sec)
18-06-05 03:22-INFO->> Step 474410 run_train: loss = 5.4504  (0.178 sec)
18-06-05 03:22-INFO->> Step 474420 run_train: loss = 5.4332  (0.177 sec)
18-06-05 03:22-INFO->> Step 474430 run_train: loss = 5.4188  (0.205 sec)
18-06-05 03:22-INFO->> Step 474440 run_train: loss = 5.4238  (0.160 sec)
18-06-05 03:22-INFO->> Step 474450 run_train: loss = 5.4556  (0.146 sec)
18-06-05 03:22-INFO->> Step 474460 run_train: loss = 5.3716  (0.152 sec)
18-06-05 03:22-INFO->> Step 474470 run_train: loss = 5.4622  (0.203 sec)
18-06-05 03:22-INFO->> Step 474480 run_train: loss = 5.4396  (0.144 sec)
18-06-05 03:22-INFO->> Step 474490 run_train: loss = 5.3508  (0.106 sec)
18-06-05 03:22-INFO->> Step 474500 run_train: loss = 5.4625  (0.146 sec)
18-06-05 03:22-INFO->> Step 474510 run_train: loss = 5.3876  (0.152 sec)
18-06-05 03:22-INFO->> Step 474520 run_train: loss = 5.3965  (0.141 sec)
18-06-05 03:23-INFO->> Step 474530 run_train: loss = 5.4662  (0.135 sec)
18-06-05 03:23-INFO->> Step 474540 run_train: loss = 5.4066  (0.144 sec)
18-06-05 03:23-INFO->> Step 474550 run_train: loss = 5.4275  (0.187 sec)
18-06-05 03:23-INFO->> Step 474560 run_train: loss = 5.4378  (0.175 sec)
18-06-05 03:23-INFO->> Step 474570 run_train: loss = 5.3300  (0.152 sec)
18-06-05 03:23-INFO->> Step 474580 run_train: loss = 5.4272  (0.156 sec)
18-06-05 03:23-INFO->> Step 474590 run_train: loss = 5.4644  (0.163 sec)
18-06-05 03:23-INFO->> Step 474600 run_train: loss = 5.3826  (0.179 sec)
18-06-05 03:23-INFO->> Step 474610 run_train: loss = 5.3758  (0.188 sec)
18-06-05 03:23-INFO->> Step 474620 run_train: loss = 5.4021  (0.182 sec)
18-06-05 03:23-INFO->> Step 474630 run_train: loss = 5.3827  (0.172 sec)
18-06-05 03:23-INFO->> Step 474640 run_train: loss = 5.4214  (0.159 sec)
18-06-05 03:23-INFO->> Step 474650 run_train: loss = 5.4013  (0.137 sec)
18-06-05 03:23-INFO->> Step 474660 run_train: loss = 5.4234  (0.158 sec)
18-06-05 03:23-INFO->> Step 474670 run_train: loss = 5.4944  (0.147 sec)
18-06-05 03:23-INFO->> Step 474680 run_train: loss = 5.4615  (0.146 sec)
18-06-05 03:23-INFO->> Step 474690 run_train: loss = 5.4251  (0.158 sec)
18-06-05 03:23-INFO->> Step 474700 run_train: loss = 5.4169  (0.148 sec)
18-06-05 03:23-INFO->> Step 474710 run_train: loss = 5.4092  (0.178 sec)
18-06-05 03:23-INFO->> Step 474720 run_train: loss = 5.4214  (0.182 sec)
18-06-05 03:23-INFO->> Step 474730 run_train: loss = 5.3823  (0.130 sec)
18-06-05 03:23-INFO->> Step 474740 run_train: loss = 5.4183  (0.166 sec)
18-06-05 03:23-INFO->> Step 474750 run_train: loss = 5.3954  (0.169 sec)
18-06-05 03:23-INFO->> Step 474760 run_train: loss = 5.3838  (0.125 sec)
18-06-05 03:23-INFO->> Step 474770 run_train: loss = 5.4881  (0.161 sec)
18-06-05 03:23-INFO->> Step 474780 run_train: loss = 5.4233  (0.145 sec)
18-06-05 03:23-INFO->> Step 474790 run_train: loss = 5.4529  (0.125 sec)
18-06-05 03:23-INFO->> Step 474800 run_train: loss = 5.3597  (0.136 sec)
18-06-05 03:23-INFO->> Step 474810 run_train: loss = 5.4189  (0.140 sec)
18-06-05 03:23-INFO->> Step 474820 run_train: loss = 5.4498  (0.169 sec)
18-06-05 03:23-INFO->> Step 474830 run_train: loss = 5.4064  (0.144 sec)
18-06-05 03:23-INFO->> Step 474840 run_train: loss = 5.4580  (0.184 sec)
18-06-05 03:23-INFO->> Step 474850 run_train: loss = 5.3849  (0.171 sec)
18-06-05 03:23-INFO->> Step 474860 run_train: loss = 5.3180  (0.136 sec)
18-06-05 03:23-INFO->> Step 474870 run_train: loss = 5.3895  (0.176 sec)
18-06-05 03:23-INFO->> Step 474880 run_train: loss = 5.3899  (0.161 sec)
18-06-05 03:23-INFO->> Step 474890 run_train: loss = 5.4820  (0.124 sec)
18-06-05 03:23-INFO->> Step 474900 run_train: loss = 5.3949  (0.171 sec)
18-06-05 03:24-INFO->> Step 474910 run_train: loss = 5.3977  (0.142 sec)
18-06-05 03:24-INFO->> Step 474920 run_train: loss = 5.4206  (0.151 sec)
18-06-05 03:24-INFO->> Step 474930 run_train: loss = 5.4269  (0.186 sec)
18-06-05 03:24-INFO->> Step 474940 run_train: loss = 5.3381  (0.170 sec)
18-06-05 03:24-INFO->> Step 474950 run_train: loss = 5.3394  (0.144 sec)
18-06-05 03:24-INFO->> Step 474960 run_train: loss = 5.3955  (0.165 sec)
18-06-05 03:24-INFO->> Step 474970 run_train: loss = 5.3897  (0.166 sec)
18-06-05 03:24-INFO->> Step 474980 run_train: loss = 5.4283  (0.154 sec)
18-06-05 03:24-INFO->> Step 474990 run_train: loss = 5.3780  (0.179 sec)
18-06-05 03:24-INFO->> Step 475000 run_train: loss = 5.3930  (0.153 sec)
18-06-05 03:24-INFO->> 2018-06-05 03:24:14.316100 Saving in ckpt
18-06-05 03:24-INFO-Test Data Eval:
18-06-05 03:24-INFO-fpr95 = 0.17319507173219978 and auc = 0.9690952373809911
18-06-05 03:24-INFO->> Step 475010 run_train: loss = 5.3370  (0.136 sec)
18-06-05 03:24-INFO->> Step 475020 run_train: loss = 5.4467  (0.149 sec)
18-06-05 03:24-INFO->> Step 475030 run_train: loss = 5.4386  (0.157 sec)
18-06-05 03:25-INFO->> Step 475040 run_train: loss = 5.4351  (0.151 sec)
18-06-05 03:25-INFO->> Step 475050 run_train: loss = 5.3649  (0.181 sec)
18-06-05 03:25-INFO->> Step 475060 run_train: loss = 5.4162  (0.160 sec)
18-06-05 03:25-INFO->> Step 475070 run_train: loss = 5.4645  (0.189 sec)
18-06-05 03:25-INFO->> Step 475080 run_train: loss = 5.4307  (0.187 sec)
18-06-05 03:25-INFO->> Step 475090 run_train: loss = 5.4277  (0.174 sec)
18-06-05 03:25-INFO->> Step 475100 run_train: loss = 5.4407  (0.131 sec)
18-06-05 03:25-INFO->> Step 475110 run_train: loss = 5.4077  (0.145 sec)
18-06-05 03:25-INFO->> Step 475120 run_train: loss = 5.4086  (0.164 sec)
18-06-05 03:25-INFO->> Step 475130 run_train: loss = 5.3969  (0.188 sec)
18-06-05 03:25-INFO->> Step 475140 run_train: loss = 5.4030  (0.158 sec)
18-06-05 03:25-INFO->> Step 475150 run_train: loss = 5.4082  (0.163 sec)
18-06-05 03:25-INFO->> Step 475160 run_train: loss = 5.4221  (0.177 sec)
18-06-05 03:25-INFO->> Step 475170 run_train: loss = 5.3806  (0.180 sec)
18-06-05 03:25-INFO->> Step 475180 run_train: loss = 5.4323  (0.140 sec)
18-06-05 03:25-INFO->> Step 475190 run_train: loss = 5.3986  (0.156 sec)
18-06-05 03:25-INFO->> Step 475200 run_train: loss = 5.4372  (0.182 sec)
18-06-05 03:25-INFO->> Step 475210 run_train: loss = 5.4574  (0.168 sec)
18-06-05 03:25-INFO->> Step 475220 run_train: loss = 5.3747  (0.142 sec)
18-06-05 03:25-INFO->> Step 475230 run_train: loss = 5.3075  (0.180 sec)
18-06-05 03:25-INFO->> Step 475240 run_train: loss = 5.3773  (0.144 sec)
18-06-05 03:25-INFO->> Step 475250 run_train: loss = 5.4437  (0.119 sec)
18-06-05 03:25-INFO->> Step 475260 run_train: loss = 5.3161  (0.152 sec)
18-06-05 03:25-INFO->> Step 475270 run_train: loss = 5.3483  (0.166 sec)
18-06-05 03:25-INFO->> Step 475280 run_train: loss = 5.3968  (0.184 sec)
18-06-05 03:25-INFO->> Step 475290 run_train: loss = 5.4250  (0.181 sec)
18-06-05 03:25-INFO->> Step 475300 run_train: loss = 5.3433  (0.195 sec)
18-06-05 03:25-INFO->> Step 475310 run_train: loss = 5.4157  (0.183 sec)
18-06-05 03:25-INFO->> Step 475320 run_train: loss = 5.3536  (0.158 sec)
18-06-05 03:25-INFO->> Step 475330 run_train: loss = 5.3450  (0.130 sec)
18-06-05 03:25-INFO->> Step 475340 run_train: loss = 5.3746  (0.178 sec)
18-06-05 03:25-INFO->> Step 475350 run_train: loss = 5.4025  (0.175 sec)
18-06-05 03:25-INFO->> Step 475360 run_train: loss = 5.4285  (0.174 sec)
18-06-05 03:25-INFO->> Step 475370 run_train: loss = 5.4298  (0.193 sec)
18-06-05 03:25-INFO->> Step 475380 run_train: loss = 5.3931  (0.182 sec)
18-06-05 03:25-INFO->> Step 475390 run_train: loss = 5.4335  (0.167 sec)
18-06-05 03:25-INFO->> Step 475400 run_train: loss = 5.4514  (0.125 sec)
18-06-05 03:25-INFO->> Step 475410 run_train: loss = 5.3629  (0.159 sec)
18-06-05 03:26-INFO->> Step 475420 run_train: loss = 5.4098  (0.152 sec)
18-06-05 03:26-INFO->> Step 475430 run_train: loss = 5.3778  (0.129 sec)
18-06-05 03:26-INFO->> Step 475440 run_train: loss = 5.4666  (0.175 sec)
18-06-05 03:26-INFO->> Step 475450 run_train: loss = 5.4499  (0.154 sec)
18-06-05 03:26-INFO->> Step 475460 run_train: loss = 5.4134  (0.175 sec)
18-06-05 03:26-INFO->> Step 475470 run_train: loss = 5.3819  (0.161 sec)
18-06-05 03:26-INFO->> Step 475480 run_train: loss = 5.3870  (0.145 sec)
18-06-05 03:26-INFO->> Step 475490 run_train: loss = 5.3765  (0.165 sec)
18-06-05 03:26-INFO->> Step 475500 run_train: loss = 5.4551  (0.157 sec)
18-06-05 03:26-INFO->> Step 475510 run_train: loss = 5.4615  (0.173 sec)
18-06-05 03:26-INFO->> Step 475520 run_train: loss = 5.3198  (0.154 sec)
18-06-05 03:26-INFO->> Step 475530 run_train: loss = 5.4505  (0.192 sec)
18-06-05 03:26-INFO->> Step 475540 run_train: loss = 5.4370  (0.160 sec)
18-06-05 03:26-INFO->> Step 475550 run_train: loss = 5.3925  (0.187 sec)
18-06-05 03:26-INFO->> Step 475560 run_train: loss = 5.4259  (0.189 sec)
18-06-05 03:26-INFO->> Step 475570 run_train: loss = 5.4569  (0.170 sec)
18-06-05 03:26-INFO->> Step 475580 run_train: loss = 5.4252  (0.157 sec)
18-06-05 03:26-INFO->> Step 475590 run_train: loss = 5.4362  (0.169 sec)
18-06-05 03:26-INFO->> Step 475600 run_train: loss = 5.3347  (0.150 sec)
18-06-05 03:26-INFO->> Step 475610 run_train: loss = 5.4099  (0.146 sec)
18-06-05 03:26-INFO->> Step 475620 run_train: loss = 5.4230  (0.160 sec)
18-06-05 03:26-INFO->> Step 475630 run_train: loss = 5.3889  (0.155 sec)
18-06-05 03:26-INFO->> Step 475640 run_train: loss = 5.4738  (0.145 sec)
18-06-05 03:26-INFO->> Step 475650 run_train: loss = 5.3515  (0.148 sec)
18-06-05 03:26-INFO->> Step 475660 run_train: loss = 5.3517  (0.140 sec)
18-06-05 03:26-INFO->> Step 475670 run_train: loss = 5.4115  (0.133 sec)
18-06-05 03:26-INFO->> Step 475680 run_train: loss = 5.4372  (0.148 sec)
18-06-05 03:26-INFO->> Step 475690 run_train: loss = 5.3678  (0.161 sec)
18-06-05 03:26-INFO->> Step 475700 run_train: loss = 5.4490  (0.130 sec)
18-06-05 03:26-INFO->> Step 475710 run_train: loss = 5.4139  (0.167 sec)
18-06-05 03:26-INFO->> Step 475720 run_train: loss = 5.4077  (0.149 sec)
18-06-05 03:26-INFO->> Step 475730 run_train: loss = 5.4204  (0.190 sec)
18-06-05 03:26-INFO->> Step 475740 run_train: loss = 5.3614  (0.152 sec)
18-06-05 03:26-INFO->> Step 475750 run_train: loss = 5.4363  (0.192 sec)
18-06-05 03:26-INFO->> Step 475760 run_train: loss = 5.3806  (0.154 sec)
18-06-05 03:26-INFO->> Step 475770 run_train: loss = 5.3959  (0.165 sec)
18-06-05 03:26-INFO->> Step 475780 run_train: loss = 5.4835  (0.185 sec)
18-06-05 03:26-INFO->> Step 475790 run_train: loss = 5.4231  (0.157 sec)
18-06-05 03:27-INFO->> Step 475800 run_train: loss = 5.3755  (0.182 sec)
18-06-05 03:27-INFO->> Step 475810 run_train: loss = 5.4250  (0.194 sec)
18-06-05 03:27-INFO->> Step 475820 run_train: loss = 5.3508  (0.184 sec)
18-06-05 03:27-INFO->> Step 475830 run_train: loss = 5.3651  (0.174 sec)
18-06-05 03:27-INFO->> Step 475840 run_train: loss = 5.4048  (0.151 sec)
18-06-05 03:27-INFO->> Step 475850 run_train: loss = 5.3237  (0.156 sec)
18-06-05 03:27-INFO->> Step 475860 run_train: loss = 5.3891  (0.147 sec)
18-06-05 03:27-INFO->> Step 475870 run_train: loss = 5.3727  (0.154 sec)
18-06-05 03:27-INFO->> Step 475880 run_train: loss = 5.3950  (0.144 sec)
18-06-05 03:27-INFO->> Step 475890 run_train: loss = 5.4093  (0.151 sec)
18-06-05 03:27-INFO->> Step 475900 run_train: loss = 5.2926  (0.132 sec)
18-06-05 03:27-INFO->> Step 475910 run_train: loss = 5.4300  (0.184 sec)
18-06-05 03:27-INFO->> Step 475920 run_train: loss = 5.4095  (0.165 sec)
18-06-05 03:27-INFO->> Step 475930 run_train: loss = 5.3486  (0.150 sec)
18-06-05 03:27-INFO->> Step 475940 run_train: loss = 5.4386  (0.176 sec)
18-06-05 03:27-INFO->> Step 475950 run_train: loss = 5.4549  (0.154 sec)
18-06-05 03:27-INFO->> Step 475960 run_train: loss = 5.3906  (0.138 sec)
18-06-05 03:27-INFO->> Step 475970 run_train: loss = 5.4275  (0.145 sec)
18-06-05 03:27-INFO->> Step 475980 run_train: loss = 5.3828  (0.134 sec)
18-06-05 03:27-INFO->> Step 475990 run_train: loss = 5.4034  (0.162 sec)
18-06-05 03:27-INFO->> Step 476000 run_train: loss = 5.4282  (0.172 sec)
18-06-05 03:27-INFO->> 2018-06-05 03:27:32.688125 Saving in ckpt
18-06-05 03:27-INFO-Test Data Eval:
18-06-05 03:28-INFO-fpr95 = 0.16957525239107332 and auc = 0.9693521819469954
18-06-05 03:28-INFO->> Step 476010 run_train: loss = 5.4276  (0.129 sec)
18-06-05 03:28-INFO->> Step 476020 run_train: loss = 5.3864  (0.167 sec)
18-06-05 03:28-INFO->> Step 476030 run_train: loss = 5.3554  (0.155 sec)
18-06-05 03:28-INFO->> Step 476040 run_train: loss = 5.3904  (0.160 sec)
18-06-05 03:28-INFO->> Step 476050 run_train: loss = 5.3896  (0.152 sec)
18-06-05 03:28-INFO->> Step 476060 run_train: loss = 5.3872  (0.182 sec)
18-06-05 03:28-INFO->> Step 476070 run_train: loss = 5.4583  (0.178 sec)
18-06-05 03:28-INFO->> Step 476080 run_train: loss = 5.3870  (0.137 sec)
18-06-05 03:28-INFO->> Step 476090 run_train: loss = 5.3925  (0.204 sec)
18-06-05 03:28-INFO->> Step 476100 run_train: loss = 5.4319  (0.133 sec)
18-06-05 03:28-INFO->> Step 476110 run_train: loss = 5.3826  (0.180 sec)
18-06-05 03:28-INFO->> Step 476120 run_train: loss = 5.4095  (0.144 sec)
18-06-05 03:28-INFO->> Step 476130 run_train: loss = 5.3912  (0.114 sec)
18-06-05 03:28-INFO->> Step 476140 run_train: loss = 5.4755  (0.171 sec)
18-06-05 03:28-INFO->> Step 476150 run_train: loss = 5.4279  (0.170 sec)
18-06-05 03:28-INFO->> Step 476160 run_train: loss = 5.5207  (0.150 sec)
18-06-05 03:28-INFO->> Step 476170 run_train: loss = 5.3854  (0.184 sec)
18-06-05 03:28-INFO->> Step 476180 run_train: loss = 5.4743  (0.204 sec)
18-06-05 03:28-INFO->> Step 476190 run_train: loss = 5.3871  (0.141 sec)
18-06-05 03:28-INFO->> Step 476200 run_train: loss = 5.3484  (0.151 sec)
18-06-05 03:28-INFO->> Step 476210 run_train: loss = 5.3995  (0.177 sec)
18-06-05 03:28-INFO->> Step 476220 run_train: loss = 5.3739  (0.134 sec)
18-06-05 03:28-INFO->> Step 476230 run_train: loss = 5.4366  (0.173 sec)
18-06-05 03:28-INFO->> Step 476240 run_train: loss = 5.3648  (0.164 sec)
18-06-05 03:28-INFO->> Step 476250 run_train: loss = 5.4865  (0.137 sec)
18-06-05 03:28-INFO->> Step 476260 run_train: loss = 5.3758  (0.144 sec)
18-06-05 03:28-INFO->> Step 476270 run_train: loss = 5.4809  (0.126 sec)
18-06-05 03:28-INFO->> Step 476280 run_train: loss = 5.4090  (0.158 sec)
18-06-05 03:28-INFO->> Step 476290 run_train: loss = 5.4448  (0.206 sec)
18-06-05 03:29-INFO->> Step 476300 run_train: loss = 5.3331  (0.159 sec)
18-06-05 03:29-INFO->> Step 476310 run_train: loss = 5.3956  (0.197 sec)
18-06-05 03:29-INFO->> Step 476320 run_train: loss = 5.4620  (0.170 sec)
18-06-05 03:29-INFO->> Step 476330 run_train: loss = 5.3845  (0.180 sec)
18-06-05 03:29-INFO->> Step 476340 run_train: loss = 5.4328  (0.138 sec)
18-06-05 03:29-INFO->> Step 476350 run_train: loss = 5.3783  (0.152 sec)
18-06-05 03:29-INFO->> Step 476360 run_train: loss = 5.3148  (0.147 sec)
18-06-05 03:29-INFO->> Step 476370 run_train: loss = 5.4082  (0.181 sec)
18-06-05 03:29-INFO->> Step 476380 run_train: loss = 5.4555  (0.149 sec)
18-06-05 03:29-INFO->> Step 476390 run_train: loss = 5.3774  (0.142 sec)
18-06-05 03:29-INFO->> Step 476400 run_train: loss = 5.3841  (0.168 sec)
18-06-05 03:29-INFO->> Step 476410 run_train: loss = 5.4510  (0.165 sec)
18-06-05 03:29-INFO->> Step 476420 run_train: loss = 5.3847  (0.138 sec)
18-06-05 03:29-INFO->> Step 476430 run_train: loss = 5.3870  (0.125 sec)
18-06-05 03:29-INFO->> Step 476440 run_train: loss = 5.3981  (0.152 sec)
18-06-05 03:29-INFO->> Step 476450 run_train: loss = 5.3941  (0.192 sec)
18-06-05 03:29-INFO->> Step 476460 run_train: loss = 5.3518  (0.157 sec)
18-06-05 03:29-INFO->> Step 476470 run_train: loss = 5.3811  (0.155 sec)
18-06-05 03:29-INFO->> Step 476480 run_train: loss = 5.3933  (0.144 sec)
18-06-05 03:29-INFO->> Step 476490 run_train: loss = 5.3913  (0.155 sec)
18-06-05 03:29-INFO->> Step 476500 run_train: loss = 5.4998  (0.142 sec)
18-06-05 03:29-INFO->> Step 476510 run_train: loss = 5.4288  (0.218 sec)
18-06-05 03:29-INFO->> Step 476520 run_train: loss = 5.3220  (0.198 sec)
18-06-05 03:29-INFO->> Step 476530 run_train: loss = 5.3876  (0.153 sec)
18-06-05 03:29-INFO->> Step 476540 run_train: loss = 5.3883  (0.130 sec)
18-06-05 03:29-INFO->> Step 476550 run_train: loss = 5.4780  (0.159 sec)
18-06-05 03:29-INFO->> Step 476560 run_train: loss = 5.3811  (0.166 sec)
18-06-05 03:29-INFO->> Step 476570 run_train: loss = 5.3662  (0.143 sec)
18-06-05 03:29-INFO->> Step 476580 run_train: loss = 5.3593  (0.116 sec)
18-06-05 03:29-INFO->> Step 476590 run_train: loss = 5.4329  (0.142 sec)
18-06-05 03:29-INFO->> Step 476600 run_train: loss = 5.3187  (0.159 sec)
18-06-05 03:29-INFO->> Step 476610 run_train: loss = 5.4094  (0.171 sec)
18-06-05 03:29-INFO->> Step 476620 run_train: loss = 5.4120  (0.163 sec)
18-06-05 03:29-INFO->> Step 476630 run_train: loss = 5.3660  (0.155 sec)
18-06-05 03:29-INFO->> Step 476640 run_train: loss = 5.3933  (0.157 sec)
18-06-05 03:29-INFO->> Step 476650 run_train: loss = 5.3698  (0.168 sec)
18-06-05 03:29-INFO->> Step 476660 run_train: loss = 5.3737  (0.151 sec)
18-06-05 03:29-INFO->> Step 476670 run_train: loss = 5.4684  (0.145 sec)
18-06-05 03:30-INFO->> Step 476680 run_train: loss = 5.4142  (0.169 sec)
18-06-05 03:30-INFO->> Step 476690 run_train: loss = 5.4991  (0.140 sec)
18-06-05 03:30-INFO->> Step 476700 run_train: loss = 5.4649  (0.164 sec)
18-06-05 03:30-INFO->> Step 476710 run_train: loss = 5.3827  (0.130 sec)
18-06-05 03:30-INFO->> Step 476720 run_train: loss = 5.4341  (0.170 sec)
18-06-05 03:30-INFO->> Step 476730 run_train: loss = 5.4066  (0.181 sec)
18-06-05 03:30-INFO->> Step 476740 run_train: loss = 5.4218  (0.176 sec)
18-06-05 03:30-INFO->> Step 476750 run_train: loss = 5.3953  (0.159 sec)
18-06-05 03:30-INFO->> Step 476760 run_train: loss = 5.4419  (0.135 sec)
18-06-05 03:30-INFO->> Step 476770 run_train: loss = 5.3962  (0.116 sec)
18-06-05 03:30-INFO->> Step 476780 run_train: loss = 5.3587  (0.157 sec)
18-06-05 03:30-INFO->> Step 476790 run_train: loss = 5.3581  (0.167 sec)
18-06-05 03:30-INFO->> Step 476800 run_train: loss = 5.4374  (0.162 sec)
18-06-05 03:30-INFO->> Step 476810 run_train: loss = 5.4165  (0.129 sec)
18-06-05 03:30-INFO->> Step 476820 run_train: loss = 5.3840  (0.173 sec)
18-06-05 03:30-INFO->> Step 476830 run_train: loss = 5.3851  (0.164 sec)
18-06-05 03:30-INFO->> Step 476840 run_train: loss = 5.4189  (0.166 sec)
18-06-05 03:30-INFO->> Step 476850 run_train: loss = 5.4478  (0.164 sec)
18-06-05 03:30-INFO->> Step 476860 run_train: loss = 5.4185  (0.131 sec)
18-06-05 03:30-INFO->> Step 476870 run_train: loss = 5.4330  (0.166 sec)
18-06-05 03:30-INFO->> Step 476880 run_train: loss = 5.4456  (0.195 sec)
18-06-05 03:30-INFO->> Step 476890 run_train: loss = 5.4493  (0.144 sec)
18-06-05 03:30-INFO->> Step 476900 run_train: loss = 5.4356  (0.150 sec)
18-06-05 03:30-INFO->> Step 476910 run_train: loss = 5.4262  (0.148 sec)
18-06-05 03:30-INFO->> Step 476920 run_train: loss = 5.4342  (0.143 sec)
18-06-05 03:30-INFO->> Step 476930 run_train: loss = 5.3750  (0.130 sec)
18-06-05 03:30-INFO->> Step 476940 run_train: loss = 5.4234  (0.153 sec)
18-06-05 03:30-INFO->> Step 476950 run_train: loss = 5.4071  (0.174 sec)
18-06-05 03:30-INFO->> Step 476960 run_train: loss = 5.4133  (0.137 sec)
18-06-05 03:30-INFO->> Step 476970 run_train: loss = 5.4320  (0.143 sec)
18-06-05 03:30-INFO->> Step 476980 run_train: loss = 5.3157  (0.142 sec)
18-06-05 03:30-INFO->> Step 476990 run_train: loss = 5.3822  (0.140 sec)
18-06-05 03:30-INFO->> Step 477000 run_train: loss = 5.3805  (0.168 sec)
18-06-05 03:30-INFO->> 2018-06-05 03:30:51.275569 Saving in ckpt
18-06-05 03:30-INFO-Test Data Eval:
18-06-05 03:31-INFO-fpr95 = 0.1783923352816153 and auc = 0.9684034022399113
18-06-05 03:31-INFO->> Step 477010 run_train: loss = 5.4398  (0.163 sec)
18-06-05 03:31-INFO->> Step 477020 run_train: loss = 5.4108  (0.165 sec)
18-06-05 03:31-INFO->> Step 477030 run_train: loss = 5.3511  (0.168 sec)
18-06-05 03:31-INFO->> Step 477040 run_train: loss = 5.4176  (0.171 sec)
18-06-05 03:31-INFO->> Step 477050 run_train: loss = 5.3774  (0.144 sec)
18-06-05 03:31-INFO->> Step 477060 run_train: loss = 5.3786  (0.172 sec)
18-06-05 03:31-INFO->> Step 477070 run_train: loss = 5.3503  (0.156 sec)
18-06-05 03:31-INFO->> Step 477080 run_train: loss = 5.3717  (0.170 sec)
18-06-05 03:31-INFO->> Step 477090 run_train: loss = 5.4379  (0.144 sec)
18-06-05 03:31-INFO->> Step 477100 run_train: loss = 5.3388  (0.189 sec)
18-06-05 03:31-INFO->> Step 477110 run_train: loss = 5.4190  (0.158 sec)
18-06-05 03:31-INFO->> Step 477120 run_train: loss = 5.4025  (0.181 sec)
18-06-05 03:31-INFO->> Step 477130 run_train: loss = 5.4242  (0.175 sec)
18-06-05 03:31-INFO->> Step 477140 run_train: loss = 5.4014  (0.175 sec)
18-06-05 03:31-INFO->> Step 477150 run_train: loss = 5.3700  (0.158 sec)
18-06-05 03:31-INFO->> Step 477160 run_train: loss = 5.4018  (0.139 sec)
18-06-05 03:31-INFO->> Step 477170 run_train: loss = 5.3840  (0.177 sec)
18-06-05 03:31-INFO->> Step 477180 run_train: loss = 5.4439  (0.172 sec)
18-06-05 03:32-INFO->> Step 477190 run_train: loss = 5.4034  (0.172 sec)
18-06-05 03:32-INFO->> Step 477200 run_train: loss = 5.4434  (0.188 sec)
18-06-05 03:32-INFO->> Step 477210 run_train: loss = 5.3997  (0.143 sec)
18-06-05 03:32-INFO->> Step 477220 run_train: loss = 5.3646  (0.109 sec)
18-06-05 03:32-INFO->> Step 477230 run_train: loss = 5.3905  (0.152 sec)
18-06-05 03:32-INFO->> Step 477240 run_train: loss = 5.4296  (0.186 sec)
18-06-05 03:32-INFO->> Step 477250 run_train: loss = 5.3508  (0.145 sec)
18-06-05 03:32-INFO->> Step 477260 run_train: loss = 5.4002  (0.131 sec)
18-06-05 03:32-INFO->> Step 477270 run_train: loss = 5.3778  (0.146 sec)
18-06-05 03:32-INFO->> Step 477280 run_train: loss = 5.3627  (0.148 sec)
18-06-05 03:32-INFO->> Step 477290 run_train: loss = 5.3873  (0.179 sec)
18-06-05 03:32-INFO->> Step 477300 run_train: loss = 5.3544  (0.173 sec)
18-06-05 03:32-INFO->> Step 477310 run_train: loss = 5.3946  (0.147 sec)
18-06-05 03:32-INFO->> Step 477320 run_train: loss = 5.4094  (0.175 sec)
18-06-05 03:32-INFO->> Step 477330 run_train: loss = 5.4093  (0.156 sec)
18-06-05 03:32-INFO->> Step 477340 run_train: loss = 5.4215  (0.133 sec)
18-06-05 03:32-INFO->> Step 477350 run_train: loss = 5.4050  (0.158 sec)
18-06-05 03:32-INFO->> Step 477360 run_train: loss = 5.4747  (0.151 sec)
18-06-05 03:32-INFO->> Step 477370 run_train: loss = 5.4360  (0.162 sec)
18-06-05 03:32-INFO->> Step 477380 run_train: loss = 5.3806  (0.185 sec)
18-06-05 03:32-INFO->> Step 477390 run_train: loss = 5.4049  (0.184 sec)
18-06-05 03:32-INFO->> Step 477400 run_train: loss = 5.4187  (0.165 sec)
18-06-05 03:32-INFO->> Step 477410 run_train: loss = 5.4278  (0.148 sec)
18-06-05 03:32-INFO->> Step 477420 run_train: loss = 5.4023  (0.193 sec)
18-06-05 03:32-INFO->> Step 477430 run_train: loss = 5.3948  (0.157 sec)
18-06-05 03:32-INFO->> Step 477440 run_train: loss = 5.4508  (0.159 sec)
18-06-05 03:32-INFO->> Step 477450 run_train: loss = 5.4329  (0.161 sec)
18-06-05 03:32-INFO->> Step 477460 run_train: loss = 5.3879  (0.151 sec)
18-06-05 03:32-INFO->> Step 477470 run_train: loss = 5.3992  (0.137 sec)
18-06-05 03:32-INFO->> Step 477480 run_train: loss = 5.3655  (0.171 sec)
18-06-05 03:32-INFO->> Step 477490 run_train: loss = 5.4576  (0.149 sec)
18-06-05 03:32-INFO->> Step 477500 run_train: loss = 5.4381  (0.176 sec)
18-06-05 03:32-INFO->> Step 477510 run_train: loss = 5.4428  (0.143 sec)
18-06-05 03:32-INFO->> Step 477520 run_train: loss = 5.4655  (0.176 sec)
18-06-05 03:32-INFO->> Step 477530 run_train: loss = 5.4655  (0.175 sec)
18-06-05 03:32-INFO->> Step 477540 run_train: loss = 5.3571  (0.162 sec)
18-06-05 03:32-INFO->> Step 477550 run_train: loss = 5.3804  (0.142 sec)
18-06-05 03:33-INFO->> Step 477560 run_train: loss = 5.3718  (0.154 sec)
18-06-05 03:33-INFO->> Step 477570 run_train: loss = 5.3807  (0.138 sec)
18-06-05 03:33-INFO->> Step 477580 run_train: loss = 5.4112  (0.154 sec)
18-06-05 03:33-INFO->> Step 477590 run_train: loss = 5.4028  (0.149 sec)
18-06-05 03:33-INFO->> Step 477600 run_train: loss = 5.3825  (0.172 sec)
18-06-05 03:33-INFO->> Step 477610 run_train: loss = 5.3724  (0.170 sec)
18-06-05 03:33-INFO->> Step 477620 run_train: loss = 5.4086  (0.159 sec)
18-06-05 03:33-INFO->> Step 477630 run_train: loss = 5.4159  (0.157 sec)
18-06-05 03:33-INFO->> Step 477640 run_train: loss = 5.4338  (0.155 sec)
18-06-05 03:33-INFO->> Step 477650 run_train: loss = 5.4431  (0.170 sec)
18-06-05 03:33-INFO->> Step 477660 run_train: loss = 5.4287  (0.162 sec)
18-06-05 03:33-INFO->> Step 477670 run_train: loss = 5.4442  (0.151 sec)
18-06-05 03:33-INFO->> Step 477680 run_train: loss = 5.3909  (0.199 sec)
18-06-05 03:33-INFO->> Step 477690 run_train: loss = 5.3969  (0.139 sec)
18-06-05 03:33-INFO->> Step 477700 run_train: loss = 5.4093  (0.150 sec)
18-06-05 03:33-INFO->> Step 477710 run_train: loss = 5.3857  (0.159 sec)
18-06-05 03:33-INFO->> Step 477720 run_train: loss = 5.4751  (0.157 sec)
18-06-05 03:33-INFO->> Step 477730 run_train: loss = 5.4220  (0.163 sec)
18-06-05 03:33-INFO->> Step 477740 run_train: loss = 5.3908  (0.142 sec)
18-06-05 03:33-INFO->> Step 477750 run_train: loss = 5.4332  (0.155 sec)
18-06-05 03:33-INFO->> Step 477760 run_train: loss = 5.4374  (0.160 sec)
18-06-05 03:33-INFO->> Step 477770 run_train: loss = 5.4178  (0.144 sec)
18-06-05 03:33-INFO->> Step 477780 run_train: loss = 5.3972  (0.159 sec)
18-06-05 03:33-INFO->> Step 477790 run_train: loss = 5.3886  (0.192 sec)
18-06-05 03:33-INFO->> Step 477800 run_train: loss = 5.3705  (0.146 sec)
18-06-05 03:33-INFO->> Step 477810 run_train: loss = 5.3687  (0.159 sec)
18-06-05 03:33-INFO->> Step 477820 run_train: loss = 5.4716  (0.159 sec)
18-06-05 03:33-INFO->> Step 477830 run_train: loss = 5.3953  (0.169 sec)
18-06-05 03:33-INFO->> Step 477840 run_train: loss = 5.4331  (0.186 sec)
18-06-05 03:33-INFO->> Step 477850 run_train: loss = 5.3573  (0.122 sec)
18-06-05 03:33-INFO->> Step 477860 run_train: loss = 5.3604  (0.204 sec)
18-06-05 03:33-INFO->> Step 477870 run_train: loss = 5.3968  (0.181 sec)
18-06-05 03:33-INFO->> Step 477880 run_train: loss = 5.3674  (0.150 sec)
18-06-05 03:33-INFO->> Step 477890 run_train: loss = 5.3722  (0.176 sec)
18-06-05 03:33-INFO->> Step 477900 run_train: loss = 5.3619  (0.153 sec)
18-06-05 03:33-INFO->> Step 477910 run_train: loss = 5.3571  (0.147 sec)
18-06-05 03:33-INFO->> Step 477920 run_train: loss = 5.3555  (0.165 sec)
18-06-05 03:33-INFO->> Step 477930 run_train: loss = 5.4318  (0.167 sec)
18-06-05 03:34-INFO->> Step 477940 run_train: loss = 5.3432  (0.137 sec)
18-06-05 03:34-INFO->> Step 477950 run_train: loss = 5.3885  (0.167 sec)
18-06-05 03:34-INFO->> Step 477960 run_train: loss = 5.4023  (0.172 sec)
18-06-05 03:34-INFO->> Step 477970 run_train: loss = 5.4378  (0.162 sec)
18-06-05 03:34-INFO->> Step 477980 run_train: loss = 5.4234  (0.158 sec)
18-06-05 03:34-INFO->> Step 477990 run_train: loss = 5.4506  (0.161 sec)
18-06-05 03:34-INFO->> Step 478000 run_train: loss = 5.4624  (0.165 sec)
18-06-05 03:34-INFO->> 2018-06-05 03:34:09.917320 Saving in ckpt
18-06-05 03:34-INFO-Test Data Eval:
18-06-05 03:34-INFO-fpr95 = 0.17312865302869287 and auc = 0.9690376408765472
18-06-05 03:34-INFO->> Step 478010 run_train: loss = 5.4073  (0.151 sec)
18-06-05 03:34-INFO->> Step 478020 run_train: loss = 5.3918  (0.147 sec)
18-06-05 03:34-INFO->> Step 478030 run_train: loss = 5.3498  (0.177 sec)
18-06-05 03:34-INFO->> Step 478040 run_train: loss = 5.4545  (0.141 sec)
18-06-05 03:34-INFO->> Step 478050 run_train: loss = 5.3724  (0.167 sec)
18-06-05 03:34-INFO->> Step 478060 run_train: loss = 5.4283  (0.178 sec)
18-06-05 03:35-INFO->> Step 478070 run_train: loss = 5.4170  (0.126 sec)
18-06-05 03:35-INFO->> Step 478080 run_train: loss = 5.4088  (0.160 sec)
18-06-05 03:35-INFO->> Step 478090 run_train: loss = 5.4451  (0.142 sec)
18-06-05 03:35-INFO->> Step 478100 run_train: loss = 5.4278  (0.163 sec)
18-06-05 03:35-INFO->> Step 478110 run_train: loss = 5.3675  (0.190 sec)
18-06-05 03:35-INFO->> Step 478120 run_train: loss = 5.4635  (0.151 sec)
18-06-05 03:35-INFO->> Step 478130 run_train: loss = 5.4099  (0.105 sec)
18-06-05 03:35-INFO->> Step 478140 run_train: loss = 5.3763  (0.200 sec)
18-06-05 03:35-INFO->> Step 478150 run_train: loss = 5.3996  (0.137 sec)
18-06-05 03:35-INFO->> Step 478160 run_train: loss = 5.4181  (0.152 sec)
18-06-05 03:35-INFO->> Step 478170 run_train: loss = 5.4777  (0.140 sec)
18-06-05 03:35-INFO->> Step 478180 run_train: loss = 5.4191  (0.159 sec)
18-06-05 03:35-INFO->> Step 478190 run_train: loss = 5.3976  (0.134 sec)
18-06-05 03:35-INFO->> Step 478200 run_train: loss = 5.4337  (0.165 sec)
18-06-05 03:35-INFO->> Step 478210 run_train: loss = 5.4135  (0.178 sec)
18-06-05 03:35-INFO->> Step 478220 run_train: loss = 5.3857  (0.169 sec)
18-06-05 03:35-INFO->> Step 478230 run_train: loss = 5.3721  (0.147 sec)
18-06-05 03:35-INFO->> Step 478240 run_train: loss = 5.4274  (0.170 sec)
18-06-05 03:35-INFO->> Step 478250 run_train: loss = 5.4488  (0.142 sec)
18-06-05 03:35-INFO->> Step 478260 run_train: loss = 5.4381  (0.155 sec)
18-06-05 03:35-INFO->> Step 478270 run_train: loss = 5.3708  (0.155 sec)
18-06-05 03:35-INFO->> Step 478280 run_train: loss = 5.3656  (0.149 sec)
18-06-05 03:35-INFO->> Step 478290 run_train: loss = 5.2750  (0.144 sec)
18-06-05 03:35-INFO->> Step 478300 run_train: loss = 5.4008  (0.194 sec)
18-06-05 03:35-INFO->> Step 478310 run_train: loss = 5.4176  (0.150 sec)
18-06-05 03:35-INFO->> Step 478320 run_train: loss = 5.4460  (0.151 sec)
18-06-05 03:35-INFO->> Step 478330 run_train: loss = 5.3846  (0.155 sec)
18-06-05 03:35-INFO->> Step 478340 run_train: loss = 5.3306  (0.197 sec)
18-06-05 03:35-INFO->> Step 478350 run_train: loss = 5.4308  (0.172 sec)
18-06-05 03:35-INFO->> Step 478360 run_train: loss = 5.4353  (0.154 sec)
18-06-05 03:35-INFO->> Step 478370 run_train: loss = 5.3733  (0.188 sec)
18-06-05 03:35-INFO->> Step 478380 run_train: loss = 5.3945  (0.182 sec)
18-06-05 03:35-INFO->> Step 478390 run_train: loss = 5.3585  (0.161 sec)
18-06-05 03:35-INFO->> Step 478400 run_train: loss = 5.3376  (0.177 sec)
18-06-05 03:35-INFO->> Step 478410 run_train: loss = 5.3599  (0.167 sec)
18-06-05 03:35-INFO->> Step 478420 run_train: loss = 5.4127  (0.182 sec)
18-06-05 03:35-INFO->> Step 478430 run_train: loss = 5.4121  (0.186 sec)
18-06-05 03:36-INFO->> Step 478440 run_train: loss = 5.4221  (0.154 sec)
18-06-05 03:36-INFO->> Step 478450 run_train: loss = 5.3653  (0.112 sec)
18-06-05 03:36-INFO->> Step 478460 run_train: loss = 5.3737  (0.155 sec)
18-06-05 03:36-INFO->> Step 478470 run_train: loss = 5.4222  (0.119 sec)
18-06-05 03:36-INFO->> Step 478480 run_train: loss = 5.3502  (0.166 sec)
18-06-05 03:36-INFO->> Step 478490 run_train: loss = 5.4409  (0.164 sec)
18-06-05 03:36-INFO->> Step 478500 run_train: loss = 5.3324  (0.165 sec)
18-06-05 03:36-INFO->> Step 478510 run_train: loss = 5.3779  (0.156 sec)
18-06-05 03:36-INFO->> Step 478520 run_train: loss = 5.3995  (0.197 sec)
18-06-05 03:36-INFO->> Step 478530 run_train: loss = 5.4476  (0.152 sec)
18-06-05 03:36-INFO->> Step 478540 run_train: loss = 5.4714  (0.148 sec)
18-06-05 03:36-INFO->> Step 478550 run_train: loss = 5.4773  (0.179 sec)
18-06-05 03:36-INFO->> Step 478560 run_train: loss = 5.4343  (0.160 sec)
18-06-05 03:36-INFO->> Step 478570 run_train: loss = 5.4173  (0.161 sec)
18-06-05 03:36-INFO->> Step 478580 run_train: loss = 5.4159  (0.165 sec)
18-06-05 03:36-INFO->> Step 478590 run_train: loss = 5.4532  (0.144 sec)
18-06-05 03:36-INFO->> Step 478600 run_train: loss = 5.4138  (0.168 sec)
18-06-05 03:36-INFO->> Step 478610 run_train: loss = 5.3900  (0.157 sec)
18-06-05 03:36-INFO->> Step 478620 run_train: loss = 5.3589  (0.132 sec)
18-06-05 03:36-INFO->> Step 478630 run_train: loss = 5.4349  (0.164 sec)
18-06-05 03:36-INFO->> Step 478640 run_train: loss = 5.3673  (0.158 sec)
18-06-05 03:36-INFO->> Step 478650 run_train: loss = 5.4319  (0.137 sec)
18-06-05 03:36-INFO->> Step 478660 run_train: loss = 5.3753  (0.160 sec)
18-06-05 03:36-INFO->> Step 478670 run_train: loss = 5.3975  (0.173 sec)
18-06-05 03:36-INFO->> Step 478680 run_train: loss = 5.4246  (0.155 sec)
18-06-05 03:36-INFO->> Step 478690 run_train: loss = 5.4407  (0.154 sec)
18-06-05 03:36-INFO->> Step 478700 run_train: loss = 5.3973  (0.153 sec)
18-06-05 03:36-INFO->> Step 478710 run_train: loss = 5.3614  (0.153 sec)
18-06-05 03:36-INFO->> Step 478720 run_train: loss = 5.3806  (0.160 sec)
18-06-05 03:36-INFO->> Step 478730 run_train: loss = 5.4627  (0.135 sec)
18-06-05 03:36-INFO->> Step 478740 run_train: loss = 5.4662  (0.174 sec)
18-06-05 03:36-INFO->> Step 478750 run_train: loss = 5.3948  (0.167 sec)
18-06-05 03:36-INFO->> Step 478760 run_train: loss = 5.4450  (0.185 sec)
18-06-05 03:36-INFO->> Step 478770 run_train: loss = 5.3711  (0.162 sec)
18-06-05 03:36-INFO->> Step 478780 run_train: loss = 5.3770  (0.143 sec)
18-06-05 03:36-INFO->> Step 478790 run_train: loss = 5.3773  (0.156 sec)
18-06-05 03:36-INFO->> Step 478800 run_train: loss = 5.3274  (0.154 sec)
18-06-05 03:36-INFO->> Step 478810 run_train: loss = 5.4351  (0.146 sec)
18-06-05 03:37-INFO->> Step 478820 run_train: loss = 5.4214  (0.164 sec)
18-06-05 03:37-INFO->> Step 478830 run_train: loss = 5.3910  (0.166 sec)
18-06-05 03:37-INFO->> Step 478840 run_train: loss = 5.4248  (0.152 sec)
18-06-05 03:37-INFO->> Step 478850 run_train: loss = 5.4371  (0.157 sec)
18-06-05 03:37-INFO->> Step 478860 run_train: loss = 5.4481  (0.130 sec)
18-06-05 03:37-INFO->> Step 478870 run_train: loss = 5.4205  (0.134 sec)
18-06-05 03:37-INFO->> Step 478880 run_train: loss = 5.4041  (0.182 sec)
18-06-05 03:37-INFO->> Step 478890 run_train: loss = 5.4005  (0.169 sec)
18-06-05 03:37-INFO->> Step 478900 run_train: loss = 5.3929  (0.160 sec)
18-06-05 03:37-INFO->> Step 478910 run_train: loss = 5.3814  (0.171 sec)
18-06-05 03:37-INFO->> Step 478920 run_train: loss = 5.4004  (0.143 sec)
18-06-05 03:37-INFO->> Step 478930 run_train: loss = 5.4121  (0.168 sec)
18-06-05 03:37-INFO->> Step 478940 run_train: loss = 5.4417  (0.146 sec)
18-06-05 03:37-INFO->> Step 478950 run_train: loss = 5.4384  (0.146 sec)
18-06-05 03:37-INFO->> Step 478960 run_train: loss = 5.4016  (0.129 sec)
18-06-05 03:37-INFO->> Step 478970 run_train: loss = 5.3756  (0.130 sec)
18-06-05 03:37-INFO->> Step 478980 run_train: loss = 5.4194  (0.143 sec)
18-06-05 03:37-INFO->> Step 478990 run_train: loss = 5.3872  (0.165 sec)
18-06-05 03:37-INFO->> Step 479000 run_train: loss = 5.4333  (0.148 sec)
18-06-05 03:37-INFO->> 2018-06-05 03:37:29.769350 Saving in ckpt
18-06-05 03:37-INFO-Test Data Eval:
18-06-05 03:38-INFO-fpr95 = 0.16773213336875664 and auc = 0.9694182783048979
18-06-05 03:38-INFO->> Step 479010 run_train: loss = 5.4133  (0.163 sec)
18-06-05 03:38-INFO->> Step 479020 run_train: loss = 5.4038  (0.172 sec)
18-06-05 03:38-INFO->> Step 479030 run_train: loss = 5.4073  (0.164 sec)
18-06-05 03:38-INFO->> Step 479040 run_train: loss = 5.3923  (0.160 sec)
18-06-05 03:38-INFO->> Step 479050 run_train: loss = 5.3971  (0.164 sec)
18-06-05 03:38-INFO->> Step 479060 run_train: loss = 5.3938  (0.128 sec)
18-06-05 03:38-INFO->> Step 479070 run_train: loss = 5.4913  (0.197 sec)
18-06-05 03:38-INFO->> Step 479080 run_train: loss = 5.4245  (0.154 sec)
18-06-05 03:38-INFO->> Step 479090 run_train: loss = 5.3817  (0.179 sec)
18-06-05 03:38-INFO->> Step 479100 run_train: loss = 5.4406  (0.180 sec)
18-06-05 03:38-INFO->> Step 479110 run_train: loss = 5.3979  (0.173 sec)
18-06-05 03:38-INFO->> Step 479120 run_train: loss = 5.3744  (0.158 sec)
18-06-05 03:38-INFO->> Step 479130 run_train: loss = 5.4007  (0.139 sec)
18-06-05 03:38-INFO->> Step 479140 run_train: loss = 5.4553  (0.134 sec)
18-06-05 03:38-INFO->> Step 479150 run_train: loss = 5.3651  (0.139 sec)
18-06-05 03:38-INFO->> Step 479160 run_train: loss = 5.4530  (0.166 sec)
18-06-05 03:38-INFO->> Step 479170 run_train: loss = 5.4164  (0.136 sec)
18-06-05 03:38-INFO->> Step 479180 run_train: loss = 5.4463  (0.137 sec)
18-06-05 03:38-INFO->> Step 479190 run_train: loss = 5.4137  (0.189 sec)
18-06-05 03:38-INFO->> Step 479200 run_train: loss = 5.3571  (0.154 sec)
18-06-05 03:38-INFO->> Step 479210 run_train: loss = 5.4015  (0.142 sec)
18-06-05 03:38-INFO->> Step 479220 run_train: loss = 5.3828  (0.141 sec)
18-06-05 03:38-INFO->> Step 479230 run_train: loss = 5.4291  (0.180 sec)
18-06-05 03:38-INFO->> Step 479240 run_train: loss = 5.4159  (0.168 sec)
18-06-05 03:38-INFO->> Step 479250 run_train: loss = 5.4160  (0.140 sec)
18-06-05 03:38-INFO->> Step 479260 run_train: loss = 5.3918  (0.158 sec)
18-06-05 03:38-INFO->> Step 479270 run_train: loss = 5.3984  (0.161 sec)
18-06-05 03:38-INFO->> Step 479280 run_train: loss = 5.3609  (0.168 sec)
18-06-05 03:38-INFO->> Step 479290 run_train: loss = 5.4260  (0.186 sec)
18-06-05 03:38-INFO->> Step 479300 run_train: loss = 5.3867  (0.141 sec)
18-06-05 03:38-INFO->> Step 479310 run_train: loss = 5.4141  (0.148 sec)
18-06-05 03:39-INFO->> Step 479320 run_train: loss = 5.3992  (0.186 sec)
18-06-05 03:39-INFO->> Step 479330 run_train: loss = 5.4658  (0.169 sec)
18-06-05 03:39-INFO->> Step 479340 run_train: loss = 5.4249  (0.157 sec)
18-06-05 03:39-INFO->> Step 479350 run_train: loss = 5.3462  (0.167 sec)
18-06-05 03:39-INFO->> Step 479360 run_train: loss = 5.3595  (0.182 sec)
18-06-05 03:39-INFO->> Step 479370 run_train: loss = 5.4103  (0.194 sec)
18-06-05 03:39-INFO->> Step 479380 run_train: loss = 5.3884  (0.134 sec)
18-06-05 03:39-INFO->> Step 479390 run_train: loss = 5.4361  (0.140 sec)
18-06-05 03:39-INFO->> Step 479400 run_train: loss = 5.5025  (0.156 sec)
18-06-05 03:39-INFO->> Step 479410 run_train: loss = 5.4276  (0.149 sec)
18-06-05 03:39-INFO->> Step 479420 run_train: loss = 5.3863  (0.174 sec)
18-06-05 03:39-INFO->> Step 479430 run_train: loss = 5.3659  (0.141 sec)
18-06-05 03:39-INFO->> Step 479440 run_train: loss = 5.3847  (0.174 sec)
18-06-05 03:39-INFO->> Step 479450 run_train: loss = 5.3596  (0.203 sec)
18-06-05 03:39-INFO->> Step 479460 run_train: loss = 5.4136  (0.162 sec)
18-06-05 03:39-INFO->> Step 479470 run_train: loss = 5.3716  (0.193 sec)
18-06-05 03:39-INFO->> Step 479480 run_train: loss = 5.4077  (0.156 sec)
18-06-05 03:39-INFO->> Step 479490 run_train: loss = 5.4274  (0.140 sec)
18-06-05 03:39-INFO->> Step 479500 run_train: loss = 5.4251  (0.141 sec)
18-06-05 03:39-INFO->> Step 479510 run_train: loss = 5.4852  (0.174 sec)
18-06-05 03:39-INFO->> Step 479520 run_train: loss = 5.4068  (0.183 sec)
18-06-05 03:39-INFO->> Step 479530 run_train: loss = 5.3937  (0.151 sec)
18-06-05 03:39-INFO->> Step 479540 run_train: loss = 5.4293  (0.133 sec)
18-06-05 03:39-INFO->> Step 479550 run_train: loss = 5.3606  (0.159 sec)
18-06-05 03:39-INFO->> Step 479560 run_train: loss = 5.4365  (0.140 sec)
18-06-05 03:39-INFO->> Step 479570 run_train: loss = 5.4405  (0.179 sec)
18-06-05 03:39-INFO->> Step 479580 run_train: loss = 5.4336  (0.154 sec)
18-06-05 03:39-INFO->> Step 479590 run_train: loss = 5.3488  (0.147 sec)
18-06-05 03:39-INFO->> Step 479600 run_train: loss = 5.4572  (0.175 sec)
18-06-05 03:39-INFO->> Step 479610 run_train: loss = 5.3566  (0.159 sec)
18-06-05 03:39-INFO->> Step 479620 run_train: loss = 5.4286  (0.162 sec)
18-06-05 03:39-INFO->> Step 479630 run_train: loss = 5.3140  (0.139 sec)
18-06-05 03:39-INFO->> Step 479640 run_train: loss = 5.4485  (0.151 sec)
18-06-05 03:39-INFO->> Step 479650 run_train: loss = 5.3821  (0.175 sec)
18-06-05 03:39-INFO->> Step 479660 run_train: loss = 5.4255  (0.157 sec)
18-06-05 03:39-INFO->> Step 479670 run_train: loss = 5.3661  (0.159 sec)
18-06-05 03:39-INFO->> Step 479680 run_train: loss = 5.4483  (0.150 sec)
18-06-05 03:39-INFO->> Step 479690 run_train: loss = 5.3272  (0.160 sec)
18-06-05 03:40-INFO->> Step 479700 run_train: loss = 5.4190  (0.214 sec)
18-06-05 03:40-INFO->> Step 479710 run_train: loss = 5.3737  (0.165 sec)
18-06-05 03:40-INFO->> Step 479720 run_train: loss = 5.3653  (0.129 sec)
18-06-05 03:40-INFO->> Step 479730 run_train: loss = 5.3824  (0.181 sec)
18-06-05 03:40-INFO->> Step 479740 run_train: loss = 5.3538  (0.187 sec)
18-06-05 03:40-INFO->> Step 479750 run_train: loss = 5.4183  (0.176 sec)
18-06-05 03:40-INFO->> Step 479760 run_train: loss = 5.3993  (0.165 sec)
18-06-05 03:40-INFO->> Step 479770 run_train: loss = 5.4814  (0.155 sec)
18-06-05 03:40-INFO->> Step 479780 run_train: loss = 5.4222  (0.151 sec)
18-06-05 03:40-INFO->> Step 479790 run_train: loss = 5.3965  (0.167 sec)
18-06-05 03:40-INFO->> Step 479800 run_train: loss = 5.3753  (0.147 sec)
18-06-05 03:40-INFO->> Step 479810 run_train: loss = 5.4506  (0.136 sec)
18-06-05 03:40-INFO->> Step 479820 run_train: loss = 5.3972  (0.158 sec)
18-06-05 03:40-INFO->> Step 479830 run_train: loss = 5.3709  (0.151 sec)
18-06-05 03:40-INFO->> Step 479840 run_train: loss = 5.3873  (0.151 sec)
18-06-05 03:40-INFO->> Step 479850 run_train: loss = 5.2951  (0.155 sec)
18-06-05 03:40-INFO->> Step 479860 run_train: loss = 5.4312  (0.149 sec)
18-06-05 03:40-INFO->> Step 479870 run_train: loss = 5.4163  (0.148 sec)
18-06-05 03:40-INFO->> Step 479880 run_train: loss = 5.4077  (0.172 sec)
18-06-05 03:40-INFO->> Step 479890 run_train: loss = 5.3634  (0.163 sec)
18-06-05 03:40-INFO->> Step 479900 run_train: loss = 5.3644  (0.155 sec)
18-06-05 03:40-INFO->> Step 479910 run_train: loss = 5.3112  (0.137 sec)
18-06-05 03:40-INFO->> Step 479920 run_train: loss = 5.4386  (0.136 sec)
18-06-05 03:40-INFO->> Step 479930 run_train: loss = 5.4173  (0.127 sec)
18-06-05 03:40-INFO->> Step 479940 run_train: loss = 5.4000  (0.189 sec)
18-06-05 03:40-INFO->> Step 479950 run_train: loss = 5.4263  (0.137 sec)
18-06-05 03:40-INFO->> Step 479960 run_train: loss = 5.3691  (0.162 sec)
18-06-05 03:40-INFO->> Step 479970 run_train: loss = 5.4116  (0.171 sec)
18-06-05 03:40-INFO->> Step 479980 run_train: loss = 5.3956  (0.167 sec)
18-06-05 03:40-INFO->> Step 479990 run_train: loss = 5.4375  (0.151 sec)
18-06-05 03:40-INFO->> Step 480000 run_train: loss = 5.4109  (0.165 sec)
18-06-05 03:40-INFO->> 2018-06-05 03:40:48.677122 Saving in ckpt
18-06-05 03:40-INFO-Test Data Eval:
18-06-05 03:41-INFO-fpr95 = 0.17728812433581295 and auc = 0.9686611494819689
18-06-05 03:41-INFO->> Step 480010 run_train: loss = 5.4088  (0.153 sec)
18-06-05 03:41-INFO->> Step 480020 run_train: loss = 5.3350  (0.161 sec)
18-06-05 03:41-INFO->> Step 480030 run_train: loss = 5.4145  (0.152 sec)
18-06-05 03:41-INFO->> Step 480040 run_train: loss = 5.4248  (0.170 sec)
18-06-05 03:41-INFO->> Step 480050 run_train: loss = 5.3894  (0.133 sec)
18-06-05 03:41-INFO->> Step 480060 run_train: loss = 5.4239  (0.155 sec)
18-06-05 03:41-INFO->> Step 480070 run_train: loss = 5.3027  (0.155 sec)
18-06-05 03:41-INFO->> Step 480080 run_train: loss = 5.4090  (0.149 sec)
18-06-05 03:41-INFO->> Step 480090 run_train: loss = 5.3768  (0.173 sec)
18-06-05 03:41-INFO->> Step 480100 run_train: loss = 5.4344  (0.151 sec)
18-06-05 03:41-INFO->> Step 480110 run_train: loss = 5.4049  (0.122 sec)
18-06-05 03:41-INFO->> Step 480120 run_train: loss = 5.3540  (0.137 sec)
18-06-05 03:41-INFO->> Step 480130 run_train: loss = 5.4180  (0.156 sec)
18-06-05 03:41-INFO->> Step 480140 run_train: loss = 5.3590  (0.167 sec)
18-06-05 03:41-INFO->> Step 480150 run_train: loss = 5.3963  (0.158 sec)
18-06-05 03:41-INFO->> Step 480160 run_train: loss = 5.3779  (0.149 sec)
18-06-05 03:41-INFO->> Step 480170 run_train: loss = 5.4061  (0.173 sec)
18-06-05 03:41-INFO->> Step 480180 run_train: loss = 5.3852  (0.166 sec)
18-06-05 03:41-INFO->> Step 480190 run_train: loss = 5.4170  (0.191 sec)
18-06-05 03:42-INFO->> Step 480200 run_train: loss = 5.4427  (0.156 sec)
18-06-05 03:42-INFO->> Step 480210 run_train: loss = 5.3833  (0.160 sec)
18-06-05 03:42-INFO->> Step 480220 run_train: loss = 5.4316  (0.129 sec)
18-06-05 03:42-INFO->> Step 480230 run_train: loss = 5.4090  (0.153 sec)
18-06-05 03:42-INFO->> Step 480240 run_train: loss = 5.4359  (0.167 sec)
18-06-05 03:42-INFO->> Step 480250 run_train: loss = 5.3987  (0.193 sec)
18-06-05 03:42-INFO->> Step 480260 run_train: loss = 5.4126  (0.157 sec)
18-06-05 03:42-INFO->> Step 480270 run_train: loss = 5.3926  (0.163 sec)
18-06-05 03:42-INFO->> Step 480280 run_train: loss = 5.4377  (0.121 sec)
18-06-05 03:42-INFO->> Step 480290 run_train: loss = 5.3874  (0.176 sec)
18-06-05 03:42-INFO->> Step 480300 run_train: loss = 5.4101  (0.154 sec)
18-06-05 03:42-INFO->> Step 480310 run_train: loss = 5.4504  (0.164 sec)
18-06-05 03:42-INFO->> Step 480320 run_train: loss = 5.3755  (0.147 sec)
18-06-05 03:42-INFO->> Step 480330 run_train: loss = 5.4248  (0.164 sec)
18-06-05 03:42-INFO->> Step 480340 run_train: loss = 5.4739  (0.161 sec)
18-06-05 03:42-INFO->> Step 480350 run_train: loss = 5.4030  (0.147 sec)
18-06-05 03:42-INFO->> Step 480360 run_train: loss = 5.3949  (0.142 sec)
18-06-05 03:42-INFO->> Step 480370 run_train: loss = 5.4205  (0.155 sec)
18-06-05 03:42-INFO->> Step 480380 run_train: loss = 5.4615  (0.178 sec)
18-06-05 03:42-INFO->> Step 480390 run_train: loss = 5.4076  (0.165 sec)
18-06-05 03:42-INFO->> Step 480400 run_train: loss = 5.4521  (0.169 sec)
18-06-05 03:42-INFO->> Step 480410 run_train: loss = 5.4032  (0.143 sec)
18-06-05 03:42-INFO->> Step 480420 run_train: loss = 5.4283  (0.136 sec)
18-06-05 03:42-INFO->> Step 480430 run_train: loss = 5.4180  (0.188 sec)
18-06-05 03:42-INFO->> Step 480440 run_train: loss = 5.3991  (0.145 sec)
18-06-05 03:42-INFO->> Step 480450 run_train: loss = 5.3668  (0.152 sec)
18-06-05 03:42-INFO->> Step 480460 run_train: loss = 5.4261  (0.166 sec)
18-06-05 03:42-INFO->> Step 480470 run_train: loss = 5.3479  (0.152 sec)
18-06-05 03:42-INFO->> Step 480480 run_train: loss = 5.4063  (0.185 sec)
18-06-05 03:42-INFO->> Step 480490 run_train: loss = 5.4224  (0.118 sec)
18-06-05 03:42-INFO->> Step 480500 run_train: loss = 5.4151  (0.149 sec)
18-06-05 03:42-INFO->> Step 480510 run_train: loss = 5.3954  (0.150 sec)
18-06-05 03:42-INFO->> Step 480520 run_train: loss = 5.3689  (0.157 sec)
18-06-05 03:42-INFO->> Step 480530 run_train: loss = 5.2920  (0.159 sec)
18-06-05 03:42-INFO->> Step 480540 run_train: loss = 5.3731  (0.154 sec)
18-06-05 03:42-INFO->> Step 480550 run_train: loss = 5.4445  (0.162 sec)
18-06-05 03:42-INFO->> Step 480560 run_train: loss = 5.3685  (0.166 sec)
18-06-05 03:42-INFO->> Step 480570 run_train: loss = 5.4691  (0.155 sec)
18-06-05 03:43-INFO->> Step 480580 run_train: loss = 5.4307  (0.131 sec)
18-06-05 03:43-INFO->> Step 480590 run_train: loss = 5.4262  (0.173 sec)
18-06-05 03:43-INFO->> Step 480600 run_train: loss = 5.3933  (0.158 sec)
18-06-05 03:43-INFO->> Step 480610 run_train: loss = 5.4681  (0.149 sec)
18-06-05 03:43-INFO->> Step 480620 run_train: loss = 5.3788  (0.172 sec)
18-06-05 03:43-INFO->> Step 480630 run_train: loss = 5.4683  (0.157 sec)
18-06-05 03:43-INFO->> Step 480640 run_train: loss = 5.3773  (0.165 sec)
18-06-05 03:43-INFO->> Step 480650 run_train: loss = 5.4240  (0.144 sec)
18-06-05 03:43-INFO->> Step 480660 run_train: loss = 5.4317  (0.128 sec)
18-06-05 03:43-INFO->> Step 480670 run_train: loss = 5.4248  (0.160 sec)
18-06-05 03:43-INFO->> Step 480680 run_train: loss = 5.4415  (0.136 sec)
18-06-05 03:43-INFO->> Step 480690 run_train: loss = 5.4694  (0.148 sec)
18-06-05 03:43-INFO->> Step 480700 run_train: loss = 5.3305  (0.148 sec)
18-06-05 03:43-INFO->> Step 480710 run_train: loss = 5.3275  (0.158 sec)
18-06-05 03:43-INFO->> Step 480720 run_train: loss = 5.3670  (0.148 sec)
18-06-05 03:43-INFO->> Step 480730 run_train: loss = 5.3862  (0.147 sec)
18-06-05 03:43-INFO->> Step 480740 run_train: loss = 5.4511  (0.158 sec)
18-06-05 03:43-INFO->> Step 480750 run_train: loss = 5.4170  (0.144 sec)
18-06-05 03:43-INFO->> Step 480760 run_train: loss = 5.4468  (0.146 sec)
18-06-05 03:43-INFO->> Step 480770 run_train: loss = 5.3990  (0.200 sec)
18-06-05 03:43-INFO->> Step 480780 run_train: loss = 5.3820  (0.151 sec)
18-06-05 03:43-INFO->> Step 480790 run_train: loss = 5.4356  (0.152 sec)
18-06-05 03:43-INFO->> Step 480800 run_train: loss = 5.3778  (0.211 sec)
18-06-05 03:43-INFO->> Step 480810 run_train: loss = 5.3405  (0.114 sec)
18-06-05 03:43-INFO->> Step 480820 run_train: loss = 5.3834  (0.118 sec)
18-06-05 03:43-INFO->> Step 480830 run_train: loss = 5.4201  (0.132 sec)
18-06-05 03:43-INFO->> Step 480840 run_train: loss = 5.3806  (0.180 sec)
18-06-05 03:43-INFO->> Step 480850 run_train: loss = 5.4455  (0.151 sec)
18-06-05 03:43-INFO->> Step 480860 run_train: loss = 5.3705  (0.143 sec)
18-06-05 03:43-INFO->> Step 480870 run_train: loss = 5.3872  (0.154 sec)
18-06-05 03:43-INFO->> Step 480880 run_train: loss = 5.3900  (0.170 sec)
18-06-05 03:43-INFO->> Step 480890 run_train: loss = 5.3865  (0.175 sec)
18-06-05 03:43-INFO->> Step 480900 run_train: loss = 5.4473  (0.141 sec)
18-06-05 03:43-INFO->> Step 480910 run_train: loss = 5.3831  (0.165 sec)
18-06-05 03:43-INFO->> Step 480920 run_train: loss = 5.4105  (0.191 sec)
18-06-05 03:43-INFO->> Step 480930 run_train: loss = 5.3889  (0.148 sec)
18-06-05 03:43-INFO->> Step 480940 run_train: loss = 5.3967  (0.145 sec)
18-06-05 03:43-INFO->> Step 480950 run_train: loss = 5.3997  (0.124 sec)
18-06-05 03:44-INFO->> Step 480960 run_train: loss = 5.4195  (0.187 sec)
18-06-05 03:44-INFO->> Step 480970 run_train: loss = 5.3429  (0.204 sec)
18-06-05 03:44-INFO->> Step 480980 run_train: loss = 5.4419  (0.187 sec)
18-06-05 03:44-INFO->> Step 480990 run_train: loss = 5.3704  (0.186 sec)
18-06-05 03:44-INFO->> Step 481000 run_train: loss = 5.4705  (0.138 sec)
18-06-05 03:44-INFO->> 2018-06-05 03:44:07.380047 Saving in ckpt
18-06-05 03:44-INFO-Test Data Eval:
18-06-05 03:44-INFO-fpr95 = 0.17152630180658873 and auc = 0.969302955606444
18-06-05 03:44-INFO->> Step 481010 run_train: loss = 5.4613  (0.175 sec)
18-06-05 03:44-INFO->> Step 481020 run_train: loss = 5.4499  (0.149 sec)
18-06-05 03:44-INFO->> Step 481030 run_train: loss = 5.3760  (0.180 sec)
18-06-05 03:44-INFO->> Step 481040 run_train: loss = 5.4162  (0.166 sec)
18-06-05 03:44-INFO->> Step 481050 run_train: loss = 5.3693  (0.126 sec)
18-06-05 03:44-INFO->> Step 481060 run_train: loss = 5.4078  (0.154 sec)
18-06-05 03:44-INFO->> Step 481070 run_train: loss = 5.4478  (0.178 sec)
18-06-05 03:45-INFO->> Step 481080 run_train: loss = 5.4394  (0.107 sec)
18-06-05 03:45-INFO->> Step 481090 run_train: loss = 5.3934  (0.151 sec)
18-06-05 03:45-INFO->> Step 481100 run_train: loss = 5.4186  (0.178 sec)
18-06-05 03:45-INFO->> Step 481110 run_train: loss = 5.3423  (0.165 sec)
18-06-05 03:45-INFO->> Step 481120 run_train: loss = 5.4241  (0.154 sec)
18-06-05 03:45-INFO->> Step 481130 run_train: loss = 5.4382  (0.202 sec)
18-06-05 03:45-INFO->> Step 481140 run_train: loss = 5.3996  (0.226 sec)
18-06-05 03:45-INFO->> Step 481150 run_train: loss = 5.4115  (0.157 sec)
18-06-05 03:45-INFO->> Step 481160 run_train: loss = 5.3517  (0.146 sec)
18-06-05 03:45-INFO->> Step 481170 run_train: loss = 5.3977  (0.142 sec)
18-06-05 03:45-INFO->> Step 481180 run_train: loss = 5.3829  (0.120 sec)
18-06-05 03:45-INFO->> Step 481190 run_train: loss = 5.3674  (0.167 sec)
18-06-05 03:45-INFO->> Step 481200 run_train: loss = 5.3957  (0.165 sec)
18-06-05 03:45-INFO->> Step 481210 run_train: loss = 5.3758  (0.163 sec)
18-06-05 03:45-INFO->> Step 481220 run_train: loss = 5.4594  (0.134 sec)
18-06-05 03:45-INFO->> Step 481230 run_train: loss = 5.3561  (0.168 sec)
18-06-05 03:45-INFO->> Step 481240 run_train: loss = 5.4553  (0.155 sec)
18-06-05 03:45-INFO->> Step 481250 run_train: loss = 5.4103  (0.179 sec)
18-06-05 03:45-INFO->> Step 481260 run_train: loss = 5.4543  (0.123 sec)
18-06-05 03:45-INFO->> Step 481270 run_train: loss = 5.3963  (0.199 sec)
18-06-05 03:45-INFO->> Step 481280 run_train: loss = 5.4384  (0.181 sec)
18-06-05 03:45-INFO->> Step 481290 run_train: loss = 5.3508  (0.135 sec)
18-06-05 03:45-INFO->> Step 481300 run_train: loss = 5.4207  (0.117 sec)
18-06-05 03:45-INFO->> Step 481310 run_train: loss = 5.3421  (0.155 sec)
18-06-05 03:45-INFO->> Step 481320 run_train: loss = 5.3887  (0.143 sec)
18-06-05 03:45-INFO->> Step 481330 run_train: loss = 5.4291  (0.122 sec)
18-06-05 03:45-INFO->> Step 481340 run_train: loss = 5.3982  (0.157 sec)
18-06-05 03:45-INFO->> Step 481350 run_train: loss = 5.4463  (0.141 sec)
18-06-05 03:45-INFO->> Step 481360 run_train: loss = 5.4565  (0.160 sec)
18-06-05 03:45-INFO->> Step 481370 run_train: loss = 5.3696  (0.140 sec)
18-06-05 03:45-INFO->> Step 481380 run_train: loss = 5.3677  (0.178 sec)
18-06-05 03:45-INFO->> Step 481390 run_train: loss = 5.3911  (0.155 sec)
18-06-05 03:45-INFO->> Step 481400 run_train: loss = 5.4158  (0.151 sec)
18-06-05 03:45-INFO->> Step 481410 run_train: loss = 5.4103  (0.141 sec)
18-06-05 03:45-INFO->> Step 481420 run_train: loss = 5.3848  (0.157 sec)
18-06-05 03:45-INFO->> Step 481430 run_train: loss = 5.3852  (0.163 sec)
18-06-05 03:45-INFO->> Step 481440 run_train: loss = 5.4039  (0.157 sec)
18-06-05 03:45-INFO->> Step 481450 run_train: loss = 5.3666  (0.197 sec)
18-06-05 03:46-INFO->> Step 481460 run_train: loss = 5.3992  (0.122 sec)
18-06-05 03:46-INFO->> Step 481470 run_train: loss = 5.3775  (0.146 sec)
18-06-05 03:46-INFO->> Step 481480 run_train: loss = 5.4160  (0.177 sec)
18-06-05 03:46-INFO->> Step 481490 run_train: loss = 5.4343  (0.144 sec)
18-06-05 03:46-INFO->> Step 481500 run_train: loss = 5.4086  (0.155 sec)
18-06-05 03:46-INFO->> Step 481510 run_train: loss = 5.3837  (0.156 sec)
18-06-05 03:46-INFO->> Step 481520 run_train: loss = 5.4440  (0.139 sec)
18-06-05 03:46-INFO->> Step 481530 run_train: loss = 5.4023  (0.162 sec)
18-06-05 03:46-INFO->> Step 481540 run_train: loss = 5.4012  (0.171 sec)
18-06-05 03:46-INFO->> Step 481550 run_train: loss = 5.4188  (0.142 sec)
18-06-05 03:46-INFO->> Step 481560 run_train: loss = 5.3354  (0.150 sec)
18-06-05 03:46-INFO->> Step 481570 run_train: loss = 5.3962  (0.177 sec)
18-06-05 03:46-INFO->> Step 481580 run_train: loss = 5.4161  (0.195 sec)
18-06-05 03:46-INFO->> Step 481590 run_train: loss = 5.3128  (0.169 sec)
18-06-05 03:46-INFO->> Step 481600 run_train: loss = 5.4204  (0.172 sec)
18-06-05 03:46-INFO->> Step 481610 run_train: loss = 5.3540  (0.149 sec)
18-06-05 03:46-INFO->> Step 481620 run_train: loss = 5.4409  (0.155 sec)
18-06-05 03:46-INFO->> Step 481630 run_train: loss = 5.4210  (0.152 sec)
18-06-05 03:46-INFO->> Step 481640 run_train: loss = 5.4179  (0.142 sec)
18-06-05 03:46-INFO->> Step 481650 run_train: loss = 5.3870  (0.129 sec)
18-06-05 03:46-INFO->> Step 481660 run_train: loss = 5.3801  (0.181 sec)
18-06-05 03:46-INFO->> Step 481670 run_train: loss = 5.3175  (0.175 sec)
18-06-05 03:46-INFO->> Step 481680 run_train: loss = 5.3719  (0.151 sec)
18-06-05 03:46-INFO->> Step 481690 run_train: loss = 5.4376  (0.145 sec)
18-06-05 03:46-INFO->> Step 481700 run_train: loss = 5.4354  (0.163 sec)
18-06-05 03:46-INFO->> Step 481710 run_train: loss = 5.4303  (0.129 sec)
18-06-05 03:46-INFO->> Step 481720 run_train: loss = 5.4352  (0.157 sec)
18-06-05 03:46-INFO->> Step 481730 run_train: loss = 5.3656  (0.134 sec)
18-06-05 03:46-INFO->> Step 481740 run_train: loss = 5.4234  (0.184 sec)
18-06-05 03:46-INFO->> Step 481750 run_train: loss = 5.3460  (0.161 sec)
18-06-05 03:46-INFO->> Step 481760 run_train: loss = 5.4229  (0.169 sec)
18-06-05 03:46-INFO->> Step 481770 run_train: loss = 5.4783  (0.190 sec)
18-06-05 03:46-INFO->> Step 481780 run_train: loss = 5.4381  (0.163 sec)
18-06-05 03:46-INFO->> Step 481790 run_train: loss = 5.4362  (0.167 sec)
18-06-05 03:46-INFO->> Step 481800 run_train: loss = 5.4101  (0.152 sec)
18-06-05 03:46-INFO->> Step 481810 run_train: loss = 5.3944  (0.157 sec)
18-06-05 03:46-INFO->> Step 481820 run_train: loss = 5.4048  (0.156 sec)
18-06-05 03:46-INFO->> Step 481830 run_train: loss = 5.3903  (0.165 sec)
18-06-05 03:47-INFO->> Step 481840 run_train: loss = 5.3624  (0.158 sec)
18-06-05 03:47-INFO->> Step 481850 run_train: loss = 5.4183  (0.184 sec)
18-06-05 03:47-INFO->> Step 481860 run_train: loss = 5.4594  (0.181 sec)
18-06-05 03:47-INFO->> Step 481870 run_train: loss = 5.4066  (0.147 sec)
18-06-05 03:47-INFO->> Step 481880 run_train: loss = 5.4440  (0.170 sec)
18-06-05 03:47-INFO->> Step 481890 run_train: loss = 5.3942  (0.160 sec)
18-06-05 03:47-INFO->> Step 481900 run_train: loss = 5.4344  (0.188 sec)
18-06-05 03:47-INFO->> Step 481910 run_train: loss = 5.4080  (0.148 sec)
18-06-05 03:47-INFO->> Step 481920 run_train: loss = 5.4128  (0.160 sec)
18-06-05 03:47-INFO->> Step 481930 run_train: loss = 5.4726  (0.192 sec)
18-06-05 03:47-INFO->> Step 481940 run_train: loss = 5.3905  (0.184 sec)
18-06-05 03:47-INFO->> Step 481950 run_train: loss = 5.3603  (0.161 sec)
18-06-05 03:47-INFO->> Step 481960 run_train: loss = 5.3836  (0.147 sec)
18-06-05 03:47-INFO->> Step 481970 run_train: loss = 5.3457  (0.171 sec)
18-06-05 03:47-INFO->> Step 481980 run_train: loss = 5.4110  (0.160 sec)
18-06-05 03:47-INFO->> Step 481990 run_train: loss = 5.4209  (0.140 sec)
18-06-05 03:47-INFO->> Step 482000 run_train: loss = 5.3544  (0.178 sec)
18-06-05 03:47-INFO->> 2018-06-05 03:47:26.518230 Saving in ckpt
18-06-05 03:47-INFO-Test Data Eval:
18-06-05 03:48-INFO-fpr95 = 0.17683979808714134 and auc = 0.9686931178393328
18-06-05 03:48-INFO->> Step 482010 run_train: loss = 5.3492  (0.152 sec)
18-06-05 03:48-INFO->> Step 482020 run_train: loss = 5.4191  (0.154 sec)
18-06-05 03:48-INFO->> Step 482030 run_train: loss = 5.4416  (0.150 sec)
18-06-05 03:48-INFO->> Step 482040 run_train: loss = 5.3858  (0.142 sec)
18-06-05 03:48-INFO->> Step 482050 run_train: loss = 5.3443  (0.161 sec)
18-06-05 03:48-INFO->> Step 482060 run_train: loss = 5.4339  (0.182 sec)
18-06-05 03:48-INFO->> Step 482070 run_train: loss = 5.3303  (0.163 sec)
18-06-05 03:48-INFO->> Step 482080 run_train: loss = 5.3840  (0.142 sec)
18-06-05 03:48-INFO->> Step 482090 run_train: loss = 5.3528  (0.192 sec)
18-06-05 03:48-INFO->> Step 482100 run_train: loss = 5.4540  (0.171 sec)
18-06-05 03:48-INFO->> Step 482110 run_train: loss = 5.3765  (0.137 sec)
18-06-05 03:48-INFO->> Step 482120 run_train: loss = 5.3963  (0.162 sec)
18-06-05 03:48-INFO->> Step 482130 run_train: loss = 5.4510  (0.175 sec)
18-06-05 03:48-INFO->> Step 482140 run_train: loss = 5.3786  (0.153 sec)
18-06-05 03:48-INFO->> Step 482150 run_train: loss = 5.3703  (0.171 sec)
18-06-05 03:48-INFO->> Step 482160 run_train: loss = 5.4381  (0.185 sec)
18-06-05 03:48-INFO->> Step 482170 run_train: loss = 5.4678  (0.153 sec)
18-06-05 03:48-INFO->> Step 482180 run_train: loss = 5.3921  (0.158 sec)
18-06-05 03:48-INFO->> Step 482190 run_train: loss = 5.3530  (0.169 sec)
18-06-05 03:48-INFO->> Step 482200 run_train: loss = 5.4063  (0.174 sec)
18-06-05 03:48-INFO->> Step 482210 run_train: loss = 5.3980  (0.130 sec)
18-06-05 03:48-INFO->> Step 482220 run_train: loss = 5.4076  (0.216 sec)
18-06-05 03:48-INFO->> Step 482230 run_train: loss = 5.4244  (0.176 sec)
18-06-05 03:48-INFO->> Step 482240 run_train: loss = 5.4080  (0.169 sec)
18-06-05 03:48-INFO->> Step 482250 run_train: loss = 5.3721  (0.125 sec)
18-06-05 03:48-INFO->> Step 482260 run_train: loss = 5.3922  (0.143 sec)
18-06-05 03:48-INFO->> Step 482270 run_train: loss = 5.4086  (0.127 sec)
18-06-05 03:48-INFO->> Step 482280 run_train: loss = 5.3469  (0.167 sec)
18-06-05 03:48-INFO->> Step 482290 run_train: loss = 5.4606  (0.138 sec)
18-06-05 03:48-INFO->> Step 482300 run_train: loss = 5.4009  (0.145 sec)
18-06-05 03:48-INFO->> Step 482310 run_train: loss = 5.4459  (0.198 sec)
18-06-05 03:48-INFO->> Step 482320 run_train: loss = 5.3661  (0.169 sec)
18-06-05 03:48-INFO->> Step 482330 run_train: loss = 5.3232  (0.171 sec)
18-06-05 03:49-INFO->> Step 482340 run_train: loss = 5.4219  (0.132 sec)
18-06-05 03:49-INFO->> Step 482350 run_train: loss = 5.4746  (0.122 sec)
18-06-05 03:49-INFO->> Step 482360 run_train: loss = 5.4660  (0.113 sec)
18-06-05 03:49-INFO->> Step 482370 run_train: loss = 5.3946  (0.191 sec)
18-06-05 03:49-INFO->> Step 482380 run_train: loss = 5.3906  (0.169 sec)
18-06-05 03:49-INFO->> Step 482390 run_train: loss = 5.3837  (0.169 sec)
18-06-05 03:49-INFO->> Step 482400 run_train: loss = 5.3629  (0.169 sec)
18-06-05 03:49-INFO->> Step 482410 run_train: loss = 5.4193  (0.156 sec)
18-06-05 03:49-INFO->> Step 482420 run_train: loss = 5.3692  (0.167 sec)
18-06-05 03:49-INFO->> Step 482430 run_train: loss = 5.3953  (0.151 sec)
18-06-05 03:49-INFO->> Step 482440 run_train: loss = 5.3884  (0.158 sec)
18-06-05 03:49-INFO->> Step 482450 run_train: loss = 5.3829  (0.159 sec)
18-06-05 03:49-INFO->> Step 482460 run_train: loss = 5.3758  (0.134 sec)
18-06-05 03:49-INFO->> Step 482470 run_train: loss = 5.4074  (0.164 sec)
18-06-05 03:49-INFO->> Step 482480 run_train: loss = 5.3933  (0.151 sec)
18-06-05 03:49-INFO->> Step 482490 run_train: loss = 5.4416  (0.125 sec)
18-06-05 03:49-INFO->> Step 482500 run_train: loss = 5.4472  (0.152 sec)
18-06-05 03:49-INFO->> Step 482510 run_train: loss = 5.3715  (0.193 sec)
18-06-05 03:49-INFO->> Step 482520 run_train: loss = 5.4163  (0.119 sec)
18-06-05 03:49-INFO->> Step 482530 run_train: loss = 5.4377  (0.164 sec)
18-06-05 03:49-INFO->> Step 482540 run_train: loss = 5.4215  (0.137 sec)
18-06-05 03:49-INFO->> Step 482550 run_train: loss = 5.3836  (0.172 sec)
18-06-05 03:49-INFO->> Step 482560 run_train: loss = 5.4086  (0.138 sec)
18-06-05 03:49-INFO->> Step 482570 run_train: loss = 5.3969  (0.188 sec)
18-06-05 03:49-INFO->> Step 482580 run_train: loss = 5.3943  (0.166 sec)
18-06-05 03:49-INFO->> Step 482590 run_train: loss = 5.3907  (0.179 sec)
18-06-05 03:49-INFO->> Step 482600 run_train: loss = 5.4177  (0.178 sec)
18-06-05 03:49-INFO->> Step 482610 run_train: loss = 5.3607  (0.124 sec)
18-06-05 03:49-INFO->> Step 482620 run_train: loss = 5.4377  (0.152 sec)
18-06-05 03:49-INFO->> Step 482630 run_train: loss = 5.4041  (0.139 sec)
18-06-05 03:49-INFO->> Step 482640 run_train: loss = 5.3623  (0.129 sec)
18-06-05 03:49-INFO->> Step 482650 run_train: loss = 5.4120  (0.198 sec)
18-06-05 03:49-INFO->> Step 482660 run_train: loss = 5.4136  (0.166 sec)
18-06-05 03:49-INFO->> Step 482670 run_train: loss = 5.4193  (0.163 sec)
18-06-05 03:49-INFO->> Step 482680 run_train: loss = 5.4075  (0.132 sec)
18-06-05 03:49-INFO->> Step 482690 run_train: loss = 5.4131  (0.149 sec)
18-06-05 03:49-INFO->> Step 482700 run_train: loss = 5.4411  (0.163 sec)
18-06-05 03:49-INFO->> Step 482710 run_train: loss = 5.4418  (0.165 sec)
18-06-05 03:50-INFO->> Step 482720 run_train: loss = 5.4081  (0.148 sec)
18-06-05 03:50-INFO->> Step 482730 run_train: loss = 5.4320  (0.156 sec)
18-06-05 03:50-INFO->> Step 482740 run_train: loss = 5.3640  (0.137 sec)
18-06-05 03:50-INFO->> Step 482750 run_train: loss = 5.3754  (0.147 sec)
18-06-05 03:50-INFO->> Step 482760 run_train: loss = 5.4011  (0.188 sec)
18-06-05 03:50-INFO->> Step 482770 run_train: loss = 5.4164  (0.204 sec)
18-06-05 03:50-INFO->> Step 482780 run_train: loss = 5.3918  (0.153 sec)
18-06-05 03:50-INFO->> Step 482790 run_train: loss = 5.3954  (0.168 sec)
18-06-05 03:50-INFO->> Step 482800 run_train: loss = 5.4660  (0.131 sec)
18-06-05 03:50-INFO->> Step 482810 run_train: loss = 5.3259  (0.163 sec)
18-06-05 03:50-INFO->> Step 482820 run_train: loss = 5.4140  (0.190 sec)
18-06-05 03:50-INFO->> Step 482830 run_train: loss = 5.4204  (0.160 sec)
18-06-05 03:50-INFO->> Step 482840 run_train: loss = 5.3965  (0.186 sec)
18-06-05 03:50-INFO->> Step 482850 run_train: loss = 5.3619  (0.127 sec)
18-06-05 03:50-INFO->> Step 482860 run_train: loss = 5.4048  (0.172 sec)
18-06-05 03:50-INFO->> Step 482870 run_train: loss = 5.3994  (0.117 sec)
18-06-05 03:50-INFO->> Step 482880 run_train: loss = 5.4043  (0.199 sec)
18-06-05 03:50-INFO->> Step 482890 run_train: loss = 5.4651  (0.145 sec)
18-06-05 03:50-INFO->> Step 482900 run_train: loss = 5.3978  (0.131 sec)
18-06-05 03:50-INFO->> Step 482910 run_train: loss = 5.3887  (0.164 sec)
18-06-05 03:50-INFO->> Step 482920 run_train: loss = 5.4053  (0.218 sec)
18-06-05 03:50-INFO->> Step 482930 run_train: loss = 5.3784  (0.155 sec)
18-06-05 03:50-INFO->> Step 482940 run_train: loss = 5.3589  (0.147 sec)
18-06-05 03:50-INFO->> Step 482950 run_train: loss = 5.3744  (0.152 sec)
18-06-05 03:50-INFO->> Step 482960 run_train: loss = 5.4142  (0.148 sec)
18-06-05 03:50-INFO->> Step 482970 run_train: loss = 5.4244  (0.178 sec)
18-06-05 03:50-INFO->> Step 482980 run_train: loss = 5.3676  (0.140 sec)
18-06-05 03:50-INFO->> Step 482990 run_train: loss = 5.4104  (0.170 sec)
18-06-05 03:50-INFO->> Step 483000 run_train: loss = 5.4306  (0.128 sec)
18-06-05 03:50-INFO->> 2018-06-05 03:50:45.795075 Saving in ckpt
18-06-05 03:50-INFO-Test Data Eval:
18-06-05 03:51-INFO-fpr95 = 0.1746479808714134 and auc = 0.9689142636047813
18-06-05 03:51-INFO->> Step 483010 run_train: loss = 5.3502  (0.198 sec)
18-06-05 03:51-INFO->> Step 483020 run_train: loss = 5.3540  (0.163 sec)
18-06-05 03:51-INFO->> Step 483030 run_train: loss = 5.4257  (0.147 sec)
18-06-05 03:51-INFO->> Step 483040 run_train: loss = 5.4439  (0.158 sec)
18-06-05 03:51-INFO->> Step 483050 run_train: loss = 5.3910  (0.196 sec)
18-06-05 03:51-INFO->> Step 483060 run_train: loss = 5.4241  (0.168 sec)
18-06-05 03:51-INFO->> Step 483070 run_train: loss = 5.4488  (0.204 sec)
18-06-05 03:51-INFO->> Step 483080 run_train: loss = 5.4154  (0.134 sec)
18-06-05 03:51-INFO->> Step 483090 run_train: loss = 5.4059  (0.155 sec)
18-06-05 03:51-INFO->> Step 483100 run_train: loss = 5.3681  (0.157 sec)
18-06-05 03:51-INFO->> Step 483110 run_train: loss = 5.4086  (0.141 sec)
18-06-05 03:51-INFO->> Step 483120 run_train: loss = 5.3454  (0.172 sec)
18-06-05 03:51-INFO->> Step 483130 run_train: loss = 5.4310  (0.119 sec)
18-06-05 03:51-INFO->> Step 483140 run_train: loss = 5.4414  (0.137 sec)
18-06-05 03:51-INFO->> Step 483150 run_train: loss = 5.4141  (0.151 sec)
18-06-05 03:51-INFO->> Step 483160 run_train: loss = 5.4223  (0.169 sec)
18-06-05 03:51-INFO->> Step 483170 run_train: loss = 5.4032  (0.164 sec)
18-06-05 03:51-INFO->> Step 483180 run_train: loss = 5.4507  (0.186 sec)
18-06-05 03:51-INFO->> Step 483190 run_train: loss = 5.4364  (0.147 sec)
18-06-05 03:51-INFO->> Step 483200 run_train: loss = 5.3970  (0.164 sec)
18-06-05 03:51-INFO->> Step 483210 run_train: loss = 5.4573  (0.177 sec)
18-06-05 03:52-INFO->> Step 483220 run_train: loss = 5.3254  (0.162 sec)
18-06-05 03:52-INFO->> Step 483230 run_train: loss = 5.3837  (0.158 sec)
18-06-05 03:52-INFO->> Step 483240 run_train: loss = 5.3710  (0.168 sec)
18-06-05 03:52-INFO->> Step 483250 run_train: loss = 5.3997  (0.141 sec)
18-06-05 03:52-INFO->> Step 483260 run_train: loss = 5.4158  (0.171 sec)
18-06-05 03:52-INFO->> Step 483270 run_train: loss = 5.4082  (0.163 sec)
18-06-05 03:52-INFO->> Step 483280 run_train: loss = 5.4036  (0.183 sec)
18-06-05 03:52-INFO->> Step 483290 run_train: loss = 5.4679  (0.143 sec)
18-06-05 03:52-INFO->> Step 483300 run_train: loss = 5.4739  (0.162 sec)
18-06-05 03:52-INFO->> Step 483310 run_train: loss = 5.3385  (0.153 sec)
18-06-05 03:52-INFO->> Step 483320 run_train: loss = 5.4048  (0.190 sec)
18-06-05 03:52-INFO->> Step 483330 run_train: loss = 5.4405  (0.154 sec)
18-06-05 03:52-INFO->> Step 483340 run_train: loss = 5.3501  (0.149 sec)
18-06-05 03:52-INFO->> Step 483350 run_train: loss = 5.4428  (0.154 sec)
18-06-05 03:52-INFO->> Step 483360 run_train: loss = 5.3868  (0.133 sec)
18-06-05 03:52-INFO->> Step 483370 run_train: loss = 5.3825  (0.164 sec)
18-06-05 03:52-INFO->> Step 483380 run_train: loss = 5.4217  (0.149 sec)
18-06-05 03:52-INFO->> Step 483390 run_train: loss = 5.4039  (0.150 sec)
18-06-05 03:52-INFO->> Step 483400 run_train: loss = 5.4652  (0.148 sec)
18-06-05 03:52-INFO->> Step 483410 run_train: loss = 5.4389  (0.170 sec)
18-06-05 03:52-INFO->> Step 483420 run_train: loss = 5.3286  (0.176 sec)
18-06-05 03:52-INFO->> Step 483430 run_train: loss = 5.3759  (0.143 sec)
18-06-05 03:52-INFO->> Step 483440 run_train: loss = 5.3959  (0.132 sec)
18-06-05 03:52-INFO->> Step 483450 run_train: loss = 5.4708  (0.156 sec)
18-06-05 03:52-INFO->> Step 483460 run_train: loss = 5.3952  (0.158 sec)
18-06-05 03:52-INFO->> Step 483470 run_train: loss = 5.4254  (0.139 sec)
18-06-05 03:52-INFO->> Step 483480 run_train: loss = 5.3750  (0.188 sec)
18-06-05 03:52-INFO->> Step 483490 run_train: loss = 5.4419  (0.139 sec)
18-06-05 03:52-INFO->> Step 483500 run_train: loss = 5.4261  (0.182 sec)
18-06-05 03:52-INFO->> Step 483510 run_train: loss = 5.4561  (0.168 sec)
18-06-05 03:52-INFO->> Step 483520 run_train: loss = 5.4331  (0.198 sec)
18-06-05 03:52-INFO->> Step 483530 run_train: loss = 5.4892  (0.169 sec)
18-06-05 03:52-INFO->> Step 483540 run_train: loss = 5.4282  (0.124 sec)
18-06-05 03:52-INFO->> Step 483550 run_train: loss = 5.4763  (0.149 sec)
18-06-05 03:52-INFO->> Step 483560 run_train: loss = 5.3560  (0.179 sec)
18-06-05 03:52-INFO->> Step 483570 run_train: loss = 5.3735  (0.141 sec)
18-06-05 03:52-INFO->> Step 483580 run_train: loss = 5.4285  (0.154 sec)
18-06-05 03:52-INFO->> Step 483590 run_train: loss = 5.4056  (0.160 sec)
18-06-05 03:53-INFO->> Step 483600 run_train: loss = 5.4022  (0.193 sec)
18-06-05 03:53-INFO->> Step 483610 run_train: loss = 5.4089  (0.127 sec)
18-06-05 03:53-INFO->> Step 483620 run_train: loss = 5.4048  (0.134 sec)
18-06-05 03:53-INFO->> Step 483630 run_train: loss = 5.3780  (0.145 sec)
18-06-05 03:53-INFO->> Step 483640 run_train: loss = 5.3610  (0.160 sec)
18-06-05 03:53-INFO->> Step 483650 run_train: loss = 5.4203  (0.166 sec)
18-06-05 03:53-INFO->> Step 483660 run_train: loss = 5.4523  (0.136 sec)
18-06-05 03:53-INFO->> Step 483670 run_train: loss = 5.3899  (0.134 sec)
18-06-05 03:53-INFO->> Step 483680 run_train: loss = 5.3799  (0.170 sec)
18-06-05 03:53-INFO->> Step 483690 run_train: loss = 5.4057  (0.156 sec)
18-06-05 03:53-INFO->> Step 483700 run_train: loss = 5.4065  (0.172 sec)
18-06-05 03:53-INFO->> Step 483710 run_train: loss = 5.4357  (0.157 sec)
18-06-05 03:53-INFO->> Step 483720 run_train: loss = 5.3383  (0.203 sec)
18-06-05 03:53-INFO->> Step 483730 run_train: loss = 5.4238  (0.159 sec)
18-06-05 03:53-INFO->> Step 483740 run_train: loss = 5.4427  (0.163 sec)
18-06-05 03:53-INFO->> Step 483750 run_train: loss = 5.4172  (0.171 sec)
18-06-05 03:53-INFO->> Step 483760 run_train: loss = 5.3990  (0.173 sec)
18-06-05 03:53-INFO->> Step 483770 run_train: loss = 5.3994  (0.165 sec)
18-06-05 03:53-INFO->> Step 483780 run_train: loss = 5.4149  (0.153 sec)
18-06-05 03:53-INFO->> Step 483790 run_train: loss = 5.4187  (0.162 sec)
18-06-05 03:53-INFO->> Step 483800 run_train: loss = 5.4566  (0.178 sec)
18-06-05 03:53-INFO->> Step 483810 run_train: loss = 5.3725  (0.189 sec)
18-06-05 03:53-INFO->> Step 483820 run_train: loss = 5.3586  (0.156 sec)
18-06-05 03:53-INFO->> Step 483830 run_train: loss = 5.4671  (0.152 sec)
18-06-05 03:53-INFO->> Step 483840 run_train: loss = 5.3752  (0.183 sec)
18-06-05 03:53-INFO->> Step 483850 run_train: loss = 5.3347  (0.177 sec)
18-06-05 03:53-INFO->> Step 483860 run_train: loss = 5.3531  (0.167 sec)
18-06-05 03:53-INFO->> Step 483870 run_train: loss = 5.4230  (0.183 sec)
18-06-05 03:53-INFO->> Step 483880 run_train: loss = 5.3786  (0.159 sec)
18-06-05 03:53-INFO->> Step 483890 run_train: loss = 5.4127  (0.155 sec)
18-06-05 03:53-INFO->> Step 483900 run_train: loss = 5.3847  (0.181 sec)
18-06-05 03:53-INFO->> Step 483910 run_train: loss = 5.3798  (0.120 sec)
18-06-05 03:53-INFO->> Step 483920 run_train: loss = 5.4474  (0.159 sec)
18-06-05 03:53-INFO->> Step 483930 run_train: loss = 5.3377  (0.159 sec)
18-06-05 03:53-INFO->> Step 483940 run_train: loss = 5.3595  (0.168 sec)
18-06-05 03:53-INFO->> Step 483950 run_train: loss = 5.4027  (0.134 sec)
18-06-05 03:53-INFO->> Step 483960 run_train: loss = 5.3588  (0.184 sec)
18-06-05 03:53-INFO->> Step 483970 run_train: loss = 5.4247  (0.139 sec)
18-06-05 03:54-INFO->> Step 483980 run_train: loss = 5.4120  (0.160 sec)
18-06-05 03:54-INFO->> Step 483990 run_train: loss = 5.3889  (0.164 sec)
18-06-05 03:54-INFO->> Step 484000 run_train: loss = 5.4355  (0.157 sec)
18-06-05 03:54-INFO->> 2018-06-05 03:54:04.444847 Saving in ckpt
18-06-05 03:54-INFO-Test Data Eval:
18-06-05 03:54-INFO-fpr95 = 0.17525405154091392 and auc = 0.9688337518535676
18-06-05 03:54-INFO->> Step 484010 run_train: loss = 5.4420  (0.175 sec)
18-06-05 03:54-INFO->> Step 484020 run_train: loss = 5.3824  (0.161 sec)
18-06-05 03:54-INFO->> Step 484030 run_train: loss = 5.3882  (0.153 sec)
18-06-05 03:54-INFO->> Step 484040 run_train: loss = 5.4076  (0.145 sec)
18-06-05 03:54-INFO->> Step 484050 run_train: loss = 5.4226  (0.182 sec)
18-06-05 03:54-INFO->> Step 484060 run_train: loss = 5.3941  (0.142 sec)
18-06-05 03:54-INFO->> Step 484070 run_train: loss = 5.4116  (0.156 sec)
18-06-05 03:54-INFO->> Step 484080 run_train: loss = 5.4732  (0.161 sec)
18-06-05 03:54-INFO->> Step 484090 run_train: loss = 5.3372  (0.116 sec)
18-06-05 03:55-INFO->> Step 484100 run_train: loss = 5.4320  (0.168 sec)
18-06-05 03:55-INFO->> Step 484110 run_train: loss = 5.4279  (0.159 sec)
18-06-05 03:55-INFO->> Step 484120 run_train: loss = 5.3877  (0.175 sec)
18-06-05 03:55-INFO->> Step 484130 run_train: loss = 5.4163  (0.155 sec)
18-06-05 03:55-INFO->> Step 484140 run_train: loss = 5.3937  (0.161 sec)
18-06-05 03:55-INFO->> Step 484150 run_train: loss = 5.4247  (0.191 sec)
18-06-05 03:55-INFO->> Step 484160 run_train: loss = 5.4477  (0.142 sec)
18-06-05 03:55-INFO->> Step 484170 run_train: loss = 5.3930  (0.164 sec)
18-06-05 03:55-INFO->> Step 484180 run_train: loss = 5.4464  (0.161 sec)
18-06-05 03:55-INFO->> Step 484190 run_train: loss = 5.4384  (0.166 sec)
18-06-05 03:55-INFO->> Step 484200 run_train: loss = 5.3526  (0.178 sec)
18-06-05 03:55-INFO->> Step 484210 run_train: loss = 5.4108  (0.182 sec)
18-06-05 03:55-INFO->> Step 484220 run_train: loss = 5.3689  (0.169 sec)
18-06-05 03:55-INFO->> Step 484230 run_train: loss = 5.3719  (0.132 sec)
18-06-05 03:55-INFO->> Step 484240 run_train: loss = 5.4183  (0.126 sec)
18-06-05 03:55-INFO->> Step 484250 run_train: loss = 5.4055  (0.152 sec)
18-06-05 03:55-INFO->> Step 484260 run_train: loss = 5.3161  (0.165 sec)
18-06-05 03:55-INFO->> Step 484270 run_train: loss = 5.4265  (0.153 sec)
18-06-05 03:55-INFO->> Step 484280 run_train: loss = 5.3881  (0.159 sec)
18-06-05 03:55-INFO->> Step 484290 run_train: loss = 5.3822  (0.151 sec)
18-06-05 03:55-INFO->> Step 484300 run_train: loss = 5.4215  (0.141 sec)
18-06-05 03:55-INFO->> Step 484310 run_train: loss = 5.4223  (0.133 sec)
18-06-05 03:55-INFO->> Step 484320 run_train: loss = 5.3836  (0.169 sec)
18-06-05 03:55-INFO->> Step 484330 run_train: loss = 5.3903  (0.175 sec)
18-06-05 03:55-INFO->> Step 484340 run_train: loss = 5.3619  (0.154 sec)
18-06-05 03:55-INFO->> Step 484350 run_train: loss = 5.4030  (0.193 sec)
18-06-05 03:55-INFO->> Step 484360 run_train: loss = 5.4120  (0.143 sec)
18-06-05 03:55-INFO->> Step 484370 run_train: loss = 5.4210  (0.142 sec)
18-06-05 03:55-INFO->> Step 484380 run_train: loss = 5.4630  (0.141 sec)
18-06-05 03:55-INFO->> Step 484390 run_train: loss = 5.3626  (0.124 sec)
18-06-05 03:55-INFO->> Step 484400 run_train: loss = 5.4064  (0.168 sec)
18-06-05 03:55-INFO->> Step 484410 run_train: loss = 5.4082  (0.125 sec)
18-06-05 03:55-INFO->> Step 484420 run_train: loss = 5.4281  (0.139 sec)
18-06-05 03:55-INFO->> Step 484430 run_train: loss = 5.4299  (0.163 sec)
18-06-05 03:55-INFO->> Step 484440 run_train: loss = 5.3966  (0.153 sec)
18-06-05 03:55-INFO->> Step 484450 run_train: loss = 5.4224  (0.135 sec)
18-06-05 03:55-INFO->> Step 484460 run_train: loss = 5.4083  (0.169 sec)
18-06-05 03:55-INFO->> Step 484470 run_train: loss = 5.3716  (0.141 sec)
18-06-05 03:56-INFO->> Step 484480 run_train: loss = 5.3263  (0.134 sec)
18-06-05 03:56-INFO->> Step 484490 run_train: loss = 5.3614  (0.147 sec)
18-06-05 03:56-INFO->> Step 484500 run_train: loss = 5.4385  (0.159 sec)
18-06-05 03:56-INFO->> Step 484510 run_train: loss = 5.3996  (0.170 sec)
18-06-05 03:56-INFO->> Step 484520 run_train: loss = 5.4090  (0.146 sec)
18-06-05 03:56-INFO->> Step 484530 run_train: loss = 5.3934  (0.114 sec)
18-06-05 03:56-INFO->> Step 484540 run_train: loss = 5.3748  (0.171 sec)
18-06-05 03:56-INFO->> Step 484550 run_train: loss = 5.3920  (0.202 sec)
18-06-05 03:56-INFO->> Step 484560 run_train: loss = 5.5120  (0.183 sec)
18-06-05 03:56-INFO->> Step 484570 run_train: loss = 5.4470  (0.173 sec)
18-06-05 03:56-INFO->> Step 484580 run_train: loss = 5.4113  (0.147 sec)
18-06-05 03:56-INFO->> Step 484590 run_train: loss = 5.4503  (0.164 sec)
18-06-05 03:56-INFO->> Step 484600 run_train: loss = 5.4006  (0.182 sec)
18-06-05 03:56-INFO->> Step 484610 run_train: loss = 5.4265  (0.188 sec)
18-06-05 03:56-INFO->> Step 484620 run_train: loss = 5.3838  (0.185 sec)
18-06-05 03:56-INFO->> Step 484630 run_train: loss = 5.4336  (0.204 sec)
18-06-05 03:56-INFO->> Step 484640 run_train: loss = 5.4522  (0.173 sec)
18-06-05 03:56-INFO->> Step 484650 run_train: loss = 5.4438  (0.193 sec)
18-06-05 03:56-INFO->> Step 484660 run_train: loss = 5.4113  (0.221 sec)
18-06-05 03:56-INFO->> Step 484670 run_train: loss = 5.4300  (0.153 sec)
18-06-05 03:56-INFO->> Step 484680 run_train: loss = 5.3531  (0.149 sec)
18-06-05 03:56-INFO->> Step 484690 run_train: loss = 5.4256  (0.198 sec)
18-06-05 03:56-INFO->> Step 484700 run_train: loss = 5.3748  (0.169 sec)
18-06-05 03:56-INFO->> Step 484710 run_train: loss = 5.4039  (0.143 sec)
18-06-05 03:56-INFO->> Step 484720 run_train: loss = 5.4208  (0.149 sec)
18-06-05 03:56-INFO->> Step 484730 run_train: loss = 5.3561  (0.154 sec)
18-06-05 03:56-INFO->> Step 484740 run_train: loss = 5.4614  (0.172 sec)
18-06-05 03:56-INFO->> Step 484750 run_train: loss = 5.3997  (0.158 sec)
18-06-05 03:56-INFO->> Step 484760 run_train: loss = 5.4309  (0.186 sec)
18-06-05 03:56-INFO->> Step 484770 run_train: loss = 5.3942  (0.156 sec)
18-06-05 03:56-INFO->> Step 484780 run_train: loss = 5.4091  (0.179 sec)
18-06-05 03:56-INFO->> Step 484790 run_train: loss = 5.4075  (0.169 sec)
18-06-05 03:56-INFO->> Step 484800 run_train: loss = 5.4393  (0.155 sec)
18-06-05 03:56-INFO->> Step 484810 run_train: loss = 5.4075  (0.155 sec)
18-06-05 03:56-INFO->> Step 484820 run_train: loss = 5.4189  (0.143 sec)
18-06-05 03:56-INFO->> Step 484830 run_train: loss = 5.3949  (0.117 sec)
18-06-05 03:56-INFO->> Step 484840 run_train: loss = 5.3796  (0.149 sec)
18-06-05 03:56-INFO->> Step 484850 run_train: loss = 5.3846  (0.156 sec)
18-06-05 03:57-INFO->> Step 484860 run_train: loss = 5.4230  (0.184 sec)
18-06-05 03:57-INFO->> Step 484870 run_train: loss = 5.4686  (0.160 sec)
18-06-05 03:57-INFO->> Step 484880 run_train: loss = 5.4129  (0.190 sec)
18-06-05 03:57-INFO->> Step 484890 run_train: loss = 5.3952  (0.174 sec)
18-06-05 03:57-INFO->> Step 484900 run_train: loss = 5.3400  (0.161 sec)
18-06-05 03:57-INFO->> Step 484910 run_train: loss = 5.4592  (0.165 sec)
18-06-05 03:57-INFO->> Step 484920 run_train: loss = 5.3817  (0.178 sec)
18-06-05 03:57-INFO->> Step 484930 run_train: loss = 5.4352  (0.149 sec)
18-06-05 03:57-INFO->> Step 484940 run_train: loss = 5.4281  (0.145 sec)
18-06-05 03:57-INFO->> Step 484950 run_train: loss = 5.3395  (0.140 sec)
18-06-05 03:57-INFO->> Step 484960 run_train: loss = 5.4367  (0.146 sec)
18-06-05 03:57-INFO->> Step 484970 run_train: loss = 5.3981  (0.144 sec)
18-06-05 03:57-INFO->> Step 484980 run_train: loss = 5.3935  (0.179 sec)
18-06-05 03:57-INFO->> Step 484990 run_train: loss = 5.3955  (0.159 sec)
18-06-05 03:57-INFO->> Step 485000 run_train: loss = 5.3536  (0.144 sec)
18-06-05 03:57-INFO->> 2018-06-05 03:57:22.629610 Saving in ckpt
18-06-05 03:57-INFO-Test Data Eval:
18-06-05 03:58-INFO-fpr95 = 0.17451514346439959 and auc = 0.969148795415079
18-06-05 03:58-INFO->> Step 485010 run_train: loss = 5.3727  (0.138 sec)
18-06-05 03:58-INFO->> Step 485020 run_train: loss = 5.4050  (0.151 sec)
18-06-05 03:58-INFO->> Step 485030 run_train: loss = 5.4389  (0.171 sec)
18-06-05 03:58-INFO->> Step 485040 run_train: loss = 5.4146  (0.125 sec)
18-06-05 03:58-INFO->> Step 485050 run_train: loss = 5.4295  (0.164 sec)
18-06-05 03:58-INFO->> Step 485060 run_train: loss = 5.3688  (0.184 sec)
18-06-05 03:58-INFO->> Step 485070 run_train: loss = 5.3773  (0.182 sec)
18-06-05 03:58-INFO->> Step 485080 run_train: loss = 5.4008  (0.155 sec)
18-06-05 03:58-INFO->> Step 485090 run_train: loss = 5.4078  (0.157 sec)
18-06-05 03:58-INFO->> Step 485100 run_train: loss = 5.3649  (0.170 sec)
18-06-05 03:58-INFO->> Step 485110 run_train: loss = 5.4365  (0.155 sec)
18-06-05 03:58-INFO->> Step 485120 run_train: loss = 5.4775  (0.126 sec)
18-06-05 03:58-INFO->> Step 485130 run_train: loss = 5.4362  (0.202 sec)
18-06-05 03:58-INFO->> Step 485140 run_train: loss = 5.4204  (0.179 sec)
18-06-05 03:58-INFO->> Step 485150 run_train: loss = 5.3529  (0.176 sec)
18-06-05 03:58-INFO->> Step 485160 run_train: loss = 5.3835  (0.166 sec)
18-06-05 03:58-INFO->> Step 485170 run_train: loss = 5.3729  (0.155 sec)
18-06-05 03:58-INFO->> Step 485180 run_train: loss = 5.4434  (0.144 sec)
18-06-05 03:58-INFO->> Step 485190 run_train: loss = 5.3488  (0.171 sec)
18-06-05 03:58-INFO->> Step 485200 run_train: loss = 5.4156  (0.147 sec)
18-06-05 03:58-INFO->> Step 485210 run_train: loss = 5.4068  (0.136 sec)
18-06-05 03:58-INFO->> Step 485220 run_train: loss = 5.4405  (0.183 sec)
18-06-05 03:58-INFO->> Step 485230 run_train: loss = 5.4072  (0.115 sec)
18-06-05 03:58-INFO->> Step 485240 run_train: loss = 5.3248  (0.140 sec)
18-06-05 03:58-INFO->> Step 485250 run_train: loss = 5.4534  (0.153 sec)
18-06-05 03:58-INFO->> Step 485260 run_train: loss = 5.4008  (0.163 sec)
18-06-05 03:58-INFO->> Step 485270 run_train: loss = 5.3854  (0.143 sec)
18-06-05 03:58-INFO->> Step 485280 run_train: loss = 5.4374  (0.137 sec)
18-06-05 03:58-INFO->> Step 485290 run_train: loss = 5.4721  (0.151 sec)
18-06-05 03:58-INFO->> Step 485300 run_train: loss = 5.4375  (0.121 sec)
18-06-05 03:58-INFO->> Step 485310 run_train: loss = 5.4790  (0.167 sec)
18-06-05 03:58-INFO->> Step 485320 run_train: loss = 5.3621  (0.171 sec)
18-06-05 03:58-INFO->> Step 485330 run_train: loss = 5.4236  (0.164 sec)
18-06-05 03:58-INFO->> Step 485340 run_train: loss = 5.4371  (0.134 sec)
18-06-05 03:58-INFO->> Step 485350 run_train: loss = 5.4735  (0.179 sec)
18-06-05 03:58-INFO->> Step 485360 run_train: loss = 5.4241  (0.148 sec)
18-06-05 03:59-INFO->> Step 485370 run_train: loss = 5.4193  (0.128 sec)
18-06-05 03:59-INFO->> Step 485380 run_train: loss = 5.3887  (0.153 sec)
18-06-05 03:59-INFO->> Step 485390 run_train: loss = 5.4206  (0.147 sec)
18-06-05 03:59-INFO->> Step 485400 run_train: loss = 5.3437  (0.164 sec)
18-06-05 03:59-INFO->> Step 485410 run_train: loss = 5.4159  (0.162 sec)
18-06-05 03:59-INFO->> Step 485420 run_train: loss = 5.3872  (0.184 sec)
18-06-05 03:59-INFO->> Step 485430 run_train: loss = 5.4579  (0.112 sec)
18-06-05 03:59-INFO->> Step 485440 run_train: loss = 5.4055  (0.162 sec)
18-06-05 03:59-INFO->> Step 485450 run_train: loss = 5.4397  (0.162 sec)
18-06-05 03:59-INFO->> Step 485460 run_train: loss = 5.4206  (0.131 sec)
18-06-05 03:59-INFO->> Step 485470 run_train: loss = 5.4122  (0.134 sec)
18-06-05 03:59-INFO->> Step 485480 run_train: loss = 5.4555  (0.169 sec)
18-06-05 03:59-INFO->> Step 485490 run_train: loss = 5.3670  (0.194 sec)
18-06-05 03:59-INFO->> Step 485500 run_train: loss = 5.4182  (0.164 sec)
18-06-05 03:59-INFO->> Step 485510 run_train: loss = 5.3644  (0.154 sec)
18-06-05 03:59-INFO->> Step 485520 run_train: loss = 5.4041  (0.149 sec)
18-06-05 03:59-INFO->> Step 485530 run_train: loss = 5.3292  (0.169 sec)
18-06-05 03:59-INFO->> Step 485540 run_train: loss = 5.3998  (0.205 sec)
18-06-05 03:59-INFO->> Step 485550 run_train: loss = 5.3290  (0.170 sec)
18-06-05 03:59-INFO->> Step 485560 run_train: loss = 5.3609  (0.146 sec)
18-06-05 03:59-INFO->> Step 485570 run_train: loss = 5.4094  (0.157 sec)
18-06-05 03:59-INFO->> Step 485580 run_train: loss = 5.4198  (0.155 sec)
18-06-05 03:59-INFO->> Step 485590 run_train: loss = 5.4110  (0.181 sec)
18-06-05 03:59-INFO->> Step 485600 run_train: loss = 5.3738  (0.133 sec)
18-06-05 03:59-INFO->> Step 485610 run_train: loss = 5.3910  (0.133 sec)
18-06-05 03:59-INFO->> Step 485620 run_train: loss = 5.3487  (0.197 sec)
18-06-05 03:59-INFO->> Step 485630 run_train: loss = 5.3567  (0.156 sec)
18-06-05 03:59-INFO->> Step 485640 run_train: loss = 5.3705  (0.156 sec)
18-06-05 03:59-INFO->> Step 485650 run_train: loss = 5.4284  (0.145 sec)
18-06-05 03:59-INFO->> Step 485660 run_train: loss = 5.3869  (0.140 sec)
18-06-05 03:59-INFO->> Step 485670 run_train: loss = 5.3730  (0.177 sec)
18-06-05 03:59-INFO->> Step 485680 run_train: loss = 5.3734  (0.124 sec)
18-06-05 03:59-INFO->> Step 485690 run_train: loss = 5.4353  (0.137 sec)
18-06-05 03:59-INFO->> Step 485700 run_train: loss = 5.3928  (0.180 sec)
18-06-05 03:59-INFO->> Step 485710 run_train: loss = 5.3770  (0.152 sec)
18-06-05 03:59-INFO->> Step 485720 run_train: loss = 5.4213  (0.154 sec)
18-06-05 03:59-INFO->> Step 485730 run_train: loss = 5.3957  (0.143 sec)
18-06-05 04:00-INFO->> Step 485740 run_train: loss = 5.3930  (0.182 sec)
18-06-05 04:00-INFO->> Step 485750 run_train: loss = 5.3986  (0.159 sec)
18-06-05 04:00-INFO->> Step 485760 run_train: loss = 5.4024  (0.133 sec)
18-06-05 04:00-INFO->> Step 485770 run_train: loss = 5.3924  (0.157 sec)
18-06-05 04:00-INFO->> Step 485780 run_train: loss = 5.4032  (0.150 sec)
18-06-05 04:00-INFO->> Step 485790 run_train: loss = 5.3963  (0.169 sec)
18-06-05 04:00-INFO->> Step 485800 run_train: loss = 5.4121  (0.160 sec)
18-06-05 04:00-INFO->> Step 485810 run_train: loss = 5.3701  (0.171 sec)
18-06-05 04:00-INFO->> Step 485820 run_train: loss = 5.4317  (0.202 sec)
18-06-05 04:00-INFO->> Step 485830 run_train: loss = 5.3867  (0.170 sec)
18-06-05 04:00-INFO->> Step 485840 run_train: loss = 5.4260  (0.144 sec)
18-06-05 04:00-INFO->> Step 485850 run_train: loss = 5.3691  (0.181 sec)
18-06-05 04:00-INFO->> Step 485860 run_train: loss = 5.4093  (0.148 sec)
18-06-05 04:00-INFO->> Step 485870 run_train: loss = 5.3682  (0.135 sec)
18-06-05 04:00-INFO->> Step 485880 run_train: loss = 5.3854  (0.165 sec)
18-06-05 04:00-INFO->> Step 485890 run_train: loss = 5.4304  (0.133 sec)
18-06-05 04:00-INFO->> Step 485900 run_train: loss = 5.4011  (0.159 sec)
18-06-05 04:00-INFO->> Step 485910 run_train: loss = 5.4135  (0.178 sec)
18-06-05 04:00-INFO->> Step 485920 run_train: loss = 5.4114  (0.190 sec)
18-06-05 04:00-INFO->> Step 485930 run_train: loss = 5.4206  (0.171 sec)
18-06-05 04:00-INFO->> Step 485940 run_train: loss = 5.4489  (0.137 sec)
18-06-05 04:00-INFO->> Step 485950 run_train: loss = 5.4155  (0.156 sec)
18-06-05 04:00-INFO->> Step 485960 run_train: loss = 5.3590  (0.123 sec)
18-06-05 04:00-INFO->> Step 485970 run_train: loss = 5.3249  (0.134 sec)
18-06-05 04:00-INFO->> Step 485980 run_train: loss = 5.3884  (0.133 sec)
18-06-05 04:00-INFO->> Step 485990 run_train: loss = 5.3640  (0.198 sec)
18-06-05 04:00-INFO->> Step 486000 run_train: loss = 5.3674  (0.148 sec)
18-06-05 04:00-INFO->> 2018-06-05 04:00:41.550142 Saving in ckpt
18-06-05 04:00-INFO-Test Data Eval:
18-06-05 04:01-INFO-fpr95 = 0.17340263018065888 and auc = 0.9691199740372392
18-06-05 04:01-INFO->> Step 486010 run_train: loss = 5.4468  (0.155 sec)
18-06-05 04:01-INFO->> Step 486020 run_train: loss = 5.3609  (0.184 sec)
18-06-05 04:01-INFO->> Step 486030 run_train: loss = 5.4703  (0.177 sec)
18-06-05 04:01-INFO->> Step 486040 run_train: loss = 5.4286  (0.206 sec)
18-06-05 04:01-INFO->> Step 486050 run_train: loss = 5.3768  (0.138 sec)
18-06-05 04:01-INFO->> Step 486060 run_train: loss = 5.3806  (0.181 sec)
18-06-05 04:01-INFO->> Step 486070 run_train: loss = 5.3427  (0.172 sec)
18-06-05 04:01-INFO->> Step 486080 run_train: loss = 5.4087  (0.142 sec)
18-06-05 04:01-INFO->> Step 486090 run_train: loss = 5.4831  (0.168 sec)
18-06-05 04:01-INFO->> Step 486100 run_train: loss = 5.3961  (0.152 sec)
18-06-05 04:01-INFO->> Step 486110 run_train: loss = 5.4002  (0.160 sec)
18-06-05 04:01-INFO->> Step 486120 run_train: loss = 5.3630  (0.157 sec)
18-06-05 04:01-INFO->> Step 486130 run_train: loss = 5.3323  (0.148 sec)
18-06-05 04:01-INFO->> Step 486140 run_train: loss = 5.3864  (0.176 sec)
18-06-05 04:01-INFO->> Step 486150 run_train: loss = 5.3817  (0.122 sec)
18-06-05 04:01-INFO->> Step 486160 run_train: loss = 5.4567  (0.152 sec)
18-06-05 04:01-INFO->> Step 486170 run_train: loss = 5.4191  (0.144 sec)
18-06-05 04:01-INFO->> Step 486180 run_train: loss = 5.3836  (0.153 sec)
18-06-05 04:01-INFO->> Step 486190 run_train: loss = 5.4268  (0.176 sec)
18-06-05 04:01-INFO->> Step 486200 run_train: loss = 5.4188  (0.165 sec)
18-06-05 04:01-INFO->> Step 486210 run_train: loss = 5.3934  (0.160 sec)
18-06-05 04:01-INFO->> Step 486220 run_train: loss = 5.3984  (0.182 sec)
18-06-05 04:01-INFO->> Step 486230 run_train: loss = 5.4072  (0.135 sec)
18-06-05 04:01-INFO->> Step 486240 run_train: loss = 5.4167  (0.163 sec)
18-06-05 04:02-INFO->> Step 486250 run_train: loss = 5.4140  (0.177 sec)
18-06-05 04:02-INFO->> Step 486260 run_train: loss = 5.3932  (0.151 sec)
18-06-05 04:02-INFO->> Step 486270 run_train: loss = 5.4352  (0.149 sec)
18-06-05 04:02-INFO->> Step 486280 run_train: loss = 5.4152  (0.163 sec)
18-06-05 04:02-INFO->> Step 486290 run_train: loss = 5.4494  (0.138 sec)
18-06-05 04:02-INFO->> Step 486300 run_train: loss = 5.4069  (0.189 sec)
18-06-05 04:02-INFO->> Step 486310 run_train: loss = 5.4205  (0.158 sec)
18-06-05 04:02-INFO->> Step 486320 run_train: loss = 5.3986  (0.198 sec)
18-06-05 04:02-INFO->> Step 486330 run_train: loss = 5.4170  (0.155 sec)
18-06-05 04:02-INFO->> Step 486340 run_train: loss = 5.3499  (0.144 sec)
18-06-05 04:02-INFO->> Step 486350 run_train: loss = 5.3963  (0.170 sec)
18-06-05 04:02-INFO->> Step 486360 run_train: loss = 5.3918  (0.153 sec)
18-06-05 04:02-INFO->> Step 486370 run_train: loss = 5.4196  (0.160 sec)
18-06-05 04:02-INFO->> Step 486380 run_train: loss = 5.3759  (0.181 sec)
18-06-05 04:02-INFO->> Step 486390 run_train: loss = 5.3341  (0.166 sec)
18-06-05 04:02-INFO->> Step 486400 run_train: loss = 5.4094  (0.185 sec)
18-06-05 04:02-INFO->> Step 486410 run_train: loss = 5.4261  (0.180 sec)
18-06-05 04:02-INFO->> Step 486420 run_train: loss = 5.4227  (0.151 sec)
18-06-05 04:02-INFO->> Step 486430 run_train: loss = 5.3576  (0.155 sec)
18-06-05 04:02-INFO->> Step 486440 run_train: loss = 5.4227  (0.125 sec)
18-06-05 04:02-INFO->> Step 486450 run_train: loss = 5.3859  (0.158 sec)
18-06-05 04:02-INFO->> Step 486460 run_train: loss = 5.3443  (0.184 sec)
18-06-05 04:02-INFO->> Step 486470 run_train: loss = 5.3580  (0.160 sec)
18-06-05 04:02-INFO->> Step 486480 run_train: loss = 5.4448  (0.164 sec)
18-06-05 04:02-INFO->> Step 486490 run_train: loss = 5.4306  (0.140 sec)
18-06-05 04:02-INFO->> Step 486500 run_train: loss = 5.4557  (0.157 sec)
18-06-05 04:02-INFO->> Step 486510 run_train: loss = 5.3792  (0.164 sec)
18-06-05 04:02-INFO->> Step 486520 run_train: loss = 5.3661  (0.151 sec)
18-06-05 04:02-INFO->> Step 486530 run_train: loss = 5.3666  (0.147 sec)
18-06-05 04:02-INFO->> Step 486540 run_train: loss = 5.4079  (0.156 sec)
18-06-05 04:02-INFO->> Step 486550 run_train: loss = 5.4431  (0.161 sec)
18-06-05 04:02-INFO->> Step 486560 run_train: loss = 5.3984  (0.159 sec)
18-06-05 04:02-INFO->> Step 486570 run_train: loss = 5.4551  (0.133 sec)
18-06-05 04:02-INFO->> Step 486580 run_train: loss = 5.4302  (0.154 sec)
18-06-05 04:02-INFO->> Step 486590 run_train: loss = 5.4460  (0.146 sec)
18-06-05 04:02-INFO->> Step 486600 run_train: loss = 5.4163  (0.144 sec)
18-06-05 04:02-INFO->> Step 486610 run_train: loss = 5.4017  (0.139 sec)
18-06-05 04:02-INFO->> Step 486620 run_train: loss = 5.4345  (0.146 sec)
18-06-05 04:03-INFO->> Step 486630 run_train: loss = 5.4635  (0.162 sec)
18-06-05 04:03-INFO->> Step 486640 run_train: loss = 5.4283  (0.186 sec)
18-06-05 04:03-INFO->> Step 486650 run_train: loss = 5.4250  (0.153 sec)
18-06-05 04:03-INFO->> Step 486660 run_train: loss = 5.4026  (0.146 sec)
18-06-05 04:03-INFO->> Step 486670 run_train: loss = 5.4126  (0.151 sec)
18-06-05 04:03-INFO->> Step 486680 run_train: loss = 5.3940  (0.252 sec)
18-06-05 04:03-INFO->> Step 486690 run_train: loss = 5.3928  (0.180 sec)
18-06-05 04:03-INFO->> Step 486700 run_train: loss = 5.4160  (0.154 sec)
18-06-05 04:03-INFO->> Step 486710 run_train: loss = 5.4527  (0.137 sec)
18-06-05 04:03-INFO->> Step 486720 run_train: loss = 5.4594  (0.134 sec)
18-06-05 04:03-INFO->> Step 486730 run_train: loss = 5.3799  (0.155 sec)
18-06-05 04:03-INFO->> Step 486740 run_train: loss = 5.4137  (0.188 sec)
18-06-05 04:03-INFO->> Step 486750 run_train: loss = 5.3668  (0.138 sec)
18-06-05 04:03-INFO->> Step 486760 run_train: loss = 5.4281  (0.131 sec)
18-06-05 04:03-INFO->> Step 486770 run_train: loss = 5.4102  (0.179 sec)
18-06-05 04:03-INFO->> Step 486780 run_train: loss = 5.4397  (0.188 sec)
18-06-05 04:03-INFO->> Step 486790 run_train: loss = 5.3587  (0.135 sec)
18-06-05 04:03-INFO->> Step 486800 run_train: loss = 5.3153  (0.140 sec)
18-06-05 04:03-INFO->> Step 486810 run_train: loss = 5.4437  (0.177 sec)
18-06-05 04:03-INFO->> Step 486820 run_train: loss = 5.3573  (0.193 sec)
18-06-05 04:03-INFO->> Step 486830 run_train: loss = 5.3910  (0.165 sec)
18-06-05 04:03-INFO->> Step 486840 run_train: loss = 5.3900  (0.161 sec)
18-06-05 04:03-INFO->> Step 486850 run_train: loss = 5.4269  (0.172 sec)
18-06-05 04:03-INFO->> Step 486860 run_train: loss = 5.3867  (0.143 sec)
18-06-05 04:03-INFO->> Step 486870 run_train: loss = 5.3894  (0.177 sec)
18-06-05 04:03-INFO->> Step 486880 run_train: loss = 5.4813  (0.145 sec)
18-06-05 04:03-INFO->> Step 486890 run_train: loss = 5.3575  (0.177 sec)
18-06-05 04:03-INFO->> Step 486900 run_train: loss = 5.4367  (0.133 sec)
18-06-05 04:03-INFO->> Step 486910 run_train: loss = 5.3957  (0.174 sec)
18-06-05 04:03-INFO->> Step 486920 run_train: loss = 5.4397  (0.162 sec)
18-06-05 04:03-INFO->> Step 486930 run_train: loss = 5.4187  (0.141 sec)
18-06-05 04:03-INFO->> Step 486940 run_train: loss = 5.3892  (0.198 sec)
18-06-05 04:03-INFO->> Step 486950 run_train: loss = 5.3727  (0.136 sec)
18-06-05 04:03-INFO->> Step 486960 run_train: loss = 5.3079  (0.157 sec)
18-06-05 04:03-INFO->> Step 486970 run_train: loss = 5.4401  (0.141 sec)
18-06-05 04:03-INFO->> Step 486980 run_train: loss = 5.3338  (0.174 sec)
18-06-05 04:03-INFO->> Step 486990 run_train: loss = 5.4631  (0.153 sec)
18-06-05 04:04-INFO->> Step 487000 run_train: loss = 5.3365  (0.164 sec)
18-06-05 04:04-INFO->> 2018-06-05 04:04:00.477173 Saving in ckpt
18-06-05 04:04-INFO-Test Data Eval:
18-06-05 04:04-INFO-fpr95 = 0.17608428533475026 and auc = 0.9688544404238898
18-06-05 04:04-INFO->> Step 487010 run_train: loss = 5.4075  (0.153 sec)
18-06-05 04:04-INFO->> Step 487020 run_train: loss = 5.3526  (0.149 sec)
18-06-05 04:04-INFO->> Step 487030 run_train: loss = 5.3727  (0.151 sec)
18-06-05 04:04-INFO->> Step 487040 run_train: loss = 5.4397  (0.175 sec)
18-06-05 04:04-INFO->> Step 487050 run_train: loss = 5.4654  (0.162 sec)
18-06-05 04:04-INFO->> Step 487060 run_train: loss = 5.4462  (0.176 sec)
18-06-05 04:04-INFO->> Step 487070 run_train: loss = 5.4115  (0.153 sec)
18-06-05 04:04-INFO->> Step 487080 run_train: loss = 5.3870  (0.138 sec)
18-06-05 04:04-INFO->> Step 487090 run_train: loss = 5.3856  (0.133 sec)
18-06-05 04:04-INFO->> Step 487100 run_train: loss = 5.3986  (0.140 sec)
18-06-05 04:04-INFO->> Step 487110 run_train: loss = 5.3532  (0.139 sec)
18-06-05 04:04-INFO->> Step 487120 run_train: loss = 5.4155  (0.150 sec)
18-06-05 04:05-INFO->> Step 487130 run_train: loss = 5.4010  (0.145 sec)
18-06-05 04:05-INFO->> Step 487140 run_train: loss = 5.4174  (0.152 sec)
18-06-05 04:05-INFO->> Step 487150 run_train: loss = 5.4217  (0.182 sec)
18-06-05 04:05-INFO->> Step 487160 run_train: loss = 5.3728  (0.197 sec)
18-06-05 04:05-INFO->> Step 487170 run_train: loss = 5.4815  (0.168 sec)
18-06-05 04:05-INFO->> Step 487180 run_train: loss = 5.3808  (0.145 sec)
18-06-05 04:05-INFO->> Step 487190 run_train: loss = 5.4301  (0.168 sec)
18-06-05 04:05-INFO->> Step 487200 run_train: loss = 5.3491  (0.158 sec)
18-06-05 04:05-INFO->> Step 487210 run_train: loss = 5.4236  (0.179 sec)
18-06-05 04:05-INFO->> Step 487220 run_train: loss = 5.4310  (0.163 sec)
18-06-05 04:05-INFO->> Step 487230 run_train: loss = 5.4001  (0.189 sec)
18-06-05 04:05-INFO->> Step 487240 run_train: loss = 5.4767  (0.170 sec)
18-06-05 04:05-INFO->> Step 487250 run_train: loss = 5.4426  (0.148 sec)
18-06-05 04:05-INFO->> Step 487260 run_train: loss = 5.3815  (0.171 sec)
18-06-05 04:05-INFO->> Step 487270 run_train: loss = 5.3944  (0.166 sec)
18-06-05 04:05-INFO->> Step 487280 run_train: loss = 5.4342  (0.172 sec)
18-06-05 04:05-INFO->> Step 487290 run_train: loss = 5.4137  (0.189 sec)
18-06-05 04:05-INFO->> Step 487300 run_train: loss = 5.4284  (0.156 sec)
18-06-05 04:05-INFO->> Step 487310 run_train: loss = 5.3877  (0.163 sec)
18-06-05 04:05-INFO->> Step 487320 run_train: loss = 5.3389  (0.152 sec)
18-06-05 04:05-INFO->> Step 487330 run_train: loss = 5.4110  (0.150 sec)
18-06-05 04:05-INFO->> Step 487340 run_train: loss = 5.4025  (0.166 sec)
18-06-05 04:05-INFO->> Step 487350 run_train: loss = 5.3843  (0.170 sec)
18-06-05 04:05-INFO->> Step 487360 run_train: loss = 5.4322  (0.149 sec)
18-06-05 04:05-INFO->> Step 487370 run_train: loss = 5.3930  (0.159 sec)
18-06-05 04:05-INFO->> Step 487380 run_train: loss = 5.3472  (0.159 sec)
18-06-05 04:05-INFO->> Step 487390 run_train: loss = 5.4042  (0.120 sec)
18-06-05 04:05-INFO->> Step 487400 run_train: loss = 5.3902  (0.159 sec)
18-06-05 04:05-INFO->> Step 487410 run_train: loss = 5.3927  (0.158 sec)
18-06-05 04:05-INFO->> Step 487420 run_train: loss = 5.4063  (0.158 sec)
18-06-05 04:05-INFO->> Step 487430 run_train: loss = 5.4506  (0.160 sec)
18-06-05 04:05-INFO->> Step 487440 run_train: loss = 5.3777  (0.134 sec)
18-06-05 04:05-INFO->> Step 487450 run_train: loss = 5.4394  (0.160 sec)
18-06-05 04:05-INFO->> Step 487460 run_train: loss = 5.3319  (0.129 sec)
18-06-05 04:05-INFO->> Step 487470 run_train: loss = 5.3727  (0.143 sec)
18-06-05 04:05-INFO->> Step 487480 run_train: loss = 5.3939  (0.160 sec)
18-06-05 04:05-INFO->> Step 487490 run_train: loss = 5.4070  (0.163 sec)
18-06-05 04:05-INFO->> Step 487500 run_train: loss = 5.4656  (0.131 sec)
18-06-05 04:06-INFO->> Step 487510 run_train: loss = 5.4381  (0.153 sec)
18-06-05 04:06-INFO->> Step 487520 run_train: loss = 5.4266  (0.122 sec)
18-06-05 04:06-INFO->> Step 487530 run_train: loss = 5.4081  (0.156 sec)
18-06-05 04:06-INFO->> Step 487540 run_train: loss = 5.3301  (0.146 sec)
18-06-05 04:06-INFO->> Step 487550 run_train: loss = 5.4457  (0.159 sec)
18-06-05 04:06-INFO->> Step 487560 run_train: loss = 5.4295  (0.133 sec)
18-06-05 04:06-INFO->> Step 487570 run_train: loss = 5.3640  (0.147 sec)
18-06-05 04:06-INFO->> Step 487580 run_train: loss = 5.3987  (0.163 sec)
18-06-05 04:06-INFO->> Step 487590 run_train: loss = 5.3880  (0.116 sec)
18-06-05 04:06-INFO->> Step 487600 run_train: loss = 5.3724  (0.149 sec)
18-06-05 04:06-INFO->> Step 487610 run_train: loss = 5.4742  (0.170 sec)
18-06-05 04:06-INFO->> Step 487620 run_train: loss = 5.3875  (0.164 sec)
18-06-05 04:06-INFO->> Step 487630 run_train: loss = 5.4297  (0.169 sec)
18-06-05 04:06-INFO->> Step 487640 run_train: loss = 5.4348  (0.159 sec)
18-06-05 04:06-INFO->> Step 487650 run_train: loss = 5.4434  (0.155 sec)
18-06-05 04:06-INFO->> Step 487660 run_train: loss = 5.4746  (0.160 sec)
18-06-05 04:06-INFO->> Step 487670 run_train: loss = 5.3890  (0.126 sec)
18-06-05 04:06-INFO->> Step 487680 run_train: loss = 5.3889  (0.139 sec)
18-06-05 04:06-INFO->> Step 487690 run_train: loss = 5.4015  (0.161 sec)
18-06-05 04:06-INFO->> Step 487700 run_train: loss = 5.4504  (0.150 sec)
18-06-05 04:06-INFO->> Step 487710 run_train: loss = 5.4640  (0.123 sec)
18-06-05 04:06-INFO->> Step 487720 run_train: loss = 5.4490  (0.178 sec)
18-06-05 04:06-INFO->> Step 487730 run_train: loss = 5.4216  (0.148 sec)
18-06-05 04:06-INFO->> Step 487740 run_train: loss = 5.4472  (0.187 sec)
18-06-05 04:06-INFO->> Step 487750 run_train: loss = 5.3762  (0.159 sec)
18-06-05 04:06-INFO->> Step 487760 run_train: loss = 5.4431  (0.162 sec)
18-06-05 04:06-INFO->> Step 487770 run_train: loss = 5.4250  (0.155 sec)
18-06-05 04:06-INFO->> Step 487780 run_train: loss = 5.4670  (0.149 sec)
18-06-05 04:06-INFO->> Step 487790 run_train: loss = 5.4471  (0.157 sec)
18-06-05 04:06-INFO->> Step 487800 run_train: loss = 5.4512  (0.151 sec)
18-06-05 04:06-INFO->> Step 487810 run_train: loss = 5.3696  (0.147 sec)
18-06-05 04:06-INFO->> Step 487820 run_train: loss = 5.4209  (0.150 sec)
18-06-05 04:06-INFO->> Step 487830 run_train: loss = 5.4353  (0.153 sec)
18-06-05 04:06-INFO->> Step 487840 run_train: loss = 5.3447  (0.182 sec)
18-06-05 04:06-INFO->> Step 487850 run_train: loss = 5.4009  (0.192 sec)
18-06-05 04:06-INFO->> Step 487860 run_train: loss = 5.4125  (0.204 sec)
18-06-05 04:06-INFO->> Step 487870 run_train: loss = 5.4586  (0.134 sec)
18-06-05 04:06-INFO->> Step 487880 run_train: loss = 5.4413  (0.118 sec)
18-06-05 04:07-INFO->> Step 487890 run_train: loss = 5.3516  (0.187 sec)
18-06-05 04:07-INFO->> Step 487900 run_train: loss = 5.3780  (0.165 sec)
18-06-05 04:07-INFO->> Step 487910 run_train: loss = 5.4286  (0.144 sec)
18-06-05 04:07-INFO->> Step 487920 run_train: loss = 5.4363  (0.126 sec)
18-06-05 04:07-INFO->> Step 487930 run_train: loss = 5.3715  (0.144 sec)
18-06-05 04:07-INFO->> Step 487940 run_train: loss = 5.4132  (0.144 sec)
18-06-05 04:07-INFO->> Step 487950 run_train: loss = 5.3678  (0.141 sec)
18-06-05 04:07-INFO->> Step 487960 run_train: loss = 5.3811  (0.169 sec)
18-06-05 04:07-INFO->> Step 487970 run_train: loss = 5.3633  (0.149 sec)
18-06-05 04:07-INFO->> Step 487980 run_train: loss = 5.3748  (0.185 sec)
18-06-05 04:07-INFO->> Step 487990 run_train: loss = 5.4227  (0.211 sec)
18-06-05 04:07-INFO->> Step 488000 run_train: loss = 5.4370  (0.138 sec)
18-06-05 04:07-INFO->> 2018-06-05 04:07:18.458284 Saving in ckpt
18-06-05 04:07-INFO-Test Data Eval:
18-06-05 04:07-INFO-fpr95 = 0.17431588735387885 and auc = 0.9689622455734312
18-06-05 04:08-INFO->> Step 488010 run_train: loss = 5.3823  (0.185 sec)
18-06-05 04:08-INFO->> Step 488020 run_train: loss = 5.3650  (0.167 sec)
18-06-05 04:08-INFO->> Step 488030 run_train: loss = 5.4177  (0.188 sec)
18-06-05 04:08-INFO->> Step 488040 run_train: loss = 5.4071  (0.192 sec)
18-06-05 04:08-INFO->> Step 488050 run_train: loss = 5.4147  (0.153 sec)
18-06-05 04:08-INFO->> Step 488060 run_train: loss = 5.4112  (0.164 sec)
18-06-05 04:08-INFO->> Step 488070 run_train: loss = 5.3654  (0.155 sec)
18-06-05 04:08-INFO->> Step 488080 run_train: loss = 5.4476  (0.176 sec)
18-06-05 04:08-INFO->> Step 488090 run_train: loss = 5.3306  (0.156 sec)
18-06-05 04:08-INFO->> Step 488100 run_train: loss = 5.4100  (0.162 sec)
18-06-05 04:08-INFO->> Step 488110 run_train: loss = 5.4083  (0.152 sec)
18-06-05 04:08-INFO->> Step 488120 run_train: loss = 5.3997  (0.173 sec)
18-06-05 04:08-INFO->> Step 488130 run_train: loss = 5.3786  (0.144 sec)
18-06-05 04:08-INFO->> Step 488140 run_train: loss = 5.4061  (0.148 sec)
18-06-05 04:08-INFO->> Step 488150 run_train: loss = 5.4006  (0.162 sec)
18-06-05 04:08-INFO->> Step 488160 run_train: loss = 5.3746  (0.172 sec)
18-06-05 04:08-INFO->> Step 488170 run_train: loss = 5.4354  (0.151 sec)
18-06-05 04:08-INFO->> Step 488180 run_train: loss = 5.4176  (0.120 sec)
18-06-05 04:08-INFO->> Step 488190 run_train: loss = 5.4179  (0.198 sec)
18-06-05 04:08-INFO->> Step 488200 run_train: loss = 5.4235  (0.175 sec)
18-06-05 04:08-INFO->> Step 488210 run_train: loss = 5.3921  (0.140 sec)
18-06-05 04:08-INFO->> Step 488220 run_train: loss = 5.4103  (0.157 sec)
18-06-05 04:08-INFO->> Step 488230 run_train: loss = 5.4538  (0.130 sec)
18-06-05 04:08-INFO->> Step 488240 run_train: loss = 5.3462  (0.157 sec)
18-06-05 04:08-INFO->> Step 488250 run_train: loss = 5.4590  (0.121 sec)
18-06-05 04:08-INFO->> Step 488260 run_train: loss = 5.3832  (0.156 sec)
18-06-05 04:08-INFO->> Step 488270 run_train: loss = 5.4107  (0.202 sec)
18-06-05 04:08-INFO->> Step 488280 run_train: loss = 5.4011  (0.142 sec)
18-06-05 04:08-INFO->> Step 488290 run_train: loss = 5.4051  (0.138 sec)
18-06-05 04:08-INFO->> Step 488300 run_train: loss = 5.4767  (0.134 sec)
18-06-05 04:08-INFO->> Step 488310 run_train: loss = 5.3687  (0.161 sec)
18-06-05 04:08-INFO->> Step 488320 run_train: loss = 5.4142  (0.161 sec)
18-06-05 04:08-INFO->> Step 488330 run_train: loss = 5.3589  (0.179 sec)
18-06-05 04:08-INFO->> Step 488340 run_train: loss = 5.4137  (0.181 sec)
18-06-05 04:08-INFO->> Step 488350 run_train: loss = 5.4103  (0.138 sec)
18-06-05 04:08-INFO->> Step 488360 run_train: loss = 5.4225  (0.157 sec)
18-06-05 04:08-INFO->> Step 488370 run_train: loss = 5.3552  (0.175 sec)
18-06-05 04:08-INFO->> Step 488380 run_train: loss = 5.4413  (0.171 sec)
18-06-05 04:09-INFO->> Step 488390 run_train: loss = 5.3434  (0.173 sec)
18-06-05 04:09-INFO->> Step 488400 run_train: loss = 5.4853  (0.141 sec)
18-06-05 04:09-INFO->> Step 488410 run_train: loss = 5.3467  (0.151 sec)
18-06-05 04:09-INFO->> Step 488420 run_train: loss = 5.4230  (0.172 sec)
18-06-05 04:09-INFO->> Step 488430 run_train: loss = 5.3372  (0.119 sec)
18-06-05 04:09-INFO->> Step 488440 run_train: loss = 5.4219  (0.154 sec)
18-06-05 04:09-INFO->> Step 488450 run_train: loss = 5.4357  (0.145 sec)
18-06-05 04:09-INFO->> Step 488460 run_train: loss = 5.3952  (0.152 sec)
18-06-05 04:09-INFO->> Step 488470 run_train: loss = 5.4177  (0.134 sec)
18-06-05 04:09-INFO->> Step 488480 run_train: loss = 5.4226  (0.170 sec)
18-06-05 04:09-INFO->> Step 488490 run_train: loss = 5.4064  (0.169 sec)
18-06-05 04:09-INFO->> Step 488500 run_train: loss = 5.4227  (0.156 sec)
18-06-05 04:09-INFO->> Step 488510 run_train: loss = 5.4761  (0.188 sec)
18-06-05 04:09-INFO->> Step 488520 run_train: loss = 5.3434  (0.189 sec)
18-06-05 04:09-INFO->> Step 488530 run_train: loss = 5.3894  (0.170 sec)
18-06-05 04:09-INFO->> Step 488540 run_train: loss = 5.4456  (0.159 sec)
18-06-05 04:09-INFO->> Step 488550 run_train: loss = 5.3596  (0.162 sec)
18-06-05 04:09-INFO->> Step 488560 run_train: loss = 5.4709  (0.139 sec)
18-06-05 04:09-INFO->> Step 488570 run_train: loss = 5.4348  (0.195 sec)
18-06-05 04:09-INFO->> Step 488580 run_train: loss = 5.4248  (0.156 sec)
18-06-05 04:09-INFO->> Step 488590 run_train: loss = 5.4413  (0.156 sec)
18-06-05 04:09-INFO->> Step 488600 run_train: loss = 5.4066  (0.150 sec)
18-06-05 04:09-INFO->> Step 488610 run_train: loss = 5.4104  (0.170 sec)
18-06-05 04:09-INFO->> Step 488620 run_train: loss = 5.3997  (0.151 sec)
18-06-05 04:09-INFO->> Step 488630 run_train: loss = 5.4056  (0.187 sec)
18-06-05 04:09-INFO->> Step 488640 run_train: loss = 5.4496  (0.162 sec)
18-06-05 04:09-INFO->> Step 488650 run_train: loss = 5.4646  (0.143 sec)
18-06-05 04:09-INFO->> Step 488660 run_train: loss = 5.3505  (0.157 sec)
18-06-05 04:09-INFO->> Step 488670 run_train: loss = 5.4130  (0.181 sec)
18-06-05 04:09-INFO->> Step 488680 run_train: loss = 5.4624  (0.160 sec)
18-06-05 04:09-INFO->> Step 488690 run_train: loss = 5.4578  (0.137 sec)
18-06-05 04:09-INFO->> Step 488700 run_train: loss = 5.4476  (0.180 sec)
18-06-05 04:09-INFO->> Step 488710 run_train: loss = 5.4010  (0.172 sec)
18-06-05 04:09-INFO->> Step 488720 run_train: loss = 5.3972  (0.181 sec)
18-06-05 04:09-INFO->> Step 488730 run_train: loss = 5.4200  (0.145 sec)
18-06-05 04:09-INFO->> Step 488740 run_train: loss = 5.3801  (0.193 sec)
18-06-05 04:09-INFO->> Step 488750 run_train: loss = 5.3419  (0.195 sec)
18-06-05 04:09-INFO->> Step 488760 run_train: loss = 5.3571  (0.171 sec)
18-06-05 04:10-INFO->> Step 488770 run_train: loss = 5.3903  (0.169 sec)
18-06-05 04:10-INFO->> Step 488780 run_train: loss = 5.3827  (0.181 sec)
18-06-05 04:10-INFO->> Step 488790 run_train: loss = 5.3344  (0.180 sec)
18-06-05 04:10-INFO->> Step 488800 run_train: loss = 5.4074  (0.153 sec)
18-06-05 04:10-INFO->> Step 488810 run_train: loss = 5.4324  (0.147 sec)
18-06-05 04:10-INFO->> Step 488820 run_train: loss = 5.3835  (0.159 sec)
18-06-05 04:10-INFO->> Step 488830 run_train: loss = 5.3698  (0.133 sec)
18-06-05 04:10-INFO->> Step 488840 run_train: loss = 5.4182  (0.180 sec)
18-06-05 04:10-INFO->> Step 488850 run_train: loss = 5.3792  (0.167 sec)
18-06-05 04:10-INFO->> Step 488860 run_train: loss = 5.3933  (0.149 sec)
18-06-05 04:10-INFO->> Step 488870 run_train: loss = 5.3550  (0.142 sec)
18-06-05 04:10-INFO->> Step 488880 run_train: loss = 5.3998  (0.174 sec)
18-06-05 04:10-INFO->> Step 488890 run_train: loss = 5.3476  (0.160 sec)
18-06-05 04:10-INFO->> Step 488900 run_train: loss = 5.4458  (0.132 sec)
18-06-05 04:10-INFO->> Step 488910 run_train: loss = 5.4270  (0.204 sec)
18-06-05 04:10-INFO->> Step 488920 run_train: loss = 5.4137  (0.161 sec)
18-06-05 04:10-INFO->> Step 488930 run_train: loss = 5.4822  (0.155 sec)
18-06-05 04:10-INFO->> Step 488940 run_train: loss = 5.4154  (0.211 sec)
18-06-05 04:10-INFO->> Step 488950 run_train: loss = 5.4376  (0.164 sec)
18-06-05 04:10-INFO->> Step 488960 run_train: loss = 5.4200  (0.180 sec)
18-06-05 04:10-INFO->> Step 488970 run_train: loss = 5.3400  (0.172 sec)
18-06-05 04:10-INFO->> Step 488980 run_train: loss = 5.4334  (0.141 sec)
18-06-05 04:10-INFO->> Step 488990 run_train: loss = 5.3737  (0.139 sec)
18-06-05 04:10-INFO->> Step 489000 run_train: loss = 5.3725  (0.155 sec)
18-06-05 04:10-INFO->> 2018-06-05 04:10:37.495870 Saving in ckpt
18-06-05 04:10-INFO-Test Data Eval:
18-06-05 04:11-INFO-fpr95 = 0.17340263018065888 and auc = 0.9689589942356807
18-06-05 04:11-INFO->> Step 489010 run_train: loss = 5.4342  (0.144 sec)
18-06-05 04:11-INFO->> Step 489020 run_train: loss = 5.3918  (0.161 sec)
18-06-05 04:11-INFO->> Step 489030 run_train: loss = 5.4230  (0.134 sec)
18-06-05 04:11-INFO->> Step 489040 run_train: loss = 5.4513  (0.166 sec)
18-06-05 04:11-INFO->> Step 489050 run_train: loss = 5.4149  (0.155 sec)
18-06-05 04:11-INFO->> Step 489060 run_train: loss = 5.4575  (0.177 sec)
18-06-05 04:11-INFO->> Step 489070 run_train: loss = 5.3986  (0.162 sec)
18-06-05 04:11-INFO->> Step 489080 run_train: loss = 5.4101  (0.173 sec)
18-06-05 04:11-INFO->> Step 489090 run_train: loss = 5.3446  (0.142 sec)
18-06-05 04:11-INFO->> Step 489100 run_train: loss = 5.4112  (0.138 sec)
18-06-05 04:11-INFO->> Step 489110 run_train: loss = 5.4020  (0.150 sec)
18-06-05 04:11-INFO->> Step 489120 run_train: loss = 5.4315  (0.146 sec)
18-06-05 04:11-INFO->> Step 489130 run_train: loss = 5.4272  (0.156 sec)
18-06-05 04:11-INFO->> Step 489140 run_train: loss = 5.4204  (0.132 sec)
18-06-05 04:11-INFO->> Step 489150 run_train: loss = 5.3640  (0.132 sec)
18-06-05 04:11-INFO->> Step 489160 run_train: loss = 5.3716  (0.156 sec)
18-06-05 04:11-INFO->> Step 489170 run_train: loss = 5.4750  (0.153 sec)
18-06-05 04:11-INFO->> Step 489180 run_train: loss = 5.4022  (0.145 sec)
18-06-05 04:11-INFO->> Step 489190 run_train: loss = 5.4000  (0.149 sec)
18-06-05 04:11-INFO->> Step 489200 run_train: loss = 5.3807  (0.154 sec)
18-06-05 04:11-INFO->> Step 489210 run_train: loss = 5.3702  (0.175 sec)
18-06-05 04:11-INFO->> Step 489220 run_train: loss = 5.4243  (0.152 sec)
18-06-05 04:11-INFO->> Step 489230 run_train: loss = 5.4124  (0.157 sec)
18-06-05 04:11-INFO->> Step 489240 run_train: loss = 5.3944  (0.154 sec)
18-06-05 04:11-INFO->> Step 489250 run_train: loss = 5.4100  (0.176 sec)
18-06-05 04:11-INFO->> Step 489260 run_train: loss = 5.3808  (0.181 sec)
18-06-05 04:12-INFO->> Step 489270 run_train: loss = 5.3509  (0.163 sec)
18-06-05 04:12-INFO->> Step 489280 run_train: loss = 5.3482  (0.159 sec)
18-06-05 04:12-INFO->> Step 489290 run_train: loss = 5.4097  (0.176 sec)
18-06-05 04:12-INFO->> Step 489300 run_train: loss = 5.4729  (0.127 sec)
18-06-05 04:12-INFO->> Step 489310 run_train: loss = 5.3658  (0.095 sec)
18-06-05 04:12-INFO->> Step 489320 run_train: loss = 5.4358  (0.179 sec)
18-06-05 04:12-INFO->> Step 489330 run_train: loss = 5.3709  (0.160 sec)
18-06-05 04:12-INFO->> Step 489340 run_train: loss = 5.4212  (0.150 sec)
18-06-05 04:12-INFO->> Step 489350 run_train: loss = 5.4507  (0.168 sec)
18-06-05 04:12-INFO->> Step 489360 run_train: loss = 5.3579  (0.130 sec)
18-06-05 04:12-INFO->> Step 489370 run_train: loss = 5.4010  (0.144 sec)
18-06-05 04:12-INFO->> Step 489380 run_train: loss = 5.4039  (0.136 sec)
18-06-05 04:12-INFO->> Step 489390 run_train: loss = 5.4358  (0.153 sec)
18-06-05 04:12-INFO->> Step 489400 run_train: loss = 5.3754  (0.153 sec)
18-06-05 04:12-INFO->> Step 489410 run_train: loss = 5.3564  (0.131 sec)
18-06-05 04:12-INFO->> Step 489420 run_train: loss = 5.4094  (0.189 sec)
18-06-05 04:12-INFO->> Step 489430 run_train: loss = 5.4113  (0.169 sec)
18-06-05 04:12-INFO->> Step 489440 run_train: loss = 5.3592  (0.166 sec)
18-06-05 04:12-INFO->> Step 489450 run_train: loss = 5.3687  (0.178 sec)
18-06-05 04:12-INFO->> Step 489460 run_train: loss = 5.4090  (0.137 sec)
18-06-05 04:12-INFO->> Step 489470 run_train: loss = 5.4470  (0.144 sec)
18-06-05 04:12-INFO->> Step 489480 run_train: loss = 5.3444  (0.179 sec)
18-06-05 04:12-INFO->> Step 489490 run_train: loss = 5.3690  (0.137 sec)
18-06-05 04:12-INFO->> Step 489500 run_train: loss = 5.4164  (0.155 sec)
18-06-05 04:12-INFO->> Step 489510 run_train: loss = 5.3970  (0.171 sec)
18-06-05 04:12-INFO->> Step 489520 run_train: loss = 5.3983  (0.156 sec)
18-06-05 04:12-INFO->> Step 489530 run_train: loss = 5.3070  (0.139 sec)
18-06-05 04:12-INFO->> Step 489540 run_train: loss = 5.4082  (0.160 sec)
18-06-05 04:12-INFO->> Step 489550 run_train: loss = 5.3226  (0.178 sec)
18-06-05 04:12-INFO->> Step 489560 run_train: loss = 5.3561  (0.150 sec)
18-06-05 04:12-INFO->> Step 489570 run_train: loss = 5.3911  (0.161 sec)
18-06-05 04:12-INFO->> Step 489580 run_train: loss = 5.3291  (0.182 sec)
18-06-05 04:12-INFO->> Step 489590 run_train: loss = 5.4323  (0.147 sec)
18-06-05 04:12-INFO->> Step 489600 run_train: loss = 5.4180  (0.148 sec)
18-06-05 04:12-INFO->> Step 489610 run_train: loss = 5.4037  (0.129 sec)
18-06-05 04:12-INFO->> Step 489620 run_train: loss = 5.4110  (0.132 sec)
18-06-05 04:12-INFO->> Step 489630 run_train: loss = 5.4140  (0.174 sec)
18-06-05 04:12-INFO->> Step 489640 run_train: loss = 5.4133  (0.163 sec)
18-06-05 04:13-INFO->> Step 489650 run_train: loss = 5.3099  (0.186 sec)
18-06-05 04:13-INFO->> Step 489660 run_train: loss = 5.4491  (0.144 sec)
18-06-05 04:13-INFO->> Step 489670 run_train: loss = 5.3947  (0.140 sec)
18-06-05 04:13-INFO->> Step 489680 run_train: loss = 5.3655  (0.168 sec)
18-06-05 04:13-INFO->> Step 489690 run_train: loss = 5.3610  (0.157 sec)
18-06-05 04:13-INFO->> Step 489700 run_train: loss = 5.3656  (0.134 sec)
18-06-05 04:13-INFO->> Step 489710 run_train: loss = 5.4370  (0.138 sec)
18-06-05 04:13-INFO->> Step 489720 run_train: loss = 5.3752  (0.191 sec)
18-06-05 04:13-INFO->> Step 489730 run_train: loss = 5.4351  (0.123 sec)
18-06-05 04:13-INFO->> Step 489740 run_train: loss = 5.3494  (0.145 sec)
18-06-05 04:13-INFO->> Step 489750 run_train: loss = 5.3797  (0.162 sec)
18-06-05 04:13-INFO->> Step 489760 run_train: loss = 5.4573  (0.143 sec)
18-06-05 04:13-INFO->> Step 489770 run_train: loss = 5.4277  (0.128 sec)
18-06-05 04:13-INFO->> Step 489780 run_train: loss = 5.3005  (0.143 sec)
18-06-05 04:13-INFO->> Step 489790 run_train: loss = 5.4618  (0.158 sec)
18-06-05 04:13-INFO->> Step 489800 run_train: loss = 5.3442  (0.176 sec)
18-06-05 04:13-INFO->> Step 489810 run_train: loss = 5.4526  (0.139 sec)
18-06-05 04:13-INFO->> Step 489820 run_train: loss = 5.4159  (0.161 sec)
18-06-05 04:13-INFO->> Step 489830 run_train: loss = 5.4408  (0.163 sec)
18-06-05 04:13-INFO->> Step 489840 run_train: loss = 5.4520  (0.142 sec)
18-06-05 04:13-INFO->> Step 489850 run_train: loss = 5.3659  (0.153 sec)
18-06-05 04:13-INFO->> Step 489860 run_train: loss = 5.4469  (0.172 sec)
18-06-05 04:13-INFO->> Step 489870 run_train: loss = 5.3786  (0.155 sec)
18-06-05 04:13-INFO->> Step 489880 run_train: loss = 5.3878  (0.176 sec)
18-06-05 04:13-INFO->> Step 489890 run_train: loss = 5.3969  (0.159 sec)
18-06-05 04:13-INFO->> Step 489900 run_train: loss = 5.3725  (0.190 sec)
18-06-05 04:13-INFO->> Step 489910 run_train: loss = 5.3841  (0.136 sec)
18-06-05 04:13-INFO->> Step 489920 run_train: loss = 5.3390  (0.131 sec)
18-06-05 04:13-INFO->> Step 489930 run_train: loss = 5.3468  (0.169 sec)
18-06-05 04:13-INFO->> Step 489940 run_train: loss = 5.3633  (0.190 sec)
18-06-05 04:13-INFO->> Step 489950 run_train: loss = 5.4439  (0.129 sec)
18-06-05 04:13-INFO->> Step 489960 run_train: loss = 5.3299  (0.146 sec)
18-06-05 04:13-INFO->> Step 489970 run_train: loss = 5.4680  (0.166 sec)
18-06-05 04:13-INFO->> Step 489980 run_train: loss = 5.3743  (0.166 sec)
18-06-05 04:13-INFO->> Step 489990 run_train: loss = 5.4487  (0.179 sec)
18-06-05 04:13-INFO->> Step 490000 run_train: loss = 5.3820  (0.163 sec)
18-06-05 04:13-INFO->> 2018-06-05 04:13:55.916983 Saving in ckpt
18-06-05 04:13-INFO-Test Data Eval:
18-06-05 04:14-INFO-fpr95 = 0.176192215727949 and auc = 0.9686900953107844
18-06-05 04:14-INFO->> Step 490010 run_train: loss = 5.3685  (0.161 sec)
18-06-05 04:14-INFO->> Step 490020 run_train: loss = 5.4729  (0.195 sec)
18-06-05 04:14-INFO->> Step 490030 run_train: loss = 5.4302  (0.189 sec)
18-06-05 04:14-INFO->> Step 490040 run_train: loss = 5.4114  (0.160 sec)
18-06-05 04:14-INFO->> Step 490050 run_train: loss = 5.3918  (0.181 sec)
18-06-05 04:14-INFO->> Step 490060 run_train: loss = 5.3986  (0.152 sec)
18-06-05 04:14-INFO->> Step 490070 run_train: loss = 5.4232  (0.125 sec)
18-06-05 04:14-INFO->> Step 490080 run_train: loss = 5.3440  (0.132 sec)
18-06-05 04:14-INFO->> Step 490090 run_train: loss = 5.4117  (0.152 sec)
18-06-05 04:14-INFO->> Step 490100 run_train: loss = 5.3918  (0.156 sec)
18-06-05 04:14-INFO->> Step 490110 run_train: loss = 5.4450  (0.148 sec)
18-06-05 04:14-INFO->> Step 490120 run_train: loss = 5.4165  (0.154 sec)
18-06-05 04:14-INFO->> Step 490130 run_train: loss = 5.3721  (0.144 sec)
18-06-05 04:14-INFO->> Step 490140 run_train: loss = 5.4308  (0.193 sec)
18-06-05 04:14-INFO->> Step 490150 run_train: loss = 5.4177  (0.150 sec)
18-06-05 04:15-INFO->> Step 490160 run_train: loss = 5.3628  (0.166 sec)
18-06-05 04:15-INFO->> Step 490170 run_train: loss = 5.3742  (0.143 sec)
18-06-05 04:15-INFO->> Step 490180 run_train: loss = 5.4370  (0.170 sec)
18-06-05 04:15-INFO->> Step 490190 run_train: loss = 5.4059  (0.177 sec)
18-06-05 04:15-INFO->> Step 490200 run_train: loss = 5.4013  (0.157 sec)
18-06-05 04:15-INFO->> Step 490210 run_train: loss = 5.3507  (0.190 sec)
18-06-05 04:15-INFO->> Step 490220 run_train: loss = 5.3704  (0.148 sec)
18-06-05 04:15-INFO->> Step 490230 run_train: loss = 5.4111  (0.152 sec)
18-06-05 04:15-INFO->> Step 490240 run_train: loss = 5.4135  (0.162 sec)
18-06-05 04:15-INFO->> Step 490250 run_train: loss = 5.4206  (0.181 sec)
18-06-05 04:15-INFO->> Step 490260 run_train: loss = 5.3857  (0.170 sec)
18-06-05 04:15-INFO->> Step 490270 run_train: loss = 5.4151  (0.138 sec)
18-06-05 04:15-INFO->> Step 490280 run_train: loss = 5.4230  (0.151 sec)
18-06-05 04:15-INFO->> Step 490290 run_train: loss = 5.3739  (0.154 sec)
18-06-05 04:15-INFO->> Step 490300 run_train: loss = 5.4618  (0.157 sec)
18-06-05 04:15-INFO->> Step 490310 run_train: loss = 5.3800  (0.161 sec)
18-06-05 04:15-INFO->> Step 490320 run_train: loss = 5.4107  (0.165 sec)
18-06-05 04:15-INFO->> Step 490330 run_train: loss = 5.4222  (0.144 sec)
18-06-05 04:15-INFO->> Step 490340 run_train: loss = 5.4451  (0.170 sec)
18-06-05 04:15-INFO->> Step 490350 run_train: loss = 5.3828  (0.170 sec)
18-06-05 04:15-INFO->> Step 490360 run_train: loss = 5.3623  (0.148 sec)
18-06-05 04:15-INFO->> Step 490370 run_train: loss = 5.3827  (0.134 sec)
18-06-05 04:15-INFO->> Step 490380 run_train: loss = 5.4230  (0.144 sec)
18-06-05 04:15-INFO->> Step 490390 run_train: loss = 5.3601  (0.144 sec)
18-06-05 04:15-INFO->> Step 490400 run_train: loss = 5.4013  (0.183 sec)
18-06-05 04:15-INFO->> Step 490410 run_train: loss = 5.3701  (0.183 sec)
18-06-05 04:15-INFO->> Step 490420 run_train: loss = 5.4538  (0.158 sec)
18-06-05 04:15-INFO->> Step 490430 run_train: loss = 5.4301  (0.187 sec)
18-06-05 04:15-INFO->> Step 490440 run_train: loss = 5.3806  (0.120 sec)
18-06-05 04:15-INFO->> Step 490450 run_train: loss = 5.4296  (0.167 sec)
18-06-05 04:15-INFO->> Step 490460 run_train: loss = 5.4213  (0.180 sec)
18-06-05 04:15-INFO->> Step 490470 run_train: loss = 5.4357  (0.147 sec)
18-06-05 04:15-INFO->> Step 490480 run_train: loss = 5.4647  (0.122 sec)
18-06-05 04:15-INFO->> Step 490490 run_train: loss = 5.4446  (0.181 sec)
18-06-05 04:15-INFO->> Step 490500 run_train: loss = 5.4031  (0.181 sec)
18-06-05 04:15-INFO->> Step 490510 run_train: loss = 5.4115  (0.170 sec)
18-06-05 04:15-INFO->> Step 490520 run_train: loss = 5.4107  (0.170 sec)
18-06-05 04:15-INFO->> Step 490530 run_train: loss = 5.3709  (0.171 sec)
18-06-05 04:16-INFO->> Step 490540 run_train: loss = 5.4137  (0.193 sec)
18-06-05 04:16-INFO->> Step 490550 run_train: loss = 5.3703  (0.157 sec)
18-06-05 04:16-INFO->> Step 490560 run_train: loss = 5.3759  (0.177 sec)
18-06-05 04:16-INFO->> Step 490570 run_train: loss = 5.4018  (0.155 sec)
18-06-05 04:16-INFO->> Step 490580 run_train: loss = 5.4309  (0.127 sec)
18-06-05 04:16-INFO->> Step 490590 run_train: loss = 5.4197  (0.179 sec)
18-06-05 04:16-INFO->> Step 490600 run_train: loss = 5.4629  (0.171 sec)
18-06-05 04:16-INFO->> Step 490610 run_train: loss = 5.4396  (0.163 sec)
18-06-05 04:16-INFO->> Step 490620 run_train: loss = 5.4155  (0.130 sec)
18-06-05 04:16-INFO->> Step 490630 run_train: loss = 5.3886  (0.152 sec)
18-06-05 04:16-INFO->> Step 490640 run_train: loss = 5.3840  (0.201 sec)
18-06-05 04:16-INFO->> Step 490650 run_train: loss = 5.4056  (0.144 sec)
18-06-05 04:16-INFO->> Step 490660 run_train: loss = 5.3927  (0.166 sec)
18-06-05 04:16-INFO->> Step 490670 run_train: loss = 5.3984  (0.172 sec)
18-06-05 04:16-INFO->> Step 490680 run_train: loss = 5.4213  (0.145 sec)
18-06-05 04:16-INFO->> Step 490690 run_train: loss = 5.3836  (0.176 sec)
18-06-05 04:16-INFO->> Step 490700 run_train: loss = 5.4144  (0.160 sec)
18-06-05 04:16-INFO->> Step 490710 run_train: loss = 5.3881  (0.149 sec)
18-06-05 04:16-INFO->> Step 490720 run_train: loss = 5.3665  (0.146 sec)
18-06-05 04:16-INFO->> Step 490730 run_train: loss = 5.4125  (0.154 sec)
18-06-05 04:16-INFO->> Step 490740 run_train: loss = 5.3555  (0.160 sec)
18-06-05 04:16-INFO->> Step 490750 run_train: loss = 5.4173  (0.116 sec)
18-06-05 04:16-INFO->> Step 490760 run_train: loss = 5.3817  (0.141 sec)
18-06-05 04:16-INFO->> Step 490770 run_train: loss = 5.4554  (0.187 sec)
18-06-05 04:16-INFO->> Step 490780 run_train: loss = 5.3281  (0.199 sec)
18-06-05 04:16-INFO->> Step 490790 run_train: loss = 5.3630  (0.152 sec)
18-06-05 04:16-INFO->> Step 490800 run_train: loss = 5.4307  (0.145 sec)
18-06-05 04:16-INFO->> Step 490810 run_train: loss = 5.3801  (0.187 sec)
18-06-05 04:16-INFO->> Step 490820 run_train: loss = 5.4305  (0.156 sec)
18-06-05 04:16-INFO->> Step 490830 run_train: loss = 5.4822  (0.155 sec)
18-06-05 04:16-INFO->> Step 490840 run_train: loss = 5.3833  (0.180 sec)
18-06-05 04:16-INFO->> Step 490850 run_train: loss = 5.4398  (0.161 sec)
18-06-05 04:16-INFO->> Step 490860 run_train: loss = 5.4080  (0.184 sec)
18-06-05 04:16-INFO->> Step 490870 run_train: loss = 5.4575  (0.124 sec)
18-06-05 04:16-INFO->> Step 490880 run_train: loss = 5.4141  (0.153 sec)
18-06-05 04:16-INFO->> Step 490890 run_train: loss = 5.3471  (0.171 sec)
18-06-05 04:16-INFO->> Step 490900 run_train: loss = 5.3354  (0.143 sec)
18-06-05 04:16-INFO->> Step 490910 run_train: loss = 5.3807  (0.163 sec)
18-06-05 04:17-INFO->> Step 490920 run_train: loss = 5.4041  (0.173 sec)
18-06-05 04:17-INFO->> Step 490930 run_train: loss = 5.4249  (0.117 sec)
18-06-05 04:17-INFO->> Step 490940 run_train: loss = 5.4045  (0.132 sec)
18-06-05 04:17-INFO->> Step 490950 run_train: loss = 5.3909  (0.160 sec)
18-06-05 04:17-INFO->> Step 490960 run_train: loss = 5.3930  (0.165 sec)
18-06-05 04:17-INFO->> Step 490970 run_train: loss = 5.3725  (0.181 sec)
18-06-05 04:17-INFO->> Step 490980 run_train: loss = 5.3678  (0.167 sec)
18-06-05 04:17-INFO->> Step 490990 run_train: loss = 5.4187  (0.198 sec)
18-06-05 04:17-INFO->> Step 491000 run_train: loss = 5.3909  (0.150 sec)
18-06-05 04:17-INFO->> 2018-06-05 04:17:14.020561 Saving in ckpt
18-06-05 04:17-INFO-Test Data Eval:
18-06-05 04:17-INFO-fpr95 = 0.1702145324123273 and auc = 0.9692141070184197
18-06-05 04:17-INFO->> Step 491010 run_train: loss = 5.3862  (0.147 sec)
18-06-05 04:17-INFO->> Step 491020 run_train: loss = 5.3305  (0.164 sec)
18-06-05 04:17-INFO->> Step 491030 run_train: loss = 5.3822  (0.166 sec)
18-06-05 04:18-INFO->> Step 491040 run_train: loss = 5.3587  (0.162 sec)
18-06-05 04:18-INFO->> Step 491050 run_train: loss = 5.4685  (0.148 sec)
18-06-05 04:18-INFO->> Step 491060 run_train: loss = 5.4591  (0.151 sec)
18-06-05 04:18-INFO->> Step 491070 run_train: loss = 5.3745  (0.124 sec)
18-06-05 04:18-INFO->> Step 491080 run_train: loss = 5.3531  (0.142 sec)
18-06-05 04:18-INFO->> Step 491090 run_train: loss = 5.4605  (0.163 sec)
18-06-05 04:18-INFO->> Step 491100 run_train: loss = 5.3447  (0.124 sec)
18-06-05 04:18-INFO->> Step 491110 run_train: loss = 5.3179  (0.171 sec)
18-06-05 04:18-INFO->> Step 491120 run_train: loss = 5.4097  (0.166 sec)
18-06-05 04:18-INFO->> Step 491130 run_train: loss = 5.3862  (0.149 sec)
18-06-05 04:18-INFO->> Step 491140 run_train: loss = 5.4194  (0.154 sec)
18-06-05 04:18-INFO->> Step 491150 run_train: loss = 5.3741  (0.141 sec)
18-06-05 04:18-INFO->> Step 491160 run_train: loss = 5.4246  (0.159 sec)
18-06-05 04:18-INFO->> Step 491170 run_train: loss = 5.3973  (0.146 sec)
18-06-05 04:18-INFO->> Step 491180 run_train: loss = 5.4135  (0.185 sec)
18-06-05 04:18-INFO->> Step 491190 run_train: loss = 5.4662  (0.166 sec)
18-06-05 04:18-INFO->> Step 491200 run_train: loss = 5.4493  (0.165 sec)
18-06-05 04:18-INFO->> Step 491210 run_train: loss = 5.3983  (0.164 sec)
18-06-05 04:18-INFO->> Step 491220 run_train: loss = 5.3841  (0.155 sec)
18-06-05 04:18-INFO->> Step 491230 run_train: loss = 5.4322  (0.175 sec)
18-06-05 04:18-INFO->> Step 491240 run_train: loss = 5.3857  (0.162 sec)
18-06-05 04:18-INFO->> Step 491250 run_train: loss = 5.3734  (0.160 sec)
18-06-05 04:18-INFO->> Step 491260 run_train: loss = 5.3740  (0.183 sec)
18-06-05 04:18-INFO->> Step 491270 run_train: loss = 5.3936  (0.145 sec)
18-06-05 04:18-INFO->> Step 491280 run_train: loss = 5.3848  (0.161 sec)
18-06-05 04:18-INFO->> Step 491290 run_train: loss = 5.4114  (0.161 sec)
18-06-05 04:18-INFO->> Step 491300 run_train: loss = 5.4354  (0.155 sec)
18-06-05 04:18-INFO->> Step 491310 run_train: loss = 5.4071  (0.123 sec)
18-06-05 04:18-INFO->> Step 491320 run_train: loss = 5.4037  (0.160 sec)
18-06-05 04:18-INFO->> Step 491330 run_train: loss = 5.4557  (0.143 sec)
18-06-05 04:18-INFO->> Step 491340 run_train: loss = 5.4410  (0.140 sec)
18-06-05 04:18-INFO->> Step 491350 run_train: loss = 5.3952  (0.153 sec)
18-06-05 04:18-INFO->> Step 491360 run_train: loss = 5.4385  (0.175 sec)
18-06-05 04:18-INFO->> Step 491370 run_train: loss = 5.3982  (0.135 sec)
18-06-05 04:18-INFO->> Step 491380 run_train: loss = 5.3896  (0.164 sec)
18-06-05 04:18-INFO->> Step 491390 run_train: loss = 5.3802  (0.176 sec)
18-06-05 04:18-INFO->> Step 491400 run_train: loss = 5.4634  (0.155 sec)
18-06-05 04:18-INFO->> Step 491410 run_train: loss = 5.4056  (0.174 sec)
18-06-05 04:19-INFO->> Step 491420 run_train: loss = 5.3771  (0.159 sec)
18-06-05 04:19-INFO->> Step 491430 run_train: loss = 5.4010  (0.128 sec)
18-06-05 04:19-INFO->> Step 491440 run_train: loss = 5.4432  (0.189 sec)
18-06-05 04:19-INFO->> Step 491450 run_train: loss = 5.3967  (0.148 sec)
18-06-05 04:19-INFO->> Step 491460 run_train: loss = 5.4254  (0.160 sec)
18-06-05 04:19-INFO->> Step 491470 run_train: loss = 5.3955  (0.166 sec)
18-06-05 04:19-INFO->> Step 491480 run_train: loss = 5.4141  (0.155 sec)
18-06-05 04:19-INFO->> Step 491490 run_train: loss = 5.4205  (0.168 sec)
18-06-05 04:19-INFO->> Step 491500 run_train: loss = 5.4249  (0.175 sec)
18-06-05 04:19-INFO->> Step 491510 run_train: loss = 5.4246  (0.141 sec)
18-06-05 04:19-INFO->> Step 491520 run_train: loss = 5.4415  (0.166 sec)
18-06-05 04:19-INFO->> Step 491530 run_train: loss = 5.4252  (0.148 sec)
18-06-05 04:19-INFO->> Step 491540 run_train: loss = 5.3737  (0.164 sec)
18-06-05 04:19-INFO->> Step 491550 run_train: loss = 5.4048  (0.141 sec)
18-06-05 04:19-INFO->> Step 491560 run_train: loss = 5.3281  (0.158 sec)
18-06-05 04:19-INFO->> Step 491570 run_train: loss = 5.3465  (0.184 sec)
18-06-05 04:19-INFO->> Step 491580 run_train: loss = 5.3452  (0.173 sec)
18-06-05 04:19-INFO->> Step 491590 run_train: loss = 5.4124  (0.148 sec)
18-06-05 04:19-INFO->> Step 491600 run_train: loss = 5.4351  (0.172 sec)
18-06-05 04:19-INFO->> Step 491610 run_train: loss = 5.3762  (0.130 sec)
18-06-05 04:19-INFO->> Step 491620 run_train: loss = 5.4729  (0.164 sec)
18-06-05 04:19-INFO->> Step 491630 run_train: loss = 5.4013  (0.156 sec)
18-06-05 04:19-INFO->> Step 491640 run_train: loss = 5.3936  (0.168 sec)
18-06-05 04:19-INFO->> Step 491650 run_train: loss = 5.3883  (0.143 sec)
18-06-05 04:19-INFO->> Step 491660 run_train: loss = 5.3387  (0.174 sec)
18-06-05 04:19-INFO->> Step 491670 run_train: loss = 5.4044  (0.178 sec)
18-06-05 04:19-INFO->> Step 491680 run_train: loss = 5.3147  (0.157 sec)
18-06-05 04:19-INFO->> Step 491690 run_train: loss = 5.4092  (0.157 sec)
18-06-05 04:19-INFO->> Step 491700 run_train: loss = 5.3710  (0.146 sec)
18-06-05 04:19-INFO->> Step 491710 run_train: loss = 5.4267  (0.162 sec)
18-06-05 04:19-INFO->> Step 491720 run_train: loss = 5.3690  (0.137 sec)
18-06-05 04:19-INFO->> Step 491730 run_train: loss = 5.4807  (0.182 sec)
18-06-05 04:19-INFO->> Step 491740 run_train: loss = 5.3784  (0.168 sec)
18-06-05 04:19-INFO->> Step 491750 run_train: loss = 5.4347  (0.149 sec)
18-06-05 04:19-INFO->> Step 491760 run_train: loss = 5.3731  (0.156 sec)
18-06-05 04:19-INFO->> Step 491770 run_train: loss = 5.3503  (0.171 sec)
18-06-05 04:19-INFO->> Step 491780 run_train: loss = 5.3664  (0.148 sec)
18-06-05 04:19-INFO->> Step 491790 run_train: loss = 5.4324  (0.166 sec)
18-06-05 04:20-INFO->> Step 491800 run_train: loss = 5.3912  (0.159 sec)
18-06-05 04:20-INFO->> Step 491810 run_train: loss = 5.4093  (0.171 sec)
18-06-05 04:20-INFO->> Step 491820 run_train: loss = 5.4216  (0.126 sec)
18-06-05 04:20-INFO->> Step 491830 run_train: loss = 5.3886  (0.175 sec)
18-06-05 04:20-INFO->> Step 491840 run_train: loss = 5.4702  (0.154 sec)
18-06-05 04:20-INFO->> Step 491850 run_train: loss = 5.4023  (0.152 sec)
18-06-05 04:20-INFO->> Step 491860 run_train: loss = 5.4342  (0.158 sec)
18-06-05 04:20-INFO->> Step 491870 run_train: loss = 5.4406  (0.150 sec)
18-06-05 04:20-INFO->> Step 491880 run_train: loss = 5.3869  (0.128 sec)
18-06-05 04:20-INFO->> Step 491890 run_train: loss = 5.3323  (0.145 sec)
18-06-05 04:20-INFO->> Step 491900 run_train: loss = 5.3844  (0.182 sec)
18-06-05 04:20-INFO->> Step 491910 run_train: loss = 5.3874  (0.119 sec)
18-06-05 04:20-INFO->> Step 491920 run_train: loss = 5.4124  (0.154 sec)
18-06-05 04:20-INFO->> Step 491930 run_train: loss = 5.4198  (0.170 sec)
18-06-05 04:20-INFO->> Step 491940 run_train: loss = 5.4305  (0.162 sec)
18-06-05 04:20-INFO->> Step 491950 run_train: loss = 5.3505  (0.182 sec)
18-06-05 04:20-INFO->> Step 491960 run_train: loss = 5.4259  (0.174 sec)
18-06-05 04:20-INFO->> Step 491970 run_train: loss = 5.4357  (0.130 sec)
18-06-05 04:20-INFO->> Step 491980 run_train: loss = 5.3650  (0.187 sec)
18-06-05 04:20-INFO->> Step 491990 run_train: loss = 5.4227  (0.183 sec)
18-06-05 04:20-INFO->> Step 492000 run_train: loss = 5.3314  (0.158 sec)
18-06-05 04:20-INFO->> 2018-06-05 04:20:32.704718 Saving in ckpt
18-06-05 04:20-INFO-Test Data Eval:
18-06-05 04:21-INFO-fpr95 = 0.17340263018065888 and auc = 0.9688731655569097
18-06-05 04:21-INFO->> Step 492010 run_train: loss = 5.3410  (0.143 sec)
18-06-05 04:21-INFO->> Step 492020 run_train: loss = 5.4399  (0.166 sec)
18-06-05 04:21-INFO->> Step 492030 run_train: loss = 5.3484  (0.154 sec)
18-06-05 04:21-INFO->> Step 492040 run_train: loss = 5.4239  (0.154 sec)
18-06-05 04:21-INFO->> Step 492050 run_train: loss = 5.4338  (0.161 sec)
18-06-05 04:21-INFO->> Step 492060 run_train: loss = 5.4483  (0.181 sec)
18-06-05 04:21-INFO->> Step 492070 run_train: loss = 5.4559  (0.174 sec)
18-06-05 04:21-INFO->> Step 492080 run_train: loss = 5.3541  (0.179 sec)
18-06-05 04:21-INFO->> Step 492090 run_train: loss = 5.3969  (0.139 sec)
18-06-05 04:21-INFO->> Step 492100 run_train: loss = 5.3989  (0.172 sec)
18-06-05 04:21-INFO->> Step 492110 run_train: loss = 5.3832  (0.142 sec)
18-06-05 04:21-INFO->> Step 492120 run_train: loss = 5.4905  (0.177 sec)
18-06-05 04:21-INFO->> Step 492130 run_train: loss = 5.4479  (0.154 sec)
18-06-05 04:21-INFO->> Step 492140 run_train: loss = 5.4085  (0.159 sec)
18-06-05 04:21-INFO->> Step 492150 run_train: loss = 5.3724  (0.170 sec)
18-06-05 04:21-INFO->> Step 492160 run_train: loss = 5.4665  (0.147 sec)
18-06-05 04:21-INFO->> Step 492170 run_train: loss = 5.4099  (0.172 sec)
18-06-05 04:21-INFO->> Step 492180 run_train: loss = 5.3377  (0.159 sec)
18-06-05 04:21-INFO->> Step 492190 run_train: loss = 5.3597  (0.180 sec)
18-06-05 04:21-INFO->> Step 492200 run_train: loss = 5.4269  (0.152 sec)
18-06-05 04:21-INFO->> Step 492210 run_train: loss = 5.4394  (0.161 sec)
18-06-05 04:21-INFO->> Step 492220 run_train: loss = 5.4369  (0.130 sec)
18-06-05 04:21-INFO->> Step 492230 run_train: loss = 5.4308  (0.162 sec)
18-06-05 04:21-INFO->> Step 492240 run_train: loss = 5.4656  (0.119 sec)
18-06-05 04:21-INFO->> Step 492250 run_train: loss = 5.4242  (0.163 sec)
18-06-05 04:21-INFO->> Step 492260 run_train: loss = 5.4022  (0.156 sec)
18-06-05 04:21-INFO->> Step 492270 run_train: loss = 5.4245  (0.182 sec)
18-06-05 04:21-INFO->> Step 492280 run_train: loss = 5.4105  (0.188 sec)
18-06-05 04:21-INFO->> Step 492290 run_train: loss = 5.3925  (0.178 sec)
18-06-05 04:22-INFO->> Step 492300 run_train: loss = 5.3860  (0.121 sec)
18-06-05 04:22-INFO->> Step 492310 run_train: loss = 5.3854  (0.173 sec)
18-06-05 04:22-INFO->> Step 492320 run_train: loss = 5.4202  (0.155 sec)
18-06-05 04:22-INFO->> Step 492330 run_train: loss = 5.3887  (0.134 sec)
18-06-05 04:22-INFO->> Step 492340 run_train: loss = 5.3996  (0.159 sec)
18-06-05 04:22-INFO->> Step 492350 run_train: loss = 5.3835  (0.155 sec)
18-06-05 04:22-INFO->> Step 492360 run_train: loss = 5.4359  (0.176 sec)
18-06-05 04:22-INFO->> Step 492370 run_train: loss = 5.3933  (0.145 sec)
18-06-05 04:22-INFO->> Step 492380 run_train: loss = 5.4432  (0.130 sec)
18-06-05 04:22-INFO->> Step 492390 run_train: loss = 5.3699  (0.145 sec)
18-06-05 04:22-INFO->> Step 492400 run_train: loss = 5.3798  (0.166 sec)
18-06-05 04:22-INFO->> Step 492410 run_train: loss = 5.3658  (0.193 sec)
18-06-05 04:22-INFO->> Step 492420 run_train: loss = 5.4602  (0.117 sec)
18-06-05 04:22-INFO->> Step 492430 run_train: loss = 5.4085  (0.128 sec)
18-06-05 04:22-INFO->> Step 492440 run_train: loss = 5.3434  (0.162 sec)
18-06-05 04:22-INFO->> Step 492450 run_train: loss = 5.4211  (0.154 sec)
18-06-05 04:22-INFO->> Step 492460 run_train: loss = 5.4016  (0.166 sec)
18-06-05 04:22-INFO->> Step 492470 run_train: loss = 5.4498  (0.160 sec)
18-06-05 04:22-INFO->> Step 492480 run_train: loss = 5.3808  (0.150 sec)
18-06-05 04:22-INFO->> Step 492490 run_train: loss = 5.4287  (0.144 sec)
18-06-05 04:22-INFO->> Step 492500 run_train: loss = 5.3330  (0.168 sec)
18-06-05 04:22-INFO->> Step 492510 run_train: loss = 5.4307  (0.160 sec)
18-06-05 04:22-INFO->> Step 492520 run_train: loss = 5.4181  (0.167 sec)
18-06-05 04:22-INFO->> Step 492530 run_train: loss = 5.3736  (0.179 sec)
18-06-05 04:22-INFO->> Step 492540 run_train: loss = 5.3956  (0.163 sec)
18-06-05 04:22-INFO->> Step 492550 run_train: loss = 5.3611  (0.129 sec)
18-06-05 04:22-INFO->> Step 492560 run_train: loss = 5.4082  (0.138 sec)
18-06-05 04:22-INFO->> Step 492570 run_train: loss = 5.3984  (0.157 sec)
18-06-05 04:22-INFO->> Step 492580 run_train: loss = 5.4692  (0.136 sec)
18-06-05 04:22-INFO->> Step 492590 run_train: loss = 5.4184  (0.139 sec)
18-06-05 04:22-INFO->> Step 492600 run_train: loss = 5.3753  (0.127 sec)
18-06-05 04:22-INFO->> Step 492610 run_train: loss = 5.4373  (0.158 sec)
18-06-05 04:22-INFO->> Step 492620 run_train: loss = 5.3820  (0.174 sec)
18-06-05 04:22-INFO->> Step 492630 run_train: loss = 5.3734  (0.146 sec)
18-06-05 04:22-INFO->> Step 492640 run_train: loss = 5.4439  (0.157 sec)
18-06-05 04:22-INFO->> Step 492650 run_train: loss = 5.4114  (0.143 sec)
18-06-05 04:22-INFO->> Step 492660 run_train: loss = 5.4090  (0.171 sec)
18-06-05 04:22-INFO->> Step 492670 run_train: loss = 5.4590  (0.163 sec)
18-06-05 04:23-INFO->> Step 492680 run_train: loss = 5.4225  (0.169 sec)
18-06-05 04:23-INFO->> Step 492690 run_train: loss = 5.4222  (0.192 sec)
18-06-05 04:23-INFO->> Step 492700 run_train: loss = 5.4040  (0.165 sec)
18-06-05 04:23-INFO->> Step 492710 run_train: loss = 5.3742  (0.132 sec)
18-06-05 04:23-INFO->> Step 492720 run_train: loss = 5.4163  (0.171 sec)
18-06-05 04:23-INFO->> Step 492730 run_train: loss = 5.3262  (0.145 sec)
18-06-05 04:23-INFO->> Step 492740 run_train: loss = 5.4421  (0.150 sec)
18-06-05 04:23-INFO->> Step 492750 run_train: loss = 5.3958  (0.206 sec)
18-06-05 04:23-INFO->> Step 492760 run_train: loss = 5.4646  (0.137 sec)
18-06-05 04:23-INFO->> Step 492770 run_train: loss = 5.3948  (0.156 sec)
18-06-05 04:23-INFO->> Step 492780 run_train: loss = 5.4010  (0.151 sec)
18-06-05 04:23-INFO->> Step 492790 run_train: loss = 5.4113  (0.154 sec)
18-06-05 04:23-INFO->> Step 492800 run_train: loss = 5.3971  (0.162 sec)
18-06-05 04:23-INFO->> Step 492810 run_train: loss = 5.3792  (0.159 sec)
18-06-05 04:23-INFO->> Step 492820 run_train: loss = 5.3824  (0.140 sec)
18-06-05 04:23-INFO->> Step 492830 run_train: loss = 5.4395  (0.161 sec)
18-06-05 04:23-INFO->> Step 492840 run_train: loss = 5.3807  (0.177 sec)
18-06-05 04:23-INFO->> Step 492850 run_train: loss = 5.4069  (0.156 sec)
18-06-05 04:23-INFO->> Step 492860 run_train: loss = 5.4916  (0.154 sec)
18-06-05 04:23-INFO->> Step 492870 run_train: loss = 5.4313  (0.128 sec)
18-06-05 04:23-INFO->> Step 492880 run_train: loss = 5.4196  (0.165 sec)
18-06-05 04:23-INFO->> Step 492890 run_train: loss = 5.4316  (0.174 sec)
18-06-05 04:23-INFO->> Step 492900 run_train: loss = 5.3951  (0.158 sec)
18-06-05 04:23-INFO->> Step 492910 run_train: loss = 5.3587  (0.166 sec)
18-06-05 04:23-INFO->> Step 492920 run_train: loss = 5.3804  (0.166 sec)
18-06-05 04:23-INFO->> Step 492930 run_train: loss = 5.3631  (0.162 sec)
18-06-05 04:23-INFO->> Step 492940 run_train: loss = 5.3706  (0.127 sec)
18-06-05 04:23-INFO->> Step 492950 run_train: loss = 5.4061  (0.160 sec)
18-06-05 04:23-INFO->> Step 492960 run_train: loss = 5.3807  (0.182 sec)
18-06-05 04:23-INFO->> Step 492970 run_train: loss = 5.4150  (0.169 sec)
18-06-05 04:23-INFO->> Step 492980 run_train: loss = 5.3721  (0.153 sec)
18-06-05 04:23-INFO->> Step 492990 run_train: loss = 5.3720  (0.150 sec)
18-06-05 04:23-INFO->> Step 493000 run_train: loss = 5.4751  (0.151 sec)
18-06-05 04:23-INFO->> 2018-06-05 04:23:51.614781 Saving in ckpt
18-06-05 04:23-INFO-Test Data Eval:
18-06-05 04:24-INFO-fpr95 = 0.1753951912858661 and auc = 0.9690239053281405
18-06-05 04:24-INFO->> Step 493010 run_train: loss = 5.4093  (0.126 sec)
18-06-05 04:24-INFO->> Step 493020 run_train: loss = 5.3577  (0.142 sec)
18-06-05 04:24-INFO->> Step 493030 run_train: loss = 5.4461  (0.160 sec)
18-06-05 04:24-INFO->> Step 493040 run_train: loss = 5.3064  (0.134 sec)
18-06-05 04:24-INFO->> Step 493050 run_train: loss = 5.3926  (0.139 sec)
18-06-05 04:24-INFO->> Step 493060 run_train: loss = 5.4374  (0.122 sec)
18-06-05 04:24-INFO->> Step 493070 run_train: loss = 5.3816  (0.186 sec)
18-06-05 04:24-INFO->> Step 493080 run_train: loss = 5.3758  (0.158 sec)
18-06-05 04:24-INFO->> Step 493090 run_train: loss = 5.3692  (0.174 sec)
18-06-05 04:24-INFO->> Step 493100 run_train: loss = 5.4016  (0.182 sec)
18-06-05 04:24-INFO->> Step 493110 run_train: loss = 5.3628  (0.146 sec)
18-06-05 04:24-INFO->> Step 493120 run_train: loss = 5.4087  (0.189 sec)
18-06-05 04:24-INFO->> Step 493130 run_train: loss = 5.3489  (0.137 sec)
18-06-05 04:24-INFO->> Step 493140 run_train: loss = 5.3865  (0.126 sec)
18-06-05 04:24-INFO->> Step 493150 run_train: loss = 5.3780  (0.159 sec)
18-06-05 04:24-INFO->> Step 493160 run_train: loss = 5.4262  (0.128 sec)
18-06-05 04:24-INFO->> Step 493170 run_train: loss = 5.4555  (0.140 sec)
18-06-05 04:25-INFO->> Step 493180 run_train: loss = 5.3915  (0.148 sec)
18-06-05 04:25-INFO->> Step 493190 run_train: loss = 5.3485  (0.159 sec)
18-06-05 04:25-INFO->> Step 493200 run_train: loss = 5.3541  (0.176 sec)
18-06-05 04:25-INFO->> Step 493210 run_train: loss = 5.3713  (0.146 sec)
18-06-05 04:25-INFO->> Step 493220 run_train: loss = 5.4601  (0.164 sec)
18-06-05 04:25-INFO->> Step 493230 run_train: loss = 5.4157  (0.160 sec)
18-06-05 04:25-INFO->> Step 493240 run_train: loss = 5.4033  (0.173 sec)
18-06-05 04:25-INFO->> Step 493250 run_train: loss = 5.4707  (0.151 sec)
18-06-05 04:25-INFO->> Step 493260 run_train: loss = 5.4189  (0.162 sec)
18-06-05 04:25-INFO->> Step 493270 run_train: loss = 5.3797  (0.148 sec)
18-06-05 04:25-INFO->> Step 493280 run_train: loss = 5.3781  (0.161 sec)
18-06-05 04:25-INFO->> Step 493290 run_train: loss = 5.3778  (0.150 sec)
18-06-05 04:25-INFO->> Step 493300 run_train: loss = 5.3979  (0.123 sec)
18-06-05 04:25-INFO->> Step 493310 run_train: loss = 5.4225  (0.146 sec)
18-06-05 04:25-INFO->> Step 493320 run_train: loss = 5.4311  (0.149 sec)
18-06-05 04:25-INFO->> Step 493330 run_train: loss = 5.4291  (0.150 sec)
18-06-05 04:25-INFO->> Step 493340 run_train: loss = 5.4114  (0.177 sec)
18-06-05 04:25-INFO->> Step 493350 run_train: loss = 5.3893  (0.129 sec)
18-06-05 04:25-INFO->> Step 493360 run_train: loss = 5.3792  (0.169 sec)
18-06-05 04:25-INFO->> Step 493370 run_train: loss = 5.4309  (0.133 sec)
18-06-05 04:25-INFO->> Step 493380 run_train: loss = 5.4262  (0.169 sec)
18-06-05 04:25-INFO->> Step 493390 run_train: loss = 5.4025  (0.178 sec)
18-06-05 04:25-INFO->> Step 493400 run_train: loss = 5.4496  (0.149 sec)
18-06-05 04:25-INFO->> Step 493410 run_train: loss = 5.3733  (0.163 sec)
18-06-05 04:25-INFO->> Step 493420 run_train: loss = 5.3967  (0.170 sec)
18-06-05 04:25-INFO->> Step 493430 run_train: loss = 5.4846  (0.179 sec)
18-06-05 04:25-INFO->> Step 493440 run_train: loss = 5.4102  (0.164 sec)
18-06-05 04:25-INFO->> Step 493450 run_train: loss = 5.4119  (0.147 sec)
18-06-05 04:25-INFO->> Step 493460 run_train: loss = 5.4726  (0.148 sec)
18-06-05 04:25-INFO->> Step 493470 run_train: loss = 5.3521  (0.140 sec)
18-06-05 04:25-INFO->> Step 493480 run_train: loss = 5.3923  (0.190 sec)
18-06-05 04:25-INFO->> Step 493490 run_train: loss = 5.3894  (0.164 sec)
18-06-05 04:25-INFO->> Step 493500 run_train: loss = 5.4461  (0.156 sec)
18-06-05 04:25-INFO->> Step 493510 run_train: loss = 5.3660  (0.146 sec)
18-06-05 04:25-INFO->> Step 493520 run_train: loss = 5.3916  (0.177 sec)
18-06-05 04:25-INFO->> Step 493530 run_train: loss = 5.3864  (0.130 sec)
18-06-05 04:25-INFO->> Step 493540 run_train: loss = 5.3805  (0.163 sec)
18-06-05 04:25-INFO->> Step 493550 run_train: loss = 5.3866  (0.179 sec)
18-06-05 04:26-INFO->> Step 493560 run_train: loss = 5.4227  (0.191 sec)
18-06-05 04:26-INFO->> Step 493570 run_train: loss = 5.4398  (0.120 sec)
18-06-05 04:26-INFO->> Step 493580 run_train: loss = 5.3536  (0.175 sec)
18-06-05 04:26-INFO->> Step 493590 run_train: loss = 5.4355  (0.195 sec)
18-06-05 04:26-INFO->> Step 493600 run_train: loss = 5.4971  (0.175 sec)
18-06-05 04:26-INFO->> Step 493610 run_train: loss = 5.4302  (0.166 sec)
18-06-05 04:26-INFO->> Step 493620 run_train: loss = 5.3506  (0.118 sec)
18-06-05 04:26-INFO->> Step 493630 run_train: loss = 5.4250  (0.149 sec)
18-06-05 04:26-INFO->> Step 493640 run_train: loss = 5.4237  (0.141 sec)
18-06-05 04:26-INFO->> Step 493650 run_train: loss = 5.3864  (0.157 sec)
18-06-05 04:26-INFO->> Step 493660 run_train: loss = 5.4139  (0.166 sec)
18-06-05 04:26-INFO->> Step 493670 run_train: loss = 5.3788  (0.188 sec)
18-06-05 04:26-INFO->> Step 493680 run_train: loss = 5.4167  (0.164 sec)
18-06-05 04:26-INFO->> Step 493690 run_train: loss = 5.3973  (0.135 sec)
18-06-05 04:26-INFO->> Step 493700 run_train: loss = 5.4628  (0.168 sec)
18-06-05 04:26-INFO->> Step 493710 run_train: loss = 5.4088  (0.166 sec)
18-06-05 04:26-INFO->> Step 493720 run_train: loss = 5.3794  (0.124 sec)
18-06-05 04:26-INFO->> Step 493730 run_train: loss = 5.4252  (0.185 sec)
18-06-05 04:26-INFO->> Step 493740 run_train: loss = 5.4329  (0.159 sec)
18-06-05 04:26-INFO->> Step 493750 run_train: loss = 5.3975  (0.177 sec)
18-06-05 04:26-INFO->> Step 493760 run_train: loss = 5.4065  (0.158 sec)
18-06-05 04:26-INFO->> Step 493770 run_train: loss = 5.4112  (0.144 sec)
18-06-05 04:26-INFO->> Step 493780 run_train: loss = 5.4141  (0.142 sec)
18-06-05 04:26-INFO->> Step 493790 run_train: loss = 5.3874  (0.160 sec)
18-06-05 04:26-INFO->> Step 493800 run_train: loss = 5.4399  (0.137 sec)
18-06-05 04:26-INFO->> Step 493810 run_train: loss = 5.4012  (0.150 sec)
18-06-05 04:26-INFO->> Step 493820 run_train: loss = 5.4471  (0.156 sec)
18-06-05 04:26-INFO->> Step 493830 run_train: loss = 5.3645  (0.159 sec)
18-06-05 04:26-INFO->> Step 493840 run_train: loss = 5.4392  (0.160 sec)
18-06-05 04:26-INFO->> Step 493850 run_train: loss = 5.2931  (0.124 sec)
18-06-05 04:26-INFO->> Step 493860 run_train: loss = 5.3675  (0.163 sec)
18-06-05 04:26-INFO->> Step 493870 run_train: loss = 5.3768  (0.123 sec)
18-06-05 04:26-INFO->> Step 493880 run_train: loss = 5.3413  (0.143 sec)
18-06-05 04:26-INFO->> Step 493890 run_train: loss = 5.4504  (0.162 sec)
18-06-05 04:26-INFO->> Step 493900 run_train: loss = 5.4105  (0.155 sec)
18-06-05 04:26-INFO->> Step 493910 run_train: loss = 5.3942  (0.165 sec)
18-06-05 04:26-INFO->> Step 493920 run_train: loss = 5.3806  (0.135 sec)
18-06-05 04:26-INFO->> Step 493930 run_train: loss = 5.4199  (0.158 sec)
18-06-05 04:27-INFO->> Step 493940 run_train: loss = 5.4322  (0.167 sec)
18-06-05 04:27-INFO->> Step 493950 run_train: loss = 5.4246  (0.167 sec)
18-06-05 04:27-INFO->> Step 493960 run_train: loss = 5.4036  (0.128 sec)
18-06-05 04:27-INFO->> Step 493970 run_train: loss = 5.3930  (0.204 sec)
18-06-05 04:27-INFO->> Step 493980 run_train: loss = 5.3658  (0.145 sec)
18-06-05 04:27-INFO->> Step 493990 run_train: loss = 5.3950  (0.157 sec)
18-06-05 04:27-INFO->> Step 494000 run_train: loss = 5.4082  (0.134 sec)
18-06-05 04:27-INFO->> 2018-06-05 04:27:10.673224 Saving in ckpt
18-06-05 04:27-INFO-Test Data Eval:
18-06-05 04:27-INFO-fpr95 = 0.17926408076514347 and auc = 0.9687188765031247
18-06-05 04:27-INFO->> Step 494010 run_train: loss = 5.4191  (0.136 sec)
18-06-05 04:27-INFO->> Step 494020 run_train: loss = 5.3273  (0.147 sec)
18-06-05 04:27-INFO->> Step 494030 run_train: loss = 5.4237  (0.189 sec)
18-06-05 04:27-INFO->> Step 494040 run_train: loss = 5.4082  (0.140 sec)
18-06-05 04:27-INFO->> Step 494050 run_train: loss = 5.4421  (0.128 sec)
18-06-05 04:28-INFO->> Step 494060 run_train: loss = 5.4304  (0.137 sec)
18-06-05 04:28-INFO->> Step 494070 run_train: loss = 5.3960  (0.156 sec)
18-06-05 04:28-INFO->> Step 494080 run_train: loss = 5.3919  (0.133 sec)
18-06-05 04:28-INFO->> Step 494090 run_train: loss = 5.3973  (0.152 sec)
18-06-05 04:28-INFO->> Step 494100 run_train: loss = 5.3999  (0.154 sec)
18-06-05 04:28-INFO->> Step 494110 run_train: loss = 5.3787  (0.163 sec)
18-06-05 04:28-INFO->> Step 494120 run_train: loss = 5.4278  (0.160 sec)
18-06-05 04:28-INFO->> Step 494130 run_train: loss = 5.4087  (0.161 sec)
18-06-05 04:28-INFO->> Step 494140 run_train: loss = 5.4314  (0.159 sec)
18-06-05 04:28-INFO->> Step 494150 run_train: loss = 5.3920  (0.167 sec)
18-06-05 04:28-INFO->> Step 494160 run_train: loss = 5.3730  (0.130 sec)
18-06-05 04:28-INFO->> Step 494170 run_train: loss = 5.3578  (0.132 sec)
18-06-05 04:28-INFO->> Step 494180 run_train: loss = 5.3999  (0.154 sec)
18-06-05 04:28-INFO->> Step 494190 run_train: loss = 5.3846  (0.150 sec)
18-06-05 04:28-INFO->> Step 494200 run_train: loss = 5.3828  (0.153 sec)
18-06-05 04:28-INFO->> Step 494210 run_train: loss = 5.3821  (0.162 sec)
18-06-05 04:28-INFO->> Step 494220 run_train: loss = 5.3758  (0.134 sec)
18-06-05 04:28-INFO->> Step 494230 run_train: loss = 5.4250  (0.124 sec)
18-06-05 04:28-INFO->> Step 494240 run_train: loss = 5.4371  (0.179 sec)
18-06-05 04:28-INFO->> Step 494250 run_train: loss = 5.4242  (0.137 sec)
18-06-05 04:28-INFO->> Step 494260 run_train: loss = 5.4434  (0.160 sec)
18-06-05 04:28-INFO->> Step 494270 run_train: loss = 5.3847  (0.151 sec)
18-06-05 04:28-INFO->> Step 494280 run_train: loss = 5.4200  (0.150 sec)
18-06-05 04:28-INFO->> Step 494290 run_train: loss = 5.4716  (0.147 sec)
18-06-05 04:28-INFO->> Step 494300 run_train: loss = 5.3893  (0.174 sec)
18-06-05 04:28-INFO->> Step 494310 run_train: loss = 5.3802  (0.136 sec)
18-06-05 04:28-INFO->> Step 494320 run_train: loss = 5.3865  (0.166 sec)
18-06-05 04:28-INFO->> Step 494330 run_train: loss = 5.4233  (0.165 sec)
18-06-05 04:28-INFO->> Step 494340 run_train: loss = 5.3679  (0.155 sec)
18-06-05 04:28-INFO->> Step 494350 run_train: loss = 5.3621  (0.181 sec)
18-06-05 04:28-INFO->> Step 494360 run_train: loss = 5.4105  (0.178 sec)
18-06-05 04:28-INFO->> Step 494370 run_train: loss = 5.4324  (0.177 sec)
18-06-05 04:28-INFO->> Step 494380 run_train: loss = 5.4012  (0.168 sec)
18-06-05 04:28-INFO->> Step 494390 run_train: loss = 5.4227  (0.140 sec)
18-06-05 04:28-INFO->> Step 494400 run_train: loss = 5.4665  (0.174 sec)
18-06-05 04:28-INFO->> Step 494410 run_train: loss = 5.3584  (0.188 sec)
18-06-05 04:28-INFO->> Step 494420 run_train: loss = 5.3983  (0.141 sec)
18-06-05 04:28-INFO->> Step 494430 run_train: loss = 5.4151  (0.132 sec)
18-06-05 04:29-INFO->> Step 494440 run_train: loss = 5.3923  (0.124 sec)
18-06-05 04:29-INFO->> Step 494450 run_train: loss = 5.3631  (0.151 sec)
18-06-05 04:29-INFO->> Step 494460 run_train: loss = 5.4121  (0.157 sec)
18-06-05 04:29-INFO->> Step 494470 run_train: loss = 5.4793  (0.154 sec)
18-06-05 04:29-INFO->> Step 494480 run_train: loss = 5.3890  (0.138 sec)
18-06-05 04:29-INFO->> Step 494490 run_train: loss = 5.4890  (0.133 sec)
18-06-05 04:29-INFO->> Step 494500 run_train: loss = 5.4365  (0.173 sec)
18-06-05 04:29-INFO->> Step 494510 run_train: loss = 5.4125  (0.174 sec)
18-06-05 04:29-INFO->> Step 494520 run_train: loss = 5.3520  (0.197 sec)
18-06-05 04:29-INFO->> Step 494530 run_train: loss = 5.3782  (0.153 sec)
18-06-05 04:29-INFO->> Step 494540 run_train: loss = 5.4398  (0.153 sec)
18-06-05 04:29-INFO->> Step 494550 run_train: loss = 5.3053  (0.173 sec)
18-06-05 04:29-INFO->> Step 494560 run_train: loss = 5.4120  (0.183 sec)
18-06-05 04:29-INFO->> Step 494570 run_train: loss = 5.4098  (0.137 sec)
18-06-05 04:29-INFO->> Step 494580 run_train: loss = 5.3850  (0.156 sec)
18-06-05 04:29-INFO->> Step 494590 run_train: loss = 5.4084  (0.179 sec)
18-06-05 04:29-INFO->> Step 494600 run_train: loss = 5.4334  (0.150 sec)
18-06-05 04:29-INFO->> Step 494610 run_train: loss = 5.4485  (0.149 sec)
18-06-05 04:29-INFO->> Step 494620 run_train: loss = 5.4004  (0.161 sec)
18-06-05 04:29-INFO->> Step 494630 run_train: loss = 5.3909  (0.168 sec)
18-06-05 04:29-INFO->> Step 494640 run_train: loss = 5.3585  (0.155 sec)
18-06-05 04:29-INFO->> Step 494650 run_train: loss = 5.3688  (0.177 sec)
18-06-05 04:29-INFO->> Step 494660 run_train: loss = 5.4174  (0.184 sec)
18-06-05 04:29-INFO->> Step 494670 run_train: loss = 5.3668  (0.145 sec)
18-06-05 04:29-INFO->> Step 494680 run_train: loss = 5.3751  (0.188 sec)
18-06-05 04:29-INFO->> Step 494690 run_train: loss = 5.3453  (0.167 sec)
18-06-05 04:29-INFO->> Step 494700 run_train: loss = 5.3983  (0.153 sec)
18-06-05 04:29-INFO->> Step 494710 run_train: loss = 5.3433  (0.164 sec)
18-06-05 04:29-INFO->> Step 494720 run_train: loss = 5.4200  (0.169 sec)
18-06-05 04:29-INFO->> Step 494730 run_train: loss = 5.4064  (0.152 sec)
18-06-05 04:29-INFO->> Step 494740 run_train: loss = 5.4721  (0.126 sec)
18-06-05 04:29-INFO->> Step 494750 run_train: loss = 5.4353  (0.145 sec)
18-06-05 04:29-INFO->> Step 494760 run_train: loss = 5.4000  (0.153 sec)
18-06-05 04:29-INFO->> Step 494770 run_train: loss = 5.4077  (0.192 sec)
18-06-05 04:29-INFO->> Step 494780 run_train: loss = 5.4235  (0.168 sec)
18-06-05 04:29-INFO->> Step 494790 run_train: loss = 5.3668  (0.134 sec)
18-06-05 04:29-INFO->> Step 494800 run_train: loss = 5.4320  (0.195 sec)
18-06-05 04:29-INFO->> Step 494810 run_train: loss = 5.3620  (0.179 sec)
18-06-05 04:30-INFO->> Step 494820 run_train: loss = 5.4058  (0.163 sec)
18-06-05 04:30-INFO->> Step 494830 run_train: loss = 5.3965  (0.154 sec)
18-06-05 04:30-INFO->> Step 494840 run_train: loss = 5.3699  (0.150 sec)
18-06-05 04:30-INFO->> Step 494850 run_train: loss = 5.3536  (0.149 sec)
18-06-05 04:30-INFO->> Step 494860 run_train: loss = 5.4395  (0.159 sec)
18-06-05 04:30-INFO->> Step 494870 run_train: loss = 5.4492  (0.190 sec)
18-06-05 04:30-INFO->> Step 494880 run_train: loss = 5.4255  (0.158 sec)
18-06-05 04:30-INFO->> Step 494890 run_train: loss = 5.4422  (0.145 sec)
18-06-05 04:30-INFO->> Step 494900 run_train: loss = 5.4340  (0.162 sec)
18-06-05 04:30-INFO->> Step 494910 run_train: loss = 5.3953  (0.148 sec)
18-06-05 04:30-INFO->> Step 494920 run_train: loss = 5.3807  (0.127 sec)
18-06-05 04:30-INFO->> Step 494930 run_train: loss = 5.4538  (0.159 sec)
18-06-05 04:30-INFO->> Step 494940 run_train: loss = 5.3737  (0.146 sec)
18-06-05 04:30-INFO->> Step 494950 run_train: loss = 5.3949  (0.172 sec)
18-06-05 04:30-INFO->> Step 494960 run_train: loss = 5.4111  (0.132 sec)
18-06-05 04:30-INFO->> Step 494970 run_train: loss = 5.4140  (0.154 sec)
18-06-05 04:30-INFO->> Step 494980 run_train: loss = 5.4240  (0.184 sec)
18-06-05 04:30-INFO->> Step 494990 run_train: loss = 5.3637  (0.106 sec)
18-06-05 04:30-INFO->> Step 495000 run_train: loss = 5.4335  (0.162 sec)
18-06-05 04:30-INFO->> 2018-06-05 04:30:29.183578 Saving in ckpt
18-06-05 04:30-INFO-Test Data Eval:
18-06-05 04:31-INFO-fpr95 = 0.17308714133900105 and auc = 0.9691339602452826
18-06-05 04:31-INFO->> Step 495010 run_train: loss = 5.4350  (0.199 sec)
18-06-05 04:31-INFO->> Step 495020 run_train: loss = 5.4159  (0.173 sec)
18-06-05 04:31-INFO->> Step 495030 run_train: loss = 5.4043  (0.150 sec)
18-06-05 04:31-INFO->> Step 495040 run_train: loss = 5.4115  (0.180 sec)
18-06-05 04:31-INFO->> Step 495050 run_train: loss = 5.3749  (0.145 sec)
18-06-05 04:31-INFO->> Step 495060 run_train: loss = 5.4600  (0.188 sec)
18-06-05 04:31-INFO->> Step 495070 run_train: loss = 5.3745  (0.159 sec)
18-06-05 04:31-INFO->> Step 495080 run_train: loss = 5.4605  (0.141 sec)
18-06-05 04:31-INFO->> Step 495090 run_train: loss = 5.3740  (0.133 sec)
18-06-05 04:31-INFO->> Step 495100 run_train: loss = 5.3940  (0.188 sec)
18-06-05 04:31-INFO->> Step 495110 run_train: loss = 5.3579  (0.151 sec)
18-06-05 04:31-INFO->> Step 495120 run_train: loss = 5.4077  (0.202 sec)
18-06-05 04:31-INFO->> Step 495130 run_train: loss = 5.4536  (0.139 sec)
18-06-05 04:31-INFO->> Step 495140 run_train: loss = 5.3750  (0.141 sec)
18-06-05 04:31-INFO->> Step 495150 run_train: loss = 5.4140  (0.168 sec)
18-06-05 04:31-INFO->> Step 495160 run_train: loss = 5.4623  (0.148 sec)
18-06-05 04:31-INFO->> Step 495170 run_train: loss = 5.4067  (0.157 sec)
18-06-05 04:31-INFO->> Step 495180 run_train: loss = 5.4244  (0.156 sec)
18-06-05 04:31-INFO->> Step 495190 run_train: loss = 5.4086  (0.184 sec)
18-06-05 04:31-INFO->> Step 495200 run_train: loss = 5.4286  (0.130 sec)
18-06-05 04:31-INFO->> Step 495210 run_train: loss = 5.3915  (0.150 sec)
18-06-05 04:31-INFO->> Step 495220 run_train: loss = 5.3750  (0.157 sec)
18-06-05 04:31-INFO->> Step 495230 run_train: loss = 5.3507  (0.148 sec)
18-06-05 04:31-INFO->> Step 495240 run_train: loss = 5.4464  (0.184 sec)
18-06-05 04:31-INFO->> Step 495250 run_train: loss = 5.4060  (0.138 sec)
18-06-05 04:31-INFO->> Step 495260 run_train: loss = 5.4375  (0.156 sec)
18-06-05 04:31-INFO->> Step 495270 run_train: loss = 5.4827  (0.173 sec)
18-06-05 04:31-INFO->> Step 495280 run_train: loss = 5.4715  (0.180 sec)
18-06-05 04:31-INFO->> Step 495290 run_train: loss = 5.3824  (0.160 sec)
18-06-05 04:31-INFO->> Step 495300 run_train: loss = 5.3264  (0.156 sec)
18-06-05 04:31-INFO->> Step 495310 run_train: loss = 5.4269  (0.179 sec)
18-06-05 04:32-INFO->> Step 495320 run_train: loss = 5.3932  (0.190 sec)
18-06-05 04:32-INFO->> Step 495330 run_train: loss = 5.4123  (0.160 sec)
18-06-05 04:32-INFO->> Step 495340 run_train: loss = 5.4133  (0.152 sec)
18-06-05 04:32-INFO->> Step 495350 run_train: loss = 5.4284  (0.139 sec)
18-06-05 04:32-INFO->> Step 495360 run_train: loss = 5.4328  (0.154 sec)
18-06-05 04:32-INFO->> Step 495370 run_train: loss = 5.4128  (0.130 sec)
18-06-05 04:32-INFO->> Step 495380 run_train: loss = 5.4056  (0.126 sec)
18-06-05 04:32-INFO->> Step 495390 run_train: loss = 5.4311  (0.177 sec)
18-06-05 04:32-INFO->> Step 495400 run_train: loss = 5.3490  (0.183 sec)
18-06-05 04:32-INFO->> Step 495410 run_train: loss = 5.4192  (0.161 sec)
18-06-05 04:32-INFO->> Step 495420 run_train: loss = 5.3700  (0.138 sec)
18-06-05 04:32-INFO->> Step 495430 run_train: loss = 5.3840  (0.149 sec)
18-06-05 04:32-INFO->> Step 495440 run_train: loss = 5.4529  (0.174 sec)
18-06-05 04:32-INFO->> Step 495450 run_train: loss = 5.3884  (0.161 sec)
18-06-05 04:32-INFO->> Step 495460 run_train: loss = 5.3699  (0.140 sec)
18-06-05 04:32-INFO->> Step 495470 run_train: loss = 5.4498  (0.161 sec)
18-06-05 04:32-INFO->> Step 495480 run_train: loss = 5.3804  (0.165 sec)
18-06-05 04:32-INFO->> Step 495490 run_train: loss = 5.3342  (0.172 sec)
18-06-05 04:32-INFO->> Step 495500 run_train: loss = 5.4078  (0.167 sec)
18-06-05 04:32-INFO->> Step 495510 run_train: loss = 5.4563  (0.153 sec)
18-06-05 04:32-INFO->> Step 495520 run_train: loss = 5.3953  (0.154 sec)
18-06-05 04:32-INFO->> Step 495530 run_train: loss = 5.4121  (0.147 sec)
18-06-05 04:32-INFO->> Step 495540 run_train: loss = 5.4116  (0.158 sec)
18-06-05 04:32-INFO->> Step 495550 run_train: loss = 5.4061  (0.164 sec)
18-06-05 04:32-INFO->> Step 495560 run_train: loss = 5.4502  (0.157 sec)
18-06-05 04:32-INFO->> Step 495570 run_train: loss = 5.4401  (0.144 sec)
18-06-05 04:32-INFO->> Step 495580 run_train: loss = 5.3967  (0.146 sec)
18-06-05 04:32-INFO->> Step 495590 run_train: loss = 5.4150  (0.160 sec)
18-06-05 04:32-INFO->> Step 495600 run_train: loss = 5.4699  (0.168 sec)
18-06-05 04:32-INFO->> Step 495610 run_train: loss = 5.3997  (0.146 sec)
18-06-05 04:32-INFO->> Step 495620 run_train: loss = 5.4419  (0.191 sec)
18-06-05 04:32-INFO->> Step 495630 run_train: loss = 5.4565  (0.150 sec)
18-06-05 04:32-INFO->> Step 495640 run_train: loss = 5.3988  (0.145 sec)
18-06-05 04:32-INFO->> Step 495650 run_train: loss = 5.4098  (0.174 sec)
18-06-05 04:32-INFO->> Step 495660 run_train: loss = 5.4054  (0.144 sec)
18-06-05 04:32-INFO->> Step 495670 run_train: loss = 5.4192  (0.121 sec)
18-06-05 04:32-INFO->> Step 495680 run_train: loss = 5.3780  (0.197 sec)
18-06-05 04:32-INFO->> Step 495690 run_train: loss = 5.4316  (0.151 sec)
18-06-05 04:33-INFO->> Step 495700 run_train: loss = 5.3759  (0.198 sec)
18-06-05 04:33-INFO->> Step 495710 run_train: loss = 5.3702  (0.169 sec)
18-06-05 04:33-INFO->> Step 495720 run_train: loss = 5.4129  (0.121 sec)
18-06-05 04:33-INFO->> Step 495730 run_train: loss = 5.4444  (0.151 sec)
18-06-05 04:33-INFO->> Step 495740 run_train: loss = 5.4131  (0.156 sec)
18-06-05 04:33-INFO->> Step 495750 run_train: loss = 5.4253  (0.135 sec)
18-06-05 04:33-INFO->> Step 495760 run_train: loss = 5.3343  (0.141 sec)
18-06-05 04:33-INFO->> Step 495770 run_train: loss = 5.4765  (0.151 sec)
18-06-05 04:33-INFO->> Step 495780 run_train: loss = 5.4449  (0.166 sec)
18-06-05 04:33-INFO->> Step 495790 run_train: loss = 5.4391  (0.161 sec)
18-06-05 04:33-INFO->> Step 495800 run_train: loss = 5.3386  (0.154 sec)
18-06-05 04:33-INFO->> Step 495810 run_train: loss = 5.4255  (0.118 sec)
18-06-05 04:33-INFO->> Step 495820 run_train: loss = 5.3696  (0.168 sec)
18-06-05 04:33-INFO->> Step 495830 run_train: loss = 5.4127  (0.165 sec)
18-06-05 04:33-INFO->> Step 495840 run_train: loss = 5.4528  (0.148 sec)
18-06-05 04:33-INFO->> Step 495850 run_train: loss = 5.4165  (0.131 sec)
18-06-05 04:33-INFO->> Step 495860 run_train: loss = 5.4227  (0.130 sec)
18-06-05 04:33-INFO->> Step 495870 run_train: loss = 5.3739  (0.192 sec)
18-06-05 04:33-INFO->> Step 495880 run_train: loss = 5.4190  (0.195 sec)
18-06-05 04:33-INFO->> Step 495890 run_train: loss = 5.3859  (0.157 sec)
18-06-05 04:33-INFO->> Step 495900 run_train: loss = 5.4524  (0.177 sec)
18-06-05 04:33-INFO->> Step 495910 run_train: loss = 5.3929  (0.140 sec)
18-06-05 04:33-INFO->> Step 495920 run_train: loss = 5.3780  (0.155 sec)
18-06-05 04:33-INFO->> Step 495930 run_train: loss = 5.3543  (0.178 sec)
18-06-05 04:33-INFO->> Step 495940 run_train: loss = 5.3584  (0.148 sec)
18-06-05 04:33-INFO->> Step 495950 run_train: loss = 5.3985  (0.177 sec)
18-06-05 04:33-INFO->> Step 495960 run_train: loss = 5.4083  (0.158 sec)
18-06-05 04:33-INFO->> Step 495970 run_train: loss = 5.4276  (0.140 sec)
18-06-05 04:33-INFO->> Step 495980 run_train: loss = 5.3830  (0.149 sec)
18-06-05 04:33-INFO->> Step 495990 run_train: loss = 5.4319  (0.148 sec)
18-06-05 04:33-INFO->> Step 496000 run_train: loss = 5.3974  (0.147 sec)
18-06-05 04:33-INFO->> 2018-06-05 04:33:48.260979 Saving in ckpt
18-06-05 04:33-INFO-Test Data Eval:
18-06-05 04:34-INFO-fpr95 = 0.17356867693942615 and auc = 0.9689996617886366
18-06-05 04:34-INFO->> Step 496010 run_train: loss = 5.3790  (0.168 sec)
18-06-05 04:34-INFO->> Step 496020 run_train: loss = 5.3894  (0.121 sec)
18-06-05 04:34-INFO->> Step 496030 run_train: loss = 5.3848  (0.144 sec)
18-06-05 04:34-INFO->> Step 496040 run_train: loss = 5.3778  (0.129 sec)
18-06-05 04:34-INFO->> Step 496050 run_train: loss = 5.3646  (0.168 sec)
18-06-05 04:34-INFO->> Step 496060 run_train: loss = 5.3948  (0.177 sec)
18-06-05 04:34-INFO->> Step 496070 run_train: loss = 5.4314  (0.157 sec)
18-06-05 04:34-INFO->> Step 496080 run_train: loss = 5.3765  (0.163 sec)
18-06-05 04:34-INFO->> Step 496090 run_train: loss = 5.4152  (0.148 sec)
18-06-05 04:34-INFO->> Step 496100 run_train: loss = 5.4807  (0.152 sec)
18-06-05 04:34-INFO->> Step 496110 run_train: loss = 5.4104  (0.142 sec)
18-06-05 04:34-INFO->> Step 496120 run_train: loss = 5.3893  (0.155 sec)
18-06-05 04:34-INFO->> Step 496130 run_train: loss = 5.4318  (0.179 sec)
18-06-05 04:34-INFO->> Step 496140 run_train: loss = 5.4647  (0.203 sec)
18-06-05 04:34-INFO->> Step 496150 run_train: loss = 5.3986  (0.128 sec)
18-06-05 04:34-INFO->> Step 496160 run_train: loss = 5.3524  (0.146 sec)
18-06-05 04:34-INFO->> Step 496170 run_train: loss = 5.4129  (0.164 sec)
18-06-05 04:34-INFO->> Step 496180 run_train: loss = 5.4069  (0.176 sec)
18-06-05 04:34-INFO->> Step 496190 run_train: loss = 5.4396  (0.165 sec)
18-06-05 04:35-INFO->> Step 496200 run_train: loss = 5.3602  (0.135 sec)
18-06-05 04:35-INFO->> Step 496210 run_train: loss = 5.3775  (0.170 sec)
18-06-05 04:35-INFO->> Step 496220 run_train: loss = 5.3934  (0.160 sec)
18-06-05 04:35-INFO->> Step 496230 run_train: loss = 5.4163  (0.142 sec)
18-06-05 04:35-INFO->> Step 496240 run_train: loss = 5.4243  (0.165 sec)
18-06-05 04:35-INFO->> Step 496250 run_train: loss = 5.3546  (0.145 sec)
18-06-05 04:35-INFO->> Step 496260 run_train: loss = 5.4717  (0.149 sec)
18-06-05 04:35-INFO->> Step 496270 run_train: loss = 5.3956  (0.155 sec)
18-06-05 04:35-INFO->> Step 496280 run_train: loss = 5.4622  (0.155 sec)
18-06-05 04:35-INFO->> Step 496290 run_train: loss = 5.4082  (0.144 sec)
18-06-05 04:35-INFO->> Step 496300 run_train: loss = 5.4273  (0.144 sec)
18-06-05 04:35-INFO->> Step 496310 run_train: loss = 5.4338  (0.134 sec)
18-06-05 04:35-INFO->> Step 496320 run_train: loss = 5.3426  (0.150 sec)
18-06-05 04:35-INFO->> Step 496330 run_train: loss = 5.3570  (0.158 sec)
18-06-05 04:35-INFO->> Step 496340 run_train: loss = 5.4112  (0.159 sec)
18-06-05 04:35-INFO->> Step 496350 run_train: loss = 5.3699  (0.167 sec)
18-06-05 04:35-INFO->> Step 496360 run_train: loss = 5.3829  (0.158 sec)
18-06-05 04:35-INFO->> Step 496370 run_train: loss = 5.4056  (0.128 sec)
18-06-05 04:35-INFO->> Step 496380 run_train: loss = 5.4041  (0.144 sec)
18-06-05 04:35-INFO->> Step 496390 run_train: loss = 5.3705  (0.155 sec)
18-06-05 04:35-INFO->> Step 496400 run_train: loss = 5.3383  (0.148 sec)
18-06-05 04:35-INFO->> Step 496410 run_train: loss = 5.3649  (0.170 sec)
18-06-05 04:35-INFO->> Step 496420 run_train: loss = 5.3322  (0.174 sec)
18-06-05 04:35-INFO->> Step 496430 run_train: loss = 5.4208  (0.148 sec)
18-06-05 04:35-INFO->> Step 496440 run_train: loss = 5.3784  (0.155 sec)
18-06-05 04:35-INFO->> Step 496450 run_train: loss = 5.4121  (0.163 sec)
18-06-05 04:35-INFO->> Step 496460 run_train: loss = 5.3435  (0.163 sec)
18-06-05 04:35-INFO->> Step 496470 run_train: loss = 5.3302  (0.173 sec)
18-06-05 04:35-INFO->> Step 496480 run_train: loss = 5.4177  (0.177 sec)
18-06-05 04:35-INFO->> Step 496490 run_train: loss = 5.4241  (0.150 sec)
18-06-05 04:35-INFO->> Step 496500 run_train: loss = 5.3644  (0.167 sec)
18-06-05 04:35-INFO->> Step 496510 run_train: loss = 5.3915  (0.181 sec)
18-06-05 04:35-INFO->> Step 496520 run_train: loss = 5.3331  (0.143 sec)
18-06-05 04:35-INFO->> Step 496530 run_train: loss = 5.3650  (0.156 sec)
18-06-05 04:35-INFO->> Step 496540 run_train: loss = 5.3685  (0.155 sec)
18-06-05 04:35-INFO->> Step 496550 run_train: loss = 5.3323  (0.164 sec)
18-06-05 04:35-INFO->> Step 496560 run_train: loss = 5.3393  (0.176 sec)
18-06-05 04:35-INFO->> Step 496570 run_train: loss = 5.4200  (0.105 sec)
18-06-05 04:36-INFO->> Step 496580 run_train: loss = 5.3786  (0.172 sec)
18-06-05 04:36-INFO->> Step 496590 run_train: loss = 5.4490  (0.165 sec)
18-06-05 04:36-INFO->> Step 496600 run_train: loss = 5.4282  (0.173 sec)
18-06-05 04:36-INFO->> Step 496610 run_train: loss = 5.4094  (0.150 sec)
18-06-05 04:36-INFO->> Step 496620 run_train: loss = 5.4139  (0.175 sec)
18-06-05 04:36-INFO->> Step 496630 run_train: loss = 5.4602  (0.203 sec)
18-06-05 04:36-INFO->> Step 496640 run_train: loss = 5.4053  (0.174 sec)
18-06-05 04:36-INFO->> Step 496650 run_train: loss = 5.4402  (0.151 sec)
18-06-05 04:36-INFO->> Step 496660 run_train: loss = 5.4847  (0.146 sec)
18-06-05 04:36-INFO->> Step 496670 run_train: loss = 5.4244  (0.153 sec)
18-06-05 04:36-INFO->> Step 496680 run_train: loss = 5.3877  (0.172 sec)
18-06-05 04:36-INFO->> Step 496690 run_train: loss = 5.4242  (0.167 sec)
18-06-05 04:36-INFO->> Step 496700 run_train: loss = 5.3576  (0.159 sec)
18-06-05 04:36-INFO->> Step 496710 run_train: loss = 5.4420  (0.126 sec)
18-06-05 04:36-INFO->> Step 496720 run_train: loss = 5.3738  (0.136 sec)
18-06-05 04:36-INFO->> Step 496730 run_train: loss = 5.3915  (0.139 sec)
18-06-05 04:36-INFO->> Step 496740 run_train: loss = 5.4017  (0.155 sec)
18-06-05 04:36-INFO->> Step 496750 run_train: loss = 5.3760  (0.186 sec)
18-06-05 04:36-INFO->> Step 496760 run_train: loss = 5.3733  (0.173 sec)
18-06-05 04:36-INFO->> Step 496770 run_train: loss = 5.3574  (0.158 sec)
18-06-05 04:36-INFO->> Step 496780 run_train: loss = 5.3202  (0.151 sec)
18-06-05 04:36-INFO->> Step 496790 run_train: loss = 5.3670  (0.183 sec)
18-06-05 04:36-INFO->> Step 496800 run_train: loss = 5.3856  (0.153 sec)
18-06-05 04:36-INFO->> Step 496810 run_train: loss = 5.4388  (0.214 sec)
18-06-05 04:36-INFO->> Step 496820 run_train: loss = 5.3762  (0.192 sec)
18-06-05 04:36-INFO->> Step 496830 run_train: loss = 5.3965  (0.156 sec)
18-06-05 04:36-INFO->> Step 496840 run_train: loss = 5.4452  (0.164 sec)
18-06-05 04:36-INFO->> Step 496850 run_train: loss = 5.4575  (0.159 sec)
18-06-05 04:36-INFO->> Step 496860 run_train: loss = 5.3672  (0.180 sec)
18-06-05 04:36-INFO->> Step 496870 run_train: loss = 5.3893  (0.128 sec)
18-06-05 04:36-INFO->> Step 496880 run_train: loss = 5.4121  (0.140 sec)
18-06-05 04:36-INFO->> Step 496890 run_train: loss = 5.4108  (0.146 sec)
18-06-05 04:36-INFO->> Step 496900 run_train: loss = 5.3988  (0.165 sec)
18-06-05 04:36-INFO->> Step 496910 run_train: loss = 5.4295  (0.193 sec)
18-06-05 04:36-INFO->> Step 496920 run_train: loss = 5.4235  (0.156 sec)
18-06-05 04:36-INFO->> Step 496930 run_train: loss = 5.3651  (0.145 sec)
18-06-05 04:36-INFO->> Step 496940 run_train: loss = 5.3730  (0.135 sec)
18-06-05 04:36-INFO->> Step 496950 run_train: loss = 5.3524  (0.164 sec)
18-06-05 04:37-INFO->> Step 496960 run_train: loss = 5.4129  (0.150 sec)
18-06-05 04:37-INFO->> Step 496970 run_train: loss = 5.4434  (0.164 sec)
18-06-05 04:37-INFO->> Step 496980 run_train: loss = 5.4291  (0.155 sec)
18-06-05 04:37-INFO->> Step 496990 run_train: loss = 5.4461  (0.170 sec)
18-06-05 04:37-INFO->> Step 497000 run_train: loss = 5.3115  (0.133 sec)
18-06-05 04:37-INFO->> 2018-06-05 04:37:07.191336 Saving in ckpt
18-06-05 04:37-INFO-Test Data Eval:
18-06-05 04:37-INFO-fpr95 = 0.17869952178533474 and auc = 0.9689110561402217
18-06-05 04:37-INFO->> Step 497010 run_train: loss = 5.3956  (0.153 sec)
18-06-05 04:37-INFO->> Step 497020 run_train: loss = 5.4402  (0.160 sec)
18-06-05 04:37-INFO->> Step 497030 run_train: loss = 5.4341  (0.167 sec)
18-06-05 04:37-INFO->> Step 497040 run_train: loss = 5.4189  (0.170 sec)
18-06-05 04:37-INFO->> Step 497050 run_train: loss = 5.4591  (0.173 sec)
18-06-05 04:37-INFO->> Step 497060 run_train: loss = 5.3629  (0.185 sec)
18-06-05 04:37-INFO->> Step 497070 run_train: loss = 5.4177  (0.157 sec)
18-06-05 04:37-INFO->> Step 497080 run_train: loss = 5.4420  (0.158 sec)
18-06-05 04:38-INFO->> Step 497090 run_train: loss = 5.4285  (0.168 sec)
18-06-05 04:38-INFO->> Step 497100 run_train: loss = 5.3987  (0.160 sec)
18-06-05 04:38-INFO->> Step 497110 run_train: loss = 5.4205  (0.147 sec)
18-06-05 04:38-INFO->> Step 497120 run_train: loss = 5.3839  (0.141 sec)
18-06-05 04:38-INFO->> Step 497130 run_train: loss = 5.4693  (0.172 sec)
18-06-05 04:38-INFO->> Step 497140 run_train: loss = 5.4206  (0.155 sec)
18-06-05 04:38-INFO->> Step 497150 run_train: loss = 5.4373  (0.167 sec)
18-06-05 04:38-INFO->> Step 497160 run_train: loss = 5.4041  (0.181 sec)
18-06-05 04:38-INFO->> Step 497170 run_train: loss = 5.3317  (0.129 sec)
18-06-05 04:38-INFO->> Step 497180 run_train: loss = 5.4087  (0.130 sec)
18-06-05 04:38-INFO->> Step 497190 run_train: loss = 5.4469  (0.161 sec)
18-06-05 04:38-INFO->> Step 497200 run_train: loss = 5.4227  (0.169 sec)
18-06-05 04:38-INFO->> Step 497210 run_train: loss = 5.3588  (0.163 sec)
18-06-05 04:38-INFO->> Step 497220 run_train: loss = 5.3966  (0.155 sec)
18-06-05 04:38-INFO->> Step 497230 run_train: loss = 5.4034  (0.142 sec)
18-06-05 04:38-INFO->> Step 497240 run_train: loss = 5.3938  (0.158 sec)
18-06-05 04:38-INFO->> Step 497250 run_train: loss = 5.4127  (0.193 sec)
18-06-05 04:38-INFO->> Step 497260 run_train: loss = 5.4800  (0.151 sec)
18-06-05 04:38-INFO->> Step 497270 run_train: loss = 5.4204  (0.137 sec)
18-06-05 04:38-INFO->> Step 497280 run_train: loss = 5.4009  (0.160 sec)
18-06-05 04:38-INFO->> Step 497290 run_train: loss = 5.3761  (0.173 sec)
18-06-05 04:38-INFO->> Step 497300 run_train: loss = 5.4213  (0.138 sec)
18-06-05 04:38-INFO->> Step 497310 run_train: loss = 5.4265  (0.161 sec)
18-06-05 04:38-INFO->> Step 497320 run_train: loss = 5.3906  (0.161 sec)
18-06-05 04:38-INFO->> Step 497330 run_train: loss = 5.4122  (0.149 sec)
18-06-05 04:38-INFO->> Step 497340 run_train: loss = 5.3849  (0.180 sec)
18-06-05 04:38-INFO->> Step 497350 run_train: loss = 5.4016  (0.142 sec)
18-06-05 04:38-INFO->> Step 497360 run_train: loss = 5.3721  (0.132 sec)
18-06-05 04:38-INFO->> Step 497370 run_train: loss = 5.3953  (0.125 sec)
18-06-05 04:38-INFO->> Step 497380 run_train: loss = 5.3545  (0.149 sec)
18-06-05 04:38-INFO->> Step 497390 run_train: loss = 5.3555  (0.153 sec)
18-06-05 04:38-INFO->> Step 497400 run_train: loss = 5.4154  (0.174 sec)
18-06-05 04:38-INFO->> Step 497410 run_train: loss = 5.4020  (0.167 sec)
18-06-05 04:38-INFO->> Step 497420 run_train: loss = 5.3760  (0.182 sec)
18-06-05 04:38-INFO->> Step 497430 run_train: loss = 5.3896  (0.177 sec)
18-06-05 04:38-INFO->> Step 497440 run_train: loss = 5.3517  (0.161 sec)
18-06-05 04:38-INFO->> Step 497450 run_train: loss = 5.3790  (0.174 sec)
18-06-05 04:38-INFO->> Step 497460 run_train: loss = 5.4519  (0.165 sec)
18-06-05 04:39-INFO->> Step 497470 run_train: loss = 5.3562  (0.159 sec)
18-06-05 04:39-INFO->> Step 497480 run_train: loss = 5.3823  (0.138 sec)
18-06-05 04:39-INFO->> Step 497490 run_train: loss = 5.3898  (0.157 sec)
18-06-05 04:39-INFO->> Step 497500 run_train: loss = 5.4055  (0.138 sec)
18-06-05 04:39-INFO->> Step 497510 run_train: loss = 5.3747  (0.216 sec)
18-06-05 04:39-INFO->> Step 497520 run_train: loss = 5.3733  (0.156 sec)
18-06-05 04:39-INFO->> Step 497530 run_train: loss = 5.4394  (0.180 sec)
18-06-05 04:39-INFO->> Step 497540 run_train: loss = 5.4342  (0.192 sec)
18-06-05 04:39-INFO->> Step 497550 run_train: loss = 5.4137  (0.164 sec)
18-06-05 04:39-INFO->> Step 497560 run_train: loss = 5.3714  (0.155 sec)
18-06-05 04:39-INFO->> Step 497570 run_train: loss = 5.4403  (0.175 sec)
18-06-05 04:39-INFO->> Step 497580 run_train: loss = 5.4005  (0.163 sec)
18-06-05 04:39-INFO->> Step 497590 run_train: loss = 5.3937  (0.211 sec)
18-06-05 04:39-INFO->> Step 497600 run_train: loss = 5.3648  (0.154 sec)
18-06-05 04:39-INFO->> Step 497610 run_train: loss = 5.4176  (0.180 sec)
18-06-05 04:39-INFO->> Step 497620 run_train: loss = 5.4814  (0.145 sec)
18-06-05 04:39-INFO->> Step 497630 run_train: loss = 5.4261  (0.177 sec)
18-06-05 04:39-INFO->> Step 497640 run_train: loss = 5.3764  (0.152 sec)
18-06-05 04:39-INFO->> Step 497650 run_train: loss = 5.3696  (0.166 sec)
18-06-05 04:39-INFO->> Step 497660 run_train: loss = 5.4483  (0.146 sec)
18-06-05 04:39-INFO->> Step 497670 run_train: loss = 5.3462  (0.156 sec)
18-06-05 04:39-INFO->> Step 497680 run_train: loss = 5.3710  (0.171 sec)
18-06-05 04:39-INFO->> Step 497690 run_train: loss = 5.4798  (0.152 sec)
18-06-05 04:39-INFO->> Step 497700 run_train: loss = 5.3964  (0.172 sec)
18-06-05 04:39-INFO->> Step 497710 run_train: loss = 5.4477  (0.180 sec)
18-06-05 04:39-INFO->> Step 497720 run_train: loss = 5.4194  (0.130 sec)
18-06-05 04:39-INFO->> Step 497730 run_train: loss = 5.4559  (0.175 sec)
18-06-05 04:39-INFO->> Step 497740 run_train: loss = 5.3759  (0.155 sec)
18-06-05 04:39-INFO->> Step 497750 run_train: loss = 5.4153  (0.151 sec)
18-06-05 04:39-INFO->> Step 497760 run_train: loss = 5.3979  (0.142 sec)
18-06-05 04:39-INFO->> Step 497770 run_train: loss = 5.4376  (0.145 sec)
18-06-05 04:39-INFO->> Step 497780 run_train: loss = 5.4079  (0.182 sec)
18-06-05 04:39-INFO->> Step 497790 run_train: loss = 5.3917  (0.139 sec)
18-06-05 04:39-INFO->> Step 497800 run_train: loss = 5.4563  (0.136 sec)
18-06-05 04:39-INFO->> Step 497810 run_train: loss = 5.2951  (0.173 sec)
18-06-05 04:39-INFO->> Step 497820 run_train: loss = 5.3534  (0.149 sec)
18-06-05 04:39-INFO->> Step 497830 run_train: loss = 5.4071  (0.159 sec)
18-06-05 04:40-INFO->> Step 497840 run_train: loss = 5.4065  (0.166 sec)
18-06-05 04:40-INFO->> Step 497850 run_train: loss = 5.3815  (0.188 sec)
18-06-05 04:40-INFO->> Step 497860 run_train: loss = 5.3731  (0.148 sec)
18-06-05 04:40-INFO->> Step 497870 run_train: loss = 5.3475  (0.162 sec)
18-06-05 04:40-INFO->> Step 497880 run_train: loss = 5.4138  (0.167 sec)
18-06-05 04:40-INFO->> Step 497890 run_train: loss = 5.3895  (0.157 sec)
18-06-05 04:40-INFO->> Step 497900 run_train: loss = 5.3739  (0.154 sec)
18-06-05 04:40-INFO->> Step 497910 run_train: loss = 5.4739  (0.165 sec)
18-06-05 04:40-INFO->> Step 497920 run_train: loss = 5.3959  (0.144 sec)
18-06-05 04:40-INFO->> Step 497930 run_train: loss = 5.4196  (0.134 sec)
18-06-05 04:40-INFO->> Step 497940 run_train: loss = 5.3501  (0.150 sec)
18-06-05 04:40-INFO->> Step 497950 run_train: loss = 5.3987  (0.178 sec)
18-06-05 04:40-INFO->> Step 497960 run_train: loss = 5.4067  (0.171 sec)
18-06-05 04:40-INFO->> Step 497970 run_train: loss = 5.3863  (0.170 sec)
18-06-05 04:40-INFO->> Step 497980 run_train: loss = 5.3903  (0.190 sec)
18-06-05 04:40-INFO->> Step 497990 run_train: loss = 5.3136  (0.144 sec)
18-06-05 04:40-INFO->> Step 498000 run_train: loss = 5.4280  (0.176 sec)
18-06-05 04:40-INFO->> 2018-06-05 04:40:25.558851 Saving in ckpt
18-06-05 04:40-INFO-Test Data Eval:
18-06-05 04:41-INFO-fpr95 = 0.17639977417640806 and auc = 0.9687563042294421
18-06-05 04:41-INFO->> Step 498010 run_train: loss = 5.4228  (0.149 sec)
18-06-05 04:41-INFO->> Step 498020 run_train: loss = 5.4332  (0.164 sec)
18-06-05 04:41-INFO->> Step 498030 run_train: loss = 5.4197  (0.132 sec)
18-06-05 04:41-INFO->> Step 498040 run_train: loss = 5.3694  (0.129 sec)
18-06-05 04:41-INFO->> Step 498050 run_train: loss = 5.4038  (0.178 sec)
18-06-05 04:41-INFO->> Step 498060 run_train: loss = 5.4350  (0.143 sec)
18-06-05 04:41-INFO->> Step 498070 run_train: loss = 5.3866  (0.194 sec)
18-06-05 04:41-INFO->> Step 498080 run_train: loss = 5.4391  (0.165 sec)
18-06-05 04:41-INFO->> Step 498090 run_train: loss = 5.3892  (0.151 sec)
18-06-05 04:41-INFO->> Step 498100 run_train: loss = 5.3893  (0.143 sec)
18-06-05 04:41-INFO->> Step 498110 run_train: loss = 5.4389  (0.159 sec)
18-06-05 04:41-INFO->> Step 498120 run_train: loss = 5.3758  (0.149 sec)
18-06-05 04:41-INFO->> Step 498130 run_train: loss = 5.4050  (0.149 sec)
18-06-05 04:41-INFO->> Step 498140 run_train: loss = 5.4185  (0.164 sec)
18-06-05 04:41-INFO->> Step 498150 run_train: loss = 5.3619  (0.134 sec)
18-06-05 04:41-INFO->> Step 498160 run_train: loss = 5.4234  (0.159 sec)
18-06-05 04:41-INFO->> Step 498170 run_train: loss = 5.4248  (0.187 sec)
18-06-05 04:41-INFO->> Step 498180 run_train: loss = 5.4584  (0.165 sec)
18-06-05 04:41-INFO->> Step 498190 run_train: loss = 5.3598  (0.160 sec)
18-06-05 04:41-INFO->> Step 498200 run_train: loss = 5.3669  (0.167 sec)
18-06-05 04:41-INFO->> Step 498210 run_train: loss = 5.3311  (0.167 sec)
18-06-05 04:41-INFO->> Step 498220 run_train: loss = 5.3563  (0.189 sec)
18-06-05 04:41-INFO->> Step 498230 run_train: loss = 5.4283  (0.156 sec)
18-06-05 04:41-INFO->> Step 498240 run_train: loss = 5.4818  (0.160 sec)
18-06-05 04:41-INFO->> Step 498250 run_train: loss = 5.2487  (0.140 sec)
18-06-05 04:41-INFO->> Step 498260 run_train: loss = 5.3924  (0.178 sec)
18-06-05 04:41-INFO->> Step 498270 run_train: loss = 5.3608  (0.142 sec)
18-06-05 04:41-INFO->> Step 498280 run_train: loss = 5.4638  (0.128 sec)
18-06-05 04:41-INFO->> Step 498290 run_train: loss = 5.4256  (0.173 sec)
18-06-05 04:41-INFO->> Step 498300 run_train: loss = 5.3805  (0.160 sec)
18-06-05 04:41-INFO->> Step 498310 run_train: loss = 5.3402  (0.147 sec)
18-06-05 04:41-INFO->> Step 498320 run_train: loss = 5.4161  (0.135 sec)
18-06-05 04:41-INFO->> Step 498330 run_train: loss = 5.3736  (0.164 sec)
18-06-05 04:41-INFO->> Step 498340 run_train: loss = 5.3492  (0.189 sec)
18-06-05 04:42-INFO->> Step 498350 run_train: loss = 5.4574  (0.151 sec)
18-06-05 04:42-INFO->> Step 498360 run_train: loss = 5.4081  (0.156 sec)
18-06-05 04:42-INFO->> Step 498370 run_train: loss = 5.3699  (0.156 sec)
18-06-05 04:42-INFO->> Step 498380 run_train: loss = 5.4018  (0.159 sec)
18-06-05 04:42-INFO->> Step 498390 run_train: loss = 5.3670  (0.119 sec)
18-06-05 04:42-INFO->> Step 498400 run_train: loss = 5.4002  (0.133 sec)
18-06-05 04:42-INFO->> Step 498410 run_train: loss = 5.4039  (0.160 sec)
18-06-05 04:42-INFO->> Step 498420 run_train: loss = 5.4610  (0.149 sec)
18-06-05 04:42-INFO->> Step 498430 run_train: loss = 5.4206  (0.176 sec)
18-06-05 04:42-INFO->> Step 498440 run_train: loss = 5.3995  (0.156 sec)
18-06-05 04:42-INFO->> Step 498450 run_train: loss = 5.4117  (0.186 sec)
18-06-05 04:42-INFO->> Step 498460 run_train: loss = 5.4604  (0.148 sec)
18-06-05 04:42-INFO->> Step 498470 run_train: loss = 5.3836  (0.163 sec)
18-06-05 04:42-INFO->> Step 498480 run_train: loss = 5.3355  (0.169 sec)
18-06-05 04:42-INFO->> Step 498490 run_train: loss = 5.3626  (0.145 sec)
18-06-05 04:42-INFO->> Step 498500 run_train: loss = 5.4185  (0.140 sec)
18-06-05 04:42-INFO->> Step 498510 run_train: loss = 5.4075  (0.150 sec)
18-06-05 04:42-INFO->> Step 498520 run_train: loss = 5.4099  (0.156 sec)
18-06-05 04:42-INFO->> Step 498530 run_train: loss = 5.4756  (0.172 sec)
18-06-05 04:42-INFO->> Step 498540 run_train: loss = 5.3644  (0.158 sec)
18-06-05 04:42-INFO->> Step 498550 run_train: loss = 5.3363  (0.159 sec)
18-06-05 04:42-INFO->> Step 498560 run_train: loss = 5.3850  (0.161 sec)
18-06-05 04:42-INFO->> Step 498570 run_train: loss = 5.4515  (0.147 sec)
18-06-05 04:42-INFO->> Step 498580 run_train: loss = 5.4060  (0.162 sec)
18-06-05 04:42-INFO->> Step 498590 run_train: loss = 5.3761  (0.181 sec)
18-06-05 04:42-INFO->> Step 498600 run_train: loss = 5.3764  (0.174 sec)
18-06-05 04:42-INFO->> Step 498610 run_train: loss = 5.4041  (0.192 sec)
18-06-05 04:42-INFO->> Step 498620 run_train: loss = 5.3562  (0.131 sec)
18-06-05 04:42-INFO->> Step 498630 run_train: loss = 5.3735  (0.209 sec)
18-06-05 04:42-INFO->> Step 498640 run_train: loss = 5.3651  (0.158 sec)
18-06-05 04:42-INFO->> Step 498650 run_train: loss = 5.4274  (0.198 sec)
18-06-05 04:42-INFO->> Step 498660 run_train: loss = 5.3751  (0.154 sec)
18-06-05 04:42-INFO->> Step 498670 run_train: loss = 5.4238  (0.173 sec)
18-06-05 04:42-INFO->> Step 498680 run_train: loss = 5.4578  (0.109 sec)
18-06-05 04:42-INFO->> Step 498690 run_train: loss = 5.4446  (0.122 sec)
18-06-05 04:42-INFO->> Step 498700 run_train: loss = 5.4443  (0.173 sec)
18-06-05 04:42-INFO->> Step 498710 run_train: loss = 5.3810  (0.163 sec)
18-06-05 04:43-INFO->> Step 498720 run_train: loss = 5.3449  (0.159 sec)
18-06-05 04:43-INFO->> Step 498730 run_train: loss = 5.4203  (0.150 sec)
18-06-05 04:43-INFO->> Step 498740 run_train: loss = 5.4376  (0.149 sec)
18-06-05 04:43-INFO->> Step 498750 run_train: loss = 5.3756  (0.135 sec)
18-06-05 04:43-INFO->> Step 498760 run_train: loss = 5.3807  (0.169 sec)
18-06-05 04:43-INFO->> Step 498770 run_train: loss = 5.3512  (0.141 sec)
18-06-05 04:43-INFO->> Step 498780 run_train: loss = 5.4004  (0.180 sec)
18-06-05 04:43-INFO->> Step 498790 run_train: loss = 5.4410  (0.134 sec)
18-06-05 04:43-INFO->> Step 498800 run_train: loss = 5.4211  (0.163 sec)
18-06-05 04:43-INFO->> Step 498810 run_train: loss = 5.4040  (0.144 sec)
18-06-05 04:43-INFO->> Step 498820 run_train: loss = 5.3822  (0.191 sec)
18-06-05 04:43-INFO->> Step 498830 run_train: loss = 5.3848  (0.150 sec)
18-06-05 04:43-INFO->> Step 498840 run_train: loss = 5.3795  (0.180 sec)
18-06-05 04:43-INFO->> Step 498850 run_train: loss = 5.4064  (0.125 sec)
18-06-05 04:43-INFO->> Step 498860 run_train: loss = 5.3968  (0.156 sec)
18-06-05 04:43-INFO->> Step 498870 run_train: loss = 5.3963  (0.164 sec)
18-06-05 04:43-INFO->> Step 498880 run_train: loss = 5.4014  (0.177 sec)
18-06-05 04:43-INFO->> Step 498890 run_train: loss = 5.4335  (0.138 sec)
18-06-05 04:43-INFO->> Step 498900 run_train: loss = 5.4105  (0.164 sec)
18-06-05 04:43-INFO->> Step 498910 run_train: loss = 5.4151  (0.154 sec)
18-06-05 04:43-INFO->> Step 498920 run_train: loss = 5.4037  (0.115 sec)
18-06-05 04:43-INFO->> Step 498930 run_train: loss = 5.3622  (0.174 sec)
18-06-05 04:43-INFO->> Step 498940 run_train: loss = 5.4189  (0.153 sec)
18-06-05 04:43-INFO->> Step 498950 run_train: loss = 5.4145  (0.192 sec)
18-06-05 04:43-INFO->> Step 498960 run_train: loss = 5.3840  (0.177 sec)
18-06-05 04:43-INFO->> Step 498970 run_train: loss = 5.4210  (0.132 sec)
18-06-05 04:43-INFO->> Step 498980 run_train: loss = 5.3746  (0.153 sec)
18-06-05 04:43-INFO->> Step 498990 run_train: loss = 5.4480  (0.147 sec)
18-06-05 04:43-INFO->> Step 499000 run_train: loss = 5.3978  (0.203 sec)
18-06-05 04:43-INFO->> 2018-06-05 04:43:44.775393 Saving in ckpt
18-06-05 04:43-INFO-Test Data Eval:
18-06-05 04:44-INFO-fpr95 = 0.17536198193411265 and auc = 0.9687603495582155
18-06-05 04:44-INFO->> Step 499010 run_train: loss = 5.4223  (0.156 sec)
18-06-05 04:44-INFO->> Step 499020 run_train: loss = 5.3749  (0.131 sec)
18-06-05 04:44-INFO->> Step 499030 run_train: loss = 5.4433  (0.172 sec)
18-06-05 04:44-INFO->> Step 499040 run_train: loss = 5.3722  (0.130 sec)
18-06-05 04:44-INFO->> Step 499050 run_train: loss = 5.4557  (0.162 sec)
18-06-05 04:44-INFO->> Step 499060 run_train: loss = 5.4214  (0.138 sec)
18-06-05 04:44-INFO->> Step 499070 run_train: loss = 5.4162  (0.159 sec)
18-06-05 04:44-INFO->> Step 499080 run_train: loss = 5.3842  (0.161 sec)
18-06-05 04:44-INFO->> Step 499090 run_train: loss = 5.3962  (0.179 sec)
18-06-05 04:44-INFO->> Step 499100 run_train: loss = 5.3526  (0.171 sec)
18-06-05 04:44-INFO->> Step 499110 run_train: loss = 5.4887  (0.135 sec)
18-06-05 04:44-INFO->> Step 499120 run_train: loss = 5.3810  (0.159 sec)
18-06-05 04:44-INFO->> Step 499130 run_train: loss = 5.3752  (0.169 sec)
18-06-05 04:44-INFO->> Step 499140 run_train: loss = 5.4047  (0.164 sec)
18-06-05 04:44-INFO->> Step 499150 run_train: loss = 5.3778  (0.154 sec)
18-06-05 04:44-INFO->> Step 499160 run_train: loss = 5.4344  (0.162 sec)
18-06-05 04:44-INFO->> Step 499170 run_train: loss = 5.4013  (0.141 sec)
18-06-05 04:44-INFO->> Step 499180 run_train: loss = 5.4295  (0.187 sec)
18-06-05 04:44-INFO->> Step 499190 run_train: loss = 5.3878  (0.155 sec)
18-06-05 04:44-INFO->> Step 499200 run_train: loss = 5.4463  (0.189 sec)
18-06-05 04:44-INFO->> Step 499210 run_train: loss = 5.4188  (0.162 sec)
18-06-05 04:44-INFO->> Step 499220 run_train: loss = 5.4013  (0.143 sec)
18-06-05 04:45-INFO->> Step 499230 run_train: loss = 5.3276  (0.160 sec)
18-06-05 04:45-INFO->> Step 499240 run_train: loss = 5.4398  (0.184 sec)
18-06-05 04:45-INFO->> Step 499250 run_train: loss = 5.4598  (0.187 sec)
18-06-05 04:45-INFO->> Step 499260 run_train: loss = 5.3992  (0.141 sec)
18-06-05 04:45-INFO->> Step 499270 run_train: loss = 5.4546  (0.200 sec)
18-06-05 04:45-INFO->> Step 499280 run_train: loss = 5.4140  (0.162 sec)
18-06-05 04:45-INFO->> Step 499290 run_train: loss = 5.4493  (0.180 sec)
18-06-05 04:45-INFO->> Step 499300 run_train: loss = 5.4534  (0.177 sec)
18-06-05 04:45-INFO->> Step 499310 run_train: loss = 5.3861  (0.159 sec)
18-06-05 04:45-INFO->> Step 499320 run_train: loss = 5.3941  (0.130 sec)
18-06-05 04:45-INFO->> Step 499330 run_train: loss = 5.4540  (0.139 sec)
18-06-05 04:45-INFO->> Step 499340 run_train: loss = 5.4216  (0.175 sec)
18-06-05 04:45-INFO->> Step 499350 run_train: loss = 5.4109  (0.178 sec)
18-06-05 04:45-INFO->> Step 499360 run_train: loss = 5.3417  (0.154 sec)
18-06-05 04:45-INFO->> Step 499370 run_train: loss = 5.4222  (0.154 sec)
18-06-05 04:45-INFO->> Step 499380 run_train: loss = 5.3992  (0.161 sec)
18-06-05 04:45-INFO->> Step 499390 run_train: loss = 5.4435  (0.201 sec)
18-06-05 04:45-INFO->> Step 499400 run_train: loss = 5.3589  (0.141 sec)
18-06-05 04:45-INFO->> Step 499410 run_train: loss = 5.3369  (0.174 sec)
18-06-05 04:45-INFO->> Step 499420 run_train: loss = 5.3509  (0.166 sec)
18-06-05 04:45-INFO->> Step 499430 run_train: loss = 5.3411  (0.183 sec)
18-06-05 04:45-INFO->> Step 499440 run_train: loss = 5.3740  (0.161 sec)
18-06-05 04:45-INFO->> Step 499450 run_train: loss = 5.3915  (0.161 sec)
18-06-05 04:45-INFO->> Step 499460 run_train: loss = 5.3891  (0.145 sec)
18-06-05 04:45-INFO->> Step 499470 run_train: loss = 5.3906  (0.159 sec)
18-06-05 04:45-INFO->> Step 499480 run_train: loss = 5.4133  (0.129 sec)
18-06-05 04:45-INFO->> Step 499490 run_train: loss = 5.4004  (0.153 sec)
18-06-05 04:45-INFO->> Step 499500 run_train: loss = 5.4900  (0.155 sec)
18-06-05 04:45-INFO->> Step 499510 run_train: loss = 5.4036  (0.200 sec)
18-06-05 04:45-INFO->> Step 499520 run_train: loss = 5.4464  (0.143 sec)
18-06-05 04:45-INFO->> Step 499530 run_train: loss = 5.4038  (0.150 sec)
18-06-05 04:45-INFO->> Step 499540 run_train: loss = 5.4057  (0.181 sec)
18-06-05 04:45-INFO->> Step 499550 run_train: loss = 5.3634  (0.173 sec)
18-06-05 04:45-INFO->> Step 499560 run_train: loss = 5.4281  (0.122 sec)
18-06-05 04:45-INFO->> Step 499570 run_train: loss = 5.3952  (0.145 sec)
18-06-05 04:45-INFO->> Step 499580 run_train: loss = 5.3670  (0.132 sec)
18-06-05 04:45-INFO->> Step 499590 run_train: loss = 5.3714  (0.130 sec)
18-06-05 04:46-INFO->> Step 499600 run_train: loss = 5.4143  (0.165 sec)
18-06-05 04:46-INFO->> Step 499610 run_train: loss = 5.4441  (0.139 sec)
18-06-05 04:46-INFO->> Step 499620 run_train: loss = 5.3752  (0.178 sec)
18-06-05 04:46-INFO->> Step 499630 run_train: loss = 5.3602  (0.134 sec)
18-06-05 04:46-INFO->> Step 499640 run_train: loss = 5.4556  (0.149 sec)
18-06-05 04:46-INFO->> Step 499650 run_train: loss = 5.4184  (0.195 sec)
18-06-05 04:46-INFO->> Step 499660 run_train: loss = 5.4151  (0.176 sec)
18-06-05 04:46-INFO->> Step 499670 run_train: loss = 5.3615  (0.141 sec)
18-06-05 04:46-INFO->> Step 499680 run_train: loss = 5.3713  (0.198 sec)
18-06-05 04:46-INFO->> Step 499690 run_train: loss = 5.3726  (0.160 sec)
18-06-05 04:46-INFO->> Step 499700 run_train: loss = 5.3633  (0.154 sec)
18-06-05 04:46-INFO->> Step 499710 run_train: loss = 5.3635  (0.167 sec)
18-06-05 04:46-INFO->> Step 499720 run_train: loss = 5.4684  (0.182 sec)
18-06-05 04:46-INFO->> Step 499730 run_train: loss = 5.4052  (0.187 sec)
18-06-05 04:46-INFO->> Step 499740 run_train: loss = 5.4131  (0.153 sec)
18-06-05 04:46-INFO->> Step 499750 run_train: loss = 5.4281  (0.129 sec)
18-06-05 04:46-INFO->> Step 499760 run_train: loss = 5.4187  (0.160 sec)
18-06-05 04:46-INFO->> Step 499770 run_train: loss = 5.4132  (0.152 sec)
18-06-05 04:46-INFO->> Step 499780 run_train: loss = 5.3975  (0.150 sec)
18-06-05 04:46-INFO->> Step 499790 run_train: loss = 5.3614  (0.192 sec)
18-06-05 04:46-INFO->> Step 499800 run_train: loss = 5.4000  (0.153 sec)
18-06-05 04:46-INFO->> Step 499810 run_train: loss = 5.3790  (0.150 sec)
18-06-05 04:46-INFO->> Step 499820 run_train: loss = 5.4366  (0.159 sec)
18-06-05 04:46-INFO->> Step 499830 run_train: loss = 5.3510  (0.178 sec)
18-06-05 04:46-INFO->> Step 499840 run_train: loss = 5.4311  (0.131 sec)
18-06-05 04:46-INFO->> Step 499850 run_train: loss = 5.4227  (0.162 sec)
18-06-05 04:46-INFO->> Step 499860 run_train: loss = 5.3658  (0.179 sec)
18-06-05 04:46-INFO->> Step 499870 run_train: loss = 5.3763  (0.170 sec)
18-06-05 04:46-INFO->> Step 499880 run_train: loss = 5.4246  (0.151 sec)
18-06-05 04:46-INFO->> Step 499890 run_train: loss = 5.4673  (0.140 sec)
18-06-05 04:46-INFO->> Step 499900 run_train: loss = 5.3471  (0.171 sec)
18-06-05 04:46-INFO->> Step 499910 run_train: loss = 5.4309  (0.182 sec)
18-06-05 04:46-INFO->> Step 499920 run_train: loss = 5.4220  (0.170 sec)
18-06-05 04:46-INFO->> Step 499930 run_train: loss = 5.4041  (0.139 sec)
18-06-05 04:46-INFO->> Step 499940 run_train: loss = 5.3873  (0.151 sec)
18-06-05 04:46-INFO->> Step 499950 run_train: loss = 5.4085  (0.160 sec)
18-06-05 04:46-INFO->> Step 499960 run_train: loss = 5.3785  (0.148 sec)
18-06-05 04:46-INFO->> Step 499970 run_train: loss = 5.3555  (0.147 sec)
18-06-05 04:47-INFO->> Step 499980 run_train: loss = 5.4537  (0.165 sec)
18-06-05 04:47-INFO->> Step 499990 run_train: loss = 5.3473  (0.162 sec)
